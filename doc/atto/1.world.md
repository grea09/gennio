---
title: "1. World"
author: Antoine Gréa
bibliography: bibliography/thesis.bib
style: thesis
---

# Knowledge representation

Knowledge representation is at the intersection of maths, logic, language and computer sciences. Its research begins in the end of the 19^th^ century, with Cantor inventing set theory [@cantor_property_1874]. Then after a crisis in the begining of the 20^th^ century with Russel's paradox and Gödel's incompletude theorem, revised versions of the set theory become one of the fundations of mathematics. The most accepted version is the Zermelo–Fraenkel axiomatic set theory with the axiom of Choice (ZFC) [@fraenkel_foundations_1973; @ciesielski_set_1997]. This effort was to represent mathematics in itself as a way to formalize its own expression.

A similar process happened in the 1970's, when logic based knowledge representation gained popularity among computer scientists [@baader_description_2003]. Systems at the time explored notions such as rules and networks to try and organize knowledge into a rigourous structure. At the same time other systems were built arround First Order Logic (FOL). Then, around the 1990's, the research began to merge in search of common semantics in what led to the developpement of Description Logics (DL). This domain is expressing knowledge as a hierarchy of classes containing individuals.

From there and with the advent of the world wide web, engineers were on the lookout for standardization and interoperability of computer systems. One such standardization took the name of "semantic web" and aimed to created a widespread network of connected services sharing knowledge between one another in a common language. At the begining of the 21^st^ century, several languages were created, all based around W3C specifications called Resource Description Framework (RDF) [@klyne_resource_2004]. This language is based around the notion of statements as triples. Each can store a unit of knowledge at a time. All the underlying theoritical work of DL continued with it and created more efficient and lighter derivatives. One such derivative is the family of languages called Web Ontology Language (OWL) [@horrocks_shiq_2003]. Ontologies and knowledge graphs are more recent names for the representation and definition of categories (DL classes), properties and relation between concepts, data and entities.

All these knowledge description systems rely on a syntax to interoperate systems and users to one another. The base of such language comes from the formalization of automated grammars by @chomsky_three_1956. It mostly consist in a set of hierarchical rules aiming to deconstruct an input string into a sequence of terminal symbols. This deconstruction is called parsing and is a common operation in computer science. More tools for the formalization of computer language emerged soon after thanks to @backus_syntax_1959 while working on a programming language at IBM. This is how the Backus–Naur Form (BNF) meta-language was created ontop of Chomsky's formalization.

All these tools are the base for all modern knowledge representations. In the rest of this chapter, we discuss the fundamentals of each of the aspects of knowledge description, then we propose a knowledge description framework that is able to adapt to its usage.

## Fundamentals

First and foremost, we present the list of notations in this document. While trying to stick to traditional notations, we also aim for an unambiguious notation across several domains while remaining concise and precise. In 
@tbl:symbols we present the list of all symbols and relations. When possible we use the classical mathematical operator and otherwise we prefer lower case greek letters for relations. @Tbl:sets lists all sets and general notions that we represent with uppercase letters.

**Symbol**                            **Description**
----------                            ---------------
$var : exp$                           The colon is a separator to be read as "such that".
$[exp]$                               Iverson's brackets: $[false]=0 \land [true]=1$.
$\{e : exp(e)\}$                      Set builder notation, all $e$ such that $exp(e)$ is true.
$\langle e_1, e_2, e_n \rangle$       $n$-Tuple of various elements.
$e_s \xrightarrow{e_p} e_o$           Link or edge from $e_s$ to $e_o$ having $e_p$.
$l \downarrow a$                      Link $l$ partially supports step $a$.
$\pi \Downarrow a$                    Plan $\pi$ fully supports $a$.
$\emptyset$                           Empty set, also notted $\{\}$.
$\subset, \cup, \cap$                 Set inclusion, union and intersection.
$e\in E$                              Element $e$ belongs in set $E$ also called choice operator.
$=$                                   Equality relation.
$\lnot , \land, \lor$                 Negation (not), conjonction (and), disjonction (logical or).
$|E|$                                 Cardinal of set $E$.
$t_1 \prec t_2$                       $t_1$ subsumes $t_2$. Also used for order : $t_1$ precedes $t_2$
$\top, \bot$                          Top and bottom symbols used as true and false respectively.
$\models$                             Entails, used for logical implication.
$f \circ g$                           Function composition.
$\otimes$                             Flaws in a partial plan. Several variants :
$\otimes_\downarrowbarred$            • unsupported subgoal.
$\otimes_\dagger$                     • causal threat to an existing causal link.
$\otimes_\circlearrowleft$            • cycle in the plan.
$\otimes_\ast$                        • decomposition of a compound action.
$\otimes_\origof$                     • alternative to an existing action.
$\otimes_\circledcirc$                • orphan action in the plan.
$\oplus, \ominus$                     Positive and negative resolvers.
$¢$                                   Cost of an action.
$\mathbb{i}, \mathbb{g}$              Initial and Goal step.
$\mathbb{P}(E)$                       Powerset, set of all subsets of $E$.
$\mathcal{P}(X)$                      Probability of event $X$.
$\mathit{pre}, \mathit{eff}$          Preconditions and effects of an action.
$\delta$                              Duration of an action.
$\mu^\pm(e)$                          Signed meta-relation. $\mu^+$ abstracts and $\mu^-$ reifies.
                                      $\mu$ alone gives the abstraction level.
$\nu(e)$                              Name of entity $e$.
$\pi$                                 Plan or method of compound action.
$\rho(e)$                             Parameters of entity $e$.
$\sigma(e)$                           Scope of entity $e$.
$\tau(e)$                             Type of entity $e$.
$\phi^\pm(e)$                         Signed incidence (edge) and adjacence (vertice) function for graphs. 
                                      $\phi^-$ gives the anterior (subject), $\phi^+$ the posterior (object) and
                                      $\phi^0$ gives the property of statement or cause of a causal link. 
                                      $\phi$ alone gives a triple corresponding to the edge.
$\chi^*(g)$                           Transitive cover of graph $g$. 

: List of symbols and relations. The symbol $\pm$ shows when the notation has signed variants. {#tbl:symbols}


**Name**    **Description**
----------  ---------------
$A$         Actions.
$C$         Causal links.
$D$         Domain of knowledge.
$E$         Entities.
$F$         Fluents.
$L$         Literals.
$O$         Observations.
$P$         Properties.
$Q$         Quantifiers.
$S$         Statements.
$T$         Types.
$\Pi$       Plans or method of compound action.

: List of important named sets. {#tbl:sets}

### Fundation of maths and logic systems

In order to understand knowledge representation, some mathematical and logical tools need to be presented. 

#### Logic

The first mathematical notion we define is logic. More precisely First Order Logic (FOL) in the context of DL.

Introducing the two boolean values $\top$ *true* and $\bot$ *false* along with the classical boolean operators $\lnot$ *not*, $\land$ *and* and $\lor$ *or*. These are defined the following way :

* $\lnot \top = \bot$
* $a \land b \models (a = b = \top)$
* $\lnot(a \lor b) \models (a = b = \bot)$

With $a \models b$ being the logical implication also called entailement.

#### Set theory

We also need to define the notion of set. At the begining of his funding work on set theory, Cantor wrote:

> _A set is a gathering together into a whole of definite, distinct objects of our perception or of our thought—which are called elements of the set._ -- George @cantor_beitrage_1895

We then define a set the following way:

::: {.definition #def:set name="Set"}
A collection of *distinct* objects, considered as an object in its own right. We define a set one of two way (always using braces):

* In extension by listing all the elements in the set: $\{1,2,4\}$
* In intension by specifying the rule that all elements follow: $\{n: n\in\mathbb{N} \land (n\mod2=0)\}$
:::

This definition alone gives some properties of sets. By being able to distinguish elements in the set from one another we assert that elements have an identity and we can derive equality from there. We can also give the empty set $\emptyset$ as the set with no elements. Since two sets are equals if and only if they have precisely the same elements, the empty set is unique. The member relation is noted $e \in E$ to indicate that $e$ is an element of $E$.
We note $S \subset G : (e \in S \models e\in G) \land S \neq G$, that a set $S$ is a proper subset of a more general set $G$.

We also define the union and intersection as following:

* $E \cup F : \{e : e \in E \lor e \in F \}$
* $E \cap F : \{e : e \in E \land e \in F \}$

**TODO**: Axiom of fundation

#### Description Logics

On of the most standard and flexible way of representing knowledge for databases is by using ontologies. They are based mostly on the formalism of Description Logics (DL). It is common when using DLs to store statements into three boxes [@baader_description_2003]: 

* The TBox for terminology (statements about types)
* The RBox for rules (statements about properties) [@burckert_terminologies_1994]
* The ABox for assertions (statements about individual entities)

These are used mostly to separate knowledge about general facts (intentional knowledge) from specific knowledge of individual instances (extensional knowledge).
The extra RBox is for "knowhow" or knowledge about entities behaviour. It restrict usages of roles (properties) in the ABox. The terminology is often hierarchically ordered using a subsumption relation noted $\prec$. If we represent classes or type as a set of individuals then this relation is akin to the subset relation of set theory.

**TODO**: More and some examples ?

#### Graphs

Next in line, we need to define a few notion of graph theory.

::: {.definition #def:graph name="Graph"}
A graph is a mathematical structure $g=(V,E)$ consisting of vertices $V$ (also called nodes) and edges $E$ (arcs) that links two vertices together. Each edge is basically a pair of vertices ordered or not depending if the the graph is directed or not.
:::

In the following, the signed symbols only applies to dirrected graphs.

We provide graphs with an adjacence function $\phi$ over any vertex $v \in V$ such that:

* $\phi(v) = \{ e \in E : v \in e \}$
* $\phi^+(v) = \{ \langle v, v' \rangle \in E : v' \in V \}$
* $\phi^-(v) = \{ \langle v', v \rangle \in E : v' \in V \}$

And an incidence function using the same name over any edges $e = \langle v, v' \rangle \in E$ such that:

* $\phi(e) =  \langle v, v' \rangle$
* $\phi^-(e) = v$
* $\phi^+(e) = v'$

Properties of graphs are better explained relative to their *transitive cover* $\chi^*$ of a graph $g = (V,E)$ defined as follow:

* $\chi(g) = (V,E') : E' = E \cup \{ \langle v_1, v_3 \rangle : \{ \langle v_1, v_2 \rangle, \langle v_2, v_3 \rangle \} \subset E \}$
* $\chi^2 = \chi \circ \chi$
* $\chi^n = \bigcirc_{i=0}^n \chi$
* $\chi^* = \bigcirc^{\infty} \chi$

Now we present two additional notions on graphs.

::: {.definition #def:path name="Path"}
We say that vertices $v_1$ and $v_2$ are *connected* if it exists a path from one to the other. Said otherwise, there is a path from $v_1$ to $v_2$ if and only if $\langle v_1, v_2 \rangle \in E_{\chi^*(g)}$.
:::

Similarly we define *cycles* as the existence of a path from a given vertex to itself. Some graphs can be strictly acyclical, enforcing the absence of cycles.

**TODO**: Needs diagrams

#### Hypergraphs

A generalization of graphs are **hypergraphs** where the edges are allowed to connect more than two vertices. An hypergraph is *$n$-uniform* when the edges are restricted to connect only $n$ vertices together. In that regard, classical graphs are 2-uniform hypergraphs.

**TODO**: Diagram of hypergraph

Another variant of graphs are graphs with the special case where $E \subset V$. This means that edges are allowed to connect to other edges. These kind of graphs are a generalization of hypergraphs. Informations about these kind of structures for knowledge reare hard to come by and rely mostly on a form of "folk wisdom" where knowledge is rarely published and mostly transmitted orally during lessons. One of the closest information is this forum post [@kovitz_terminology_2018] that associated this type of graph to port graphs [@silberschatz_port_1981]. 
Additional information was found in the form of a contribution of @vepstas_hypergraph_2008 on an encyclopedia article about hypergraphs. In that contribution, he says that a generalization of hypergraph allowing for edge-to-edge connections violates the axiom of fundation of ZFC by allowing edge-loops.

**TODO**: Diagram of edge-loops

This shows the limits of standard mathematics especially on the field of knowledge representation. Some strucutres needs higher dimensions than allowed by the one dimensional structure of ZFC and FOL.

However, it is important to not be mistaken: such non-standard set theories are more general than ZFC and therefore contains ZFC as a subset. All is a matter of restrictions.

#### Sheaf

In order to understand sheaves, we need to present a few auxiliary notions. Most of these definitions are adapted from [@vepstas_sheaves_2008]. The first of which is a seed.

::: {.definition #def:seed name="Seed"}
A seed correspond to a vertex along with the set of adjacent edges. Formally we note a seed $(v, \phi_g(v))$ that means that a seed build from vertex $v$ in the graph $g$ contains a set of adjacent edges $\phi_g(v)$. We call the vertex $v$ the *germ* of the seed.
:::

Now we can build a kind of partial graphs from seeds called sections.

::: {.definition #def:section name="Section"}
A section is a set of seeds that have their common edges connected. This means that if two seeds have an edge in common connecting both germs, then the seeds are connected in the section and the edges are merged.
:::

This tool was originally mostly meant for big data and categorization over large graphs. This is the reason for the next notion.

::: {.definition #def:quotient name="Graph Quotient"}
A quotient over a graph is the act of reducing a subgraph into a node while preserving the external connections. All internal structure becomes ignored and the subgraph now acts like a regular node.
:::

Porting that notion to sections instead of graphs allows us to define stalks.

::: {.definition #def:stalk name="Stalk"}
Given a projection $p:V \to V'$ over the germs of a section $s$, the stalk above the vertex $v' \in V'$ is the quotient of all seeds that have their germ follow $p(v) = v'$.
:::

Now we can add the final definition of sheaves.

::: {.definition #def:seaf name="Seaf"}
A seaf is a collection of sections, together with a projection function $p$ and gluing axioms that the projection should respect depending on the application.
:::

**TODO**: Diagrams and examples all the way.

### Grammar and Parsing

Grammar is an old tool that used to be dedicated to liguists. With the funding works of Chomsky and his Contex-Free Grammars (CFG), these tools became available to mathematicians and shortly after to computer scientists.

A CFG is a formal grammar that aims to generate a formal language given a set of hierarchical rules. Each rule is given a symbol as a name. From any finite input of text in a given alphabet, the grammar should be able to determine if the input is part of the language it generates.

In computer science, popular meta-language called BNF was created shortly after Chomsky's work on CFG. 
The syntax is of the following form :

```
<rule> ::= <other_rule> | <terminal_symbol> | "literals"
``` 

A terminal symbol is a rule that does not depend of any other rule. It is possible to use recursion, meaning that a rule will use itself in its definition. This actually allows for infinite languages. Despite its expressive power, BNF is often used in one of its extended form.

In this context, we present a widely used form of BNF syntax that is meant to be human readable despite not being very formal. We add the repetition operators `*` and `+` that respectively repeat 0 and 1 times or more the preceeding expression. We also add the negation operator `~` that matches only if the following expression does not match. We also add parentesis for grouping expression and brackets to group literals.

A regular grammar is static, it is set once and for all and will always produce the same language. In order to be more flexible we need to talk about dynamic grammars and their associated tools.

One of the main tool for both static and dynamic grammar, is a parser. It is the program that will interprete the input into whatever usage it is meant for. Most of the time, a parser will transform the input into another similarily structured language. It can be a storage inside objects or memory, or compiled into another format, or even just for syntax coloration. Since a lot of usage requires the same kind of function, a new kind of tool emerged to make the creation of a parser simpler. We call those tools parser or compiler generators [@paulson_semanticsdirected_1982]. They take a grammar description as input and gives the program of a parser of the generated language as an output.

For dynamic grammar, these tools can get more complicated. There are a few ways a grammar can become dynamic. The most straight-forward way to make a parser dynamic is to introduce code in the rule handling that will tweak variables affecting the parser itself [@souto_dynamic_1998]. This allows for handling context in CFG without needing to rewrite the grammar.

Another kind of dynamic grammar are grammar that can modify themselves. In order to do this a grammar is valuated with reified objects representing parts of itself [@hutton_monadic_1996]. These parts can be modified dynamically by rules as the input gets parsed [@renggli_practical_2010; @alessandro_ometa_2007]. This approach uses Parsing Expression Grammars (PEG)[@ford_parsing_2004] with Packrat parsing that Packrat parsing backtracks by ensuring that each production rule in the grammar is not tested more than once against each position in the input stream [@ford_packrat_2002]. While PEG are easier to implement and more efficient in practice than their classical conterparts [@loff_computational_2018; @henglein_peg_2017], it offset the computation load in memory making it actually less efficient in general [@becket_dcgs_2008].

Some tools actually just infer entire grammars from inputs and softwares [@hoschele_mining_2017; @grunwald_minimum_1996]. However, these kind of approaches require a lot of input data to perform well. They also simply provide the grammar after expensive computations.

### Ontologies and their Languages

Most AI problems needs a way to represent data. The classical way to represent knowledge has been more and more specialized for each AI community. Each their Domain Specific Language (DSL) that neatly fit the specific use it is intended to do. There was a time when the branch of AI wanted to unify knowledge description under the banner of the "semantic web". Today this effort led to a way to standardize web service description but haven't reached the consensus the founders had hoped for. From numerous works, a repeated limitation of the "semantic web" seams to come from the languages used. In order to guarantee performance of generalist inference engines, these languages have been restricted so much that they became quite complicated to use and quickly cause huge amounts of recurrent data to be stored because of some forbidden representation that will push any generalist inference engine into undecidability.

**TODO**: OWL is more expressive ?!? [@vanharmelen_handbook_2008 p850]

The most basic of these language is perhaps RDF Turtle [@beckett_turtle_2011]. It is based on tripples with an XML syntax and has a graph as its knowledge structure [@klyne_resource_2004]. A RDF graph is a set of RDF triples $\langle s, p, o \rangle$ which fields are respectively called subject, property and object. It can also be seen as a partially labeled directed graph $(N, E)$ with $N$ being the set of RDF nodes and $E$ being the set edges. This graph also comes with an incomplete $label : (N \cup E) \to URI$ relation. Nodes without an URI are called blank nodes. It is important that, while not named, blank nodes have a distinct internal identifier from one another that allows to differentiate them.

Inference in RDF has been shown to be trivially undecidable [@motik_properties_2007]. However, this language is probably the most permissive and therefore expressive language of the "semantic web". Even with this expressivity, several works still deems it as not expressive enough, mostly due to the lack of classical constructs like lists, parameters and quantifiers that don't fit the triple representation of RDF.

One of the ways which have been explored to overcome these limitations is by adding a 4^th^ field in RDF. This field is meant for context and anotations. This field is used for informations about any statement represented as a triple, such as access rights, belief and probabilities, or most of the time the source of the data [@tolksdorf_semantic_2004]. One of the other use of the fourth field of RDF is to reify statements [@hernandez_reifying_2015]. Indeed by identifying each statement, it becomes possible to efficiently for statements about statements.

A completely different approach is done by @hart_opencog_2008 in his framework for Artificial General Intelligence (AGI) called OpenCog. The structure of the knowledge is based on a rhizome, collection of trees, linked to one another. This structure is called Atomspace. Each vertex in the tree is an atom, leaf-vertexes are nodes, the others are links. Atoms are immutable, indexed objects. They can be given values that can be dynamical and, since they are not part of the rhizome, are an order of magnitude faster to access. Atoms and values alike are typed.

The goal of such a structure is to be able to merge concepts from widely different domains of AI. The major drawback being that the whole system is very slow compared to pretty much any domain specific software.


## WORLD

### Knowledge Structure

### Dynamic Grammar

### Contextual Interpretation

### Structure as a Definition

### Extended Inference Mechanisms

## Perpectives

### Literal definition using Peano's axioms

### Advanced Inference

