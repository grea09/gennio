% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
  11pt,
  a4paper,
  twoside,
  openright,
  titlepage,
  numbers=noenddot,
  headinclude,
  cleardoublepage=empty,
  openany,
  parskip]{scrreprt}
\usepackage{lmodern}
\usepackage{multicol}
\usepackage{amssymb,amsmath}
 %%%%% PANCAKE
	\usepackage{amsthm}
	\usepackage{mfirstuc}
	\theoremstyle{plain}
		\newtheorem{theorem}{\capitalisewords{theorem}}[]
			\newtheorem*{lemma}{\capitalisewords{lemma}}
		\newtheorem*{proposition}{\capitalisewords{proposition}}
		\newtheorem*{corollary}{\capitalisewords{corollary}}
		\theoremstyle{definition}
		\newtheorem{definition}{\capitalisewords{definition}}[]
		\newtheorem{conjecture}{\capitalisewords{conjecture}}[]
		\newtheorem{example}{\capitalisewords{example}}[]
		\newtheorem{postulate}{\capitalisewords{postulate}}[]
		\newtheorem{problem}{\capitalisewords{problem}}[]
			\newtheorem*{axiom}{\capitalisewords{axiom}}
		\theoremstyle{remark}
		\newtheorem{case}{\capitalisewords{case}}[]
			\newtheorem*{remark}{\capitalisewords{remark}}
		\newtheorem*{note}{\capitalisewords{note}}
	
 %%%%% PANCAKE
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  \setmainfont[]{Aller}
  \setmathfont[]{STIX Two Math}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{fancyvrb}
\usepackage{xcolor}
 %%%%% PANCAKE

\IfFileExists{xcolor-solarized.sty}{\usepackage{xcolor-solarized}} {
% We now define the sixteen \solarized{} colors.
\definecolor{solarized-base03} {RGB}{000, 043, 054}
\definecolor{solarized-base02} {RGB}{007, 054, 066}
\definecolor{solarized-base01} {RGB}{088, 110, 117}
\definecolor{solarized-base00} {RGB}{101, 123, 131}
\definecolor{solarized-base0}  {RGB}{131, 148, 150}
\definecolor{solarized-base1}  {RGB}{147, 161, 161}
\definecolor{solarized-base2}  {RGB}{238, 232, 213}
\definecolor{solarized-base3}  {RGB}{253, 246, 227}
\definecolor{solarized-yellow} {RGB}{181, 137, 000}
\definecolor{solarized-orange} {RGB}{203, 075, 022}
\definecolor{solarized-red}    {RGB}{220, 050, 047}
\definecolor{solarized-magenta}{RGB}{211, 054, 130}
\definecolor{solarized-violet} {RGB}{108, 113, 196}
\definecolor{solarized-blue}   {RGB}{038, 139, 210}
\definecolor{solarized-cyan}   {RGB}{042, 161, 152}
\definecolor{solarized-green}  {RGB}{133, 153, 000}
}
 %%%%% PANCAKE
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Title},
  pdfauthor={Antoine Gréa},
  colorlinks=true,
  linkcolor=solarized-cyan, %%%%% PANCAKE
  filecolor=solarized-magenta, %%%%% PANCAKE
  citecolor=solarized-base1, %%%%% PANCAKE
  urlcolor=solarized-blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
 %%%%% PANCAKE
  \usepackage{thmtools}
	\usepackage{nameref,cleveref}
		\crefname{section}{section}{sections}
		\crefname{table}{table}{tables}
		\crefname{listing}{listing}{listings}
		\crefname{theorem}{theorem}{theorems}
		\crefname{definition}{definition}{definitions}
		\crefname{proof}{proof}{proofs}
		\crefname{algorithm}{algorithm}{algorithms}
		\crefname{line}{line}{lines}
			\crefname{axiom}{axiom}{axioms}
	 %%%%% PANCAKE
\VerbatimFootnotes % allow verbatim text in footnotes
 %%%%% PANCAKE
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

%%%%%% for algorithms
\errorcontextlines\maxdimen
\makeatletter
\algrenewcommand\ALG@beginalgorithmic{\footnotesize}
%\algrenewcommand\alglinenumber[1]{\color{solarized-base0} #1}
\renewcommand{\algorithmiccomment}[1]{{\hfill\(\smalltriangleright\) \emph{#1}}}

\newcommand{\algorithmicbreak}{\textbf{break}}
\newcommand{\Break}{\State \algorithmicbreak}\newcommand{\algorithmiccontinue}{\textbf{continue}} 
\newcommand{\Continue}{\State \algorithmiccontinue}

% begin vertical rule patch for algorithmicx (http://tex.stackexchange.com/questions/144840/vertical-loop-block-lines-in-algorithmicx-with-noend-option)
% start with some helper code
% This is the vertical rule that is inserted
    \newcommand*{\algrule}[1][\algorithmicindent]{\makebox[#1][l]{\color{solarized-base0}\hspace*{.5em}\thealgruleextra\vrule height \thealgruleheight depth \thealgruledepth}}%
% its height and depth need to be adjustable
\newcommand*{\thealgruleextra}{}
\newcommand*{\thealgruleheight}{.95\baselineskip}
\newcommand*{\thealgruledepth}{.25\baselineskip}

\newcount\ALG@printindent@tempcnta
\def\ALG@printindent{%
    \ifnum \theALG@nested>0% is there anything to print
        \ifx\ALG@text\ALG@x@notext% is this an end group without any text?
            % do nothing
        \else
            \unskip
            \addvspace{-1pt}% FUDGE to make the rules line up
            % draw a rule for each indent level
            \ALG@printindent@tempcnta=1
            \loop
                \algrule[\csname ALG@ind@\the\ALG@printindent@tempcnta\endcsname]%
                \advance \ALG@printindent@tempcnta 1
            \ifnum \ALG@printindent@tempcnta<\numexpr\theALG@nested+1\relax% can't do <=, so add one to RHS and use < instead
            \repeat
        \fi
    \fi
    }%
\usepackage{etoolbox}
% the following line injects our new indent handling code in place of the default spacing
\patchcmd{\ALG@doentity}{\noindent\hskip\ALG@tlm}{\ALG@printindent}{}{\errmessage{failed to patch}}
\makeatother

%For nested Calls
\MakeRobust{\Call}

% the required height and depth are set by measuring the content to be shown
% this means that the content is processed twice
\newbox\statebox
\newcommand{\myState}[1]{%
    \setbox\statebox=\vbox{#1}%
    \edef\thealgruleheight{\dimexpr \the\ht\statebox+1pt\relax}%
    \edef\thealgruledepth{\dimexpr \the\dp\statebox+1pt\relax}%
    \ifdim\thealgruleheight<.75\baselineskip
        \def\thealgruleheight{\dimexpr .75\baselineskip+1pt\relax}%
    \fi
    \ifdim\thealgruledepth<.25\baselineskip
        \def\thealgruledepth{\dimexpr .25\baselineskip+1pt\relax}%
    \fi
    %\showboxdepth=100
    %\showboxbreadth=100
    %\showbox\statebox
    \State #1%
    %\State \usebox\statebox
    %\State \unvbox\statebox
    %reset in case the next command is not wrapped in \myState
    \def\thealgruleheight{\dimexpr .75\baselineskip+1pt\relax}%
    \def\thealgruledepth{\dimexpr .25\baselineskip+1pt\relax}%
}
% end vertical rule patch for algorithmicx
 %%%%% PANCAKE
 %%%%% PANCAKE
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{fontsize=\footnotesize,commandchars=\\\{\}}
\newenvironment{Shaded}{\linespread{1}}{}
\newcommand{\KeywordTok}[1]{\color{solarized-yellow}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\color{solarized-cyan}{{#1}}}
\newcommand{\DecValTok}[1]{\color{solarized-magenta}{{#1}}}
\newcommand{\BaseNTok}[1]{\color{solarized-magenta}{{#1}}}
\newcommand{\FloatTok}[1]{\color{solarized-magenta}{{#1}}}
\newcommand{\CharTok}[1]{\color{solarized-cyan}{{#1}}}
\newcommand{\StringTok}[1]{\color{solarized-cyan}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\color{solarized-violet}{\textbf{{#1}}}}
\newcommand{\CommentTok}[1]{\color{solarized-base0}{\textit{{#1}}}}
\newcommand{\OtherTok}[1]{\color{solarized-base00}{{#1}}}
\newcommand{\AlertTok}[1]{\color{solarized-orange}{\textbf{{#1}}}}
\newcommand{\FunctionTok}[1]{\color{solarized-green}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\ErrorTok}[1]{\color{solarized-red}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{\color{solarized-base02}{#1}}
\newcommand{\BuiltInTok}[1]{\color{solarized-blue}{\textbf{{#1}}}}

 %%%%% PANCAKE
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\makeatletter
\@ifpackageloaded{subfig}{}{\usepackage{subfig}}
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\captionsetup[subfloat]{margin=0.5em}
\AtBeginDocument{%
\renewcommand*\figurename{Figure}
\renewcommand*\tablename{Table}
}
\AtBeginDocument{%
\renewcommand*\listfigurename{List of Figures}
\renewcommand*\listtablename{List of Tables}
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\@ifpackageloaded{cleveref}{}{\usepackage{cleveref}}
\crefname{figure}{figure}{figures}
\Crefname{figure}{Figure}{Figures}
\crefname{table}{table}{tables}
\Crefname{table}{Table}{Tables}
\crefname{equation}{equation}{equations}
\Crefname{equation}{Equation}{Equations}
\crefname{listing}{listing}{listings}
\Crefname{listing}{Listing}{Listings}
\crefname{section}{section}{sections}
\Crefname{section}{Section}{Sections}
\crefname{codelisting}{\cref@listing@name}{\cref@listing@name@plural}
\Crefname{codelisting}{\Cref@listing@name}{\Cref@listing@name@plural}
\makeatother
\usepackage{tfrupee}
\DeclareFontFamily{U}{mathb}{\hyphenchar\font45}
\DeclareFontShape{U}{mathb}{m}{n}{<5> <6> <7> <8> <9> <10> gen * mathb <10.95> mathb10 <12> <14.4> <17.28> <20.74> <24.88> mathb12}{}
\DeclareSymbolFont{mathb}{U}{mathb}{m}{n}
\DeclareMathSymbol{\bigvarstar}{2}{mathb}{"0F}
\newfontfamily\arcfont[]{Arc}
\newfontfamily\lirisfont[]{LIRIS}
\newfontfamily\dinfont[]{DIN}
\newfontfamily\graphikregfont[]{Graphik LC Web Regular}
\newfontfamily\graphikmedfont[]{Graphik LC Web Medium}
\newfontfamily\lyondfont[]{Lyon2}
\newfontfamily\perihelionbbfont[]{PerihelionBB-Bold}
\newfontfamily\disp[]{Aller Display}
\newcommand{\arc}{\textcolor{solarized-violet}{\vspace{0pt}\arcfont arc 2} \includegraphics[height=2.8mm]{logos/arc2_pastille.pdf} }
\newcommand{\liris}{\textcolor{solarized-base02}{\lirisfont LỊRIS} }
\newcommand{\sma}{\textcolor{solarized-base02}{\disp sma} \vspace{0pt}\includegraphics[height=2.8mm]{logos/sma_pastille.pdf} }
\newcommand{\polytech}{\vspace{0pt}\includegraphics[height=2.8mm]{logos/polytech_pastille.pdf} \textcolor{solarized-base03}{ \perihelionbbfont POLYTECH} }
\newcommand{\ucbl}{\vspace{0pt}\includegraphics[height=4mm]{logos/lyon1_pastille.pdf}{\dinfont Lyon 1} }
\newcommand{\ara}{{\graphikregfont Auvergne-Rhône-Alpes} \vspace{0pt}\includegraphics[height=2.6mm]{logos/ara_pastille.pdf} }
\newcommand{\lumiere}{\textcolor{solarized-red}{\lyondfont lumière lyon 2} }
\newcommand\colonvdash{\mathrel{\ooalign{\hss$\vdash$\hss\cr\kern0.6ex\raise0.15ex\hbox{\scalebox{1}{$:$}}}}}
\usepackage{marginnote}
\newcounter{mynote}
\newcommand{\mynote}[1]{ \refstepcounter{mynote} \mbox{\textsuperscript{\themynote}} \marginnote{\mbox{\textsuperscript{\footnotesize{\themynote}}}#1}}
\renewcommand*{\footnote}{\marginnote}
\usepackage[ bottom = 3cm,]{geometry}
\newcommand{\bb}{\mathbb}
\renewcommand{\cal}{\mathcal}

\title{Title}
\author{Antoine Gréa}
\date{}

\ifdefined\and %%%%% PANCAKE
\else
  \newcommand{\and}{\quad}
\fi %%%%% PANCAKE
\begin{document}
\maketitle
 %%%%% PANCAKE
\begin{abstract}
\chapter*{Abstract}
\end{abstract}

 %%%%% PANCAKE

\renewcommand*\contentsname{Table of Content}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{4}
\tableofcontents
}
\hypertarget{acknowledgements}{%
\chapter*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{chapter}{Acknowledgements}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

In this section we present a quick guide to the presentation of the
information in this document. This will give its global structure and
each of the type of formating and its meaning.

\hypertarget{text-format}{%
\section*{Text format}\label{text-format}}
\addcontentsline{toc}{section}{Text format}

The text can be emphasized \textbf{more} or \emph{less} to make a key
word more noticeable.

\hypertarget{citations}{%
\section*{Citations}\label{citations}}
\addcontentsline{toc}{section}{Citations}

In text citations will be in this format: Author \emph{et al.}
(\href{https://citationstyles.org/}{year}) to make the author part of
the text and (Author \emph{et al.}
\href{https://citationstyles.org/}{year}) when simply referencing the
work.

\hypertarget{quotes}{%
\section*{Quotes}\label{quotes}}
\addcontentsline{toc}{section}{Quotes}

Sometimes, important quotes needs emphasis. They are presented as:

\begin{quote}
\emph{``Don't quote me on that !''}\footnote{Gréa
  (\href{antoine.grea.me}{2019})}
\end{quote}

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

\hypertarget{thesis-context}{%
\section{Thesis Context}\label{thesis-context}}

The work in this thesis was started in 2014, funded at the time by an
\arc  (Academic Research Community) research allocation courtesy of the
\ara region in France. This organism is acting toward an improvement of
the quality of life and of aging. Hence, the original subject was
oriented toward assisting dependent persons using intelligent robots.
However, such assistance is often rejected by these people who are also
very demanding on the social skills of caregivers. That's why the work
lays on the aspect of intent recognition and collaboration in assistive
robotics.

This work was done under the \ucbl university in the \liris (Laboratory
of computeR science in Image and Information Systems). I was supervised
by Samir Aknine and Laëtitia Matignon from the \sma team (Multi-Agent
System). Part of the work was funded by the university \lumiere and by
the \polytech engineering school.

\hypertarget{motivations}{%
\section{Motivations}\label{motivations}}

The social skills of modern robot are rather poor. Often, it is that
lack that inhibits human-robot communication and cooperation. Humans
being a social species, they require the use of implicit social cues in
order to interact comfortably with an interlocutor. The primary goal of
this thesis is to address this issue and create the formal foundations
of a system able to help dependent people.

In order to enhance assistance to dependent people, we need to account
for any deficiency they might have. An entire field of medicine and
computer sciences is already dedicated to helping with the physical
deficiencies. In our case we chose to address cognitive deficiencies.
The main one is anterograde amnesia and dementia caused by Alzheimer's
disease. In such cases, the patient is unable to express their needs.
That is a problem even with human caregivers as the information about
their intent needs to be inferred from their past actions.

\hypertarget{issues}{%
\section{Issues}\label{issues}}

This is the main issue addressed in the Artificial Intelligence (AI)
field of intent recognition. The problem is very simple: from
observational data, we need to infer the goals and intent of an external
agent. There are several ways to handle this issue. Most approaches use
trivial machine learning techniques like Bayesian Networks (BN) or
Markovian Decision Processes (MDP). The issue with these common
approaches is that they require an extensive amount of training data and
need to be trained on each patient. This makes the practicality of such
system quite limited. To address this issue, some works proposed hybrid
approaches using hand-made plan libraries or enforcing logical
constraints on the probabilistic method.

A work from Ramirez \textbf{CITATION}, added an interesting method to
solve that kind of problems. Indeed, they noticed an interesting
parallel of that problem with the field of automated planning. This
analogy was made by using the Theory of mind \textbf{CITATION}, that
states that any agent will infer the intent of other agents using a
projection of their own expectation on the observed behavior of the
external agent.\hypertarget{}{%
\marginpar{%
\includegraphics{/tmp/tmpj_svqy0v}

}}

This made the use of planner possible to infer intent without the need
for extensive and well-crafted plan libraries. Now only the domain of
the possible actions and their effects and prerequisites is needed to
infer the logical intent of an agent.

The main issue of planning for that particular use is that of
computation time and search space sizes. This prevents most planner to
make any decision before the intent is already realized and therefore
being useless for assistance. This time constraint lead to the search of
a real-time planner algorithm that is also very expressive and flexible
to accommodate for dynamical changes.

\hypertarget{contributions}{%
\section{Contributions}\label{contributions}}

In order to achieve such a planner, the first step was to formalize what
is exactly needed to express a domain. Hierarchical and lenient forms of
plan ordering gave the most expressivity and flexibility but at the cost
of time performance. This is why, a new formalism of knowledge
representation was needed in order to increase the speed of the search
space exploration while restricting it using semantic inference rules.

While searching for a knowledge representation system, some prototypes
were done using standard ontology tools but all proved to be way too
slow and inexpressive for that application. This made the creation of a
lighter but more flexible knowledge representation system, a requirement
of the project.

Then the planning formalism was to be converted into our general
knowledge representation tool. Since automated planning have a very
diverse ecosystem of approaches and paradigms, it was also devoid of a
universal standard formalism. A new formalism is proposed and compared
to the one used in some part of the planning community.

Then finally, a couple of planner were conceived to attempt answering
the speed and flexibility requirement of human intent recognition. The
first one is a prototype that aims to evaluate the advantages of
repairing plans using several heuristics. The second is a more complete
prototype derived from the first (without plan repair), that also
implement a Breadth-First Search (BFS) approach to hierarchical
decomposition of composite actions. This allows the algorithm to provide
intermediary plan that, while incomplete, are an abstraction of the
result plan. This allows for anytime probability computation using
existing techniques of invert planning.

\hypertarget{plan}{%
\section{Plan}\label{plan}}

\hypertarget{fondation-and-tools}{%
\chapter{Fondation and Tools}\label{fondation-and-tools}}

Mathematics and logic are at the heart of all formal sciences, including
computer science. Some of the most important problems in mathematics are
the consistency and formalization issues. Research on these issues
starts at the end of the 19\textsuperscript{th} century, with Cantor
inventing set theory (Cantor
\protect\hyperlink{ref-cantor_property_1874}{1874}). Then after a crisis
in the beginning of the 20\textsuperscript{th} century with Russel's
paradox and Gödel's incompletude theorem, revised versions of the set
theory become one of the foundations of mathematics. The most accepted
version is the Zermelo-Fraenkel axiomatic set theory with the axiom of
Choice (ZFC) (Fraenkel \emph{et al.}
\protect\hyperlink{ref-fraenkel_foundations_1973}{1973}, vol. 67;
Ciesielski \protect\hyperlink{ref-ciesielski_set_1997}{1997}). This
effort leads to a formalization of mathematics itself, at least to a
certain degree.

Any knowledge must be expressed using a medium like language. Natural
languages are quite expressive and allows for complex abstract ideas to
be communicated between individual. However, in science we encounter the
first issues with such a language. It is culturally biaised and
improperly convey formal notions and proof constructs. This is the
conclusion of Korzybski
(\protect\hyperlink{ref-korzybski_science_1958}{1958}) when trying to
pinpoint some of the main sources of error in discourse. There is a
decrepancy between the natural language and the underlying structure of
the reality. This issue is exacerbated in mathematics as the ambiguity
in the definition of a term can be the cause for a contradiction and
make the entire theory inconsistent.

\begin{quote}
\emph{``Mathematics may be defined as the subject in which we never know
what we are talking about, nor whether what we are saying is
true.''}\footnote{Russell
  (\protect\hyperlink{ref-russell_mysticism_1917}{1917})}
\end{quote}

In this first chapter we analyse the issue and propose our solution.
Then we use our fondation to build some important mathematical notions
used later to formalize data structures and algorithms.

\hypertarget{properties}{%
\section{Properties}\label{properties}}

From this point we can apply some of these ideas to mathematics to
analyse what properties a fondation of mathematics must hold.

\hypertarget{abstraction}{%
\subsection{Abstraction}\label{abstraction}}

\begin{quote}
\textbf{abstraction} (n.d.): \emph{The process of formulating
generalized ideas or concepts by extracting common qualities from
specific examples}\footnote{Collins English Dictionary
  (\protect\hyperlink{ref-collinsenglishdictionary_abstraction_2014}{2014})}
\end{quote}

The idea behind abstraction is to simplify the representation of
(potentially infinitely) complex instances. This mechanism is at the
base of any knowledge representation system. Indeed, it is unecessarily
expensive to try to represent all properties of an object. An efficient
way to reduce that knowledge representation is to prune away all
irrelevent properties while also only keeping the one that will be used
in the context. \emph{This means that abstraction is a loosy process}.
Information is lost when abstracting from an object.

Since this is done using a language as a medium, this language is a
\emph{host language}. Abstraction will refer to an instance using a
\emph{term} (also called symbol) of the host language. Since abstraction
is a generalization process, if the host language is expressive enough,
abstraction can be applied to already abstracted knowledge. The number
of underlying abstraction needed for a term is called its
\emph{abstraction level}. Very general notions have a higher abstraction
level and we usually represent reality using the null abstraction level.
In practice abstraction uses terms of the host language to bind to a
referenced instance in a lower abstraction level. This forms a structure
that is strongly hierarchical with higher abstraction level terms on
top.

\emph{Example}: We can describe an individual organism with a name that
is associated to this specific individual. If we name a dog ``Rex'' we
abstract a lot of information about a complex dynamical living being. We
can also abstract from a set of qualities of the specimen to build
higher abstraction. For example its species would be \emph{Canis lupus
familiaris} from the \emph{Canidae} family. Sometimes several terms can
be used at similar abstraction level like the commonly used denomination
``dog'' in this case.

Terms are only a part of that structure. It is possible to combine
several terms into a \emph{formula} (also called proposition or
statement).

\hypertarget{formalization}{%
\subsection{Formalization}\label{formalization}}

\begin{quote}
\textbf{formal} (adj.): \emph{Relating to or involving outward form or
structure, often in contrast to content or meaning.}\footnote{American
  Heritage Dictionary
  (\protect\hyperlink{ref-americanheritagedictionary_formal_2011}{2011}\protect\hyperlink{ref-americanheritagedictionary_formal_2011}{a})}
\end{quote}

Formalization is the act to make formal. The word ``formal'' comes from
Latin \emph{fōrmālis}, from \emph{fōrma}, meaning form, shape or
structure. This is the same base as for the word ``formula''. In
mathematics and \emph{formal sciences} the act of formalization is to
reduce knowledge down to formula. Like stated previously, a formula
combines several terms. But a formula must follow rules at different
levels:

\begin{itemize}
\tightlist
\item
  \emph{Lexical} by using terms belonging in the host language.
\item
  \emph{Syntaxic} as it must follow the grammar of the host language.
\item
  \emph{Semantic} as it must be internally consistant and meaningful.
\end{itemize}

The information conveied from a formula can be reduced to one element:
its semantic structure. Like its ethymology suggest a formula is simply
a structured statement about terms. This structure holds its meaning.
Along with using abstraction, it becomes possible to abstract a formula
and to therefore make a formula about other formulae should the host
language allowing it.

\emph{Example}: The formula using English ``dog is man's best friend''
combines terms to hold a structure between words. It is lexically
correct since it uses English words and gramatically correct since it
can be grammatically decomposed as (n.~v. n.~p.~adj. n.). In that the
(n.) stands for noumn, (v.) for verb, (adj.) for adjective and (p.) for
possessive. Since the verb ``is'' is the third person singular present
indicative of ``be'', and the adjective is the superlative of ``good'',
this form is correct in the English language. From there the semantic
aspect is correct too but that is too subjective and extensive to
formalize here. We can also build a formula about a formula like ``this
is a common phrase'' using the referencial pronoun ``this'' to refer to
the previous formula.

However, there is a strong limitation of a formalization. Indeed, a
complete formalization cannot occur about the host language. It is
possible to express formulae about the host language but \emph{it is
impossible to completely describe the host language using itself}. This
comes from two principal reasons. As abstraction is a loosy process one
cannot completely describe a language that can evolve and be infinitely
complex. And even when taking a simple language, there is a problem with
describing the different levels of rules to process the language using
itself. This supposes of a knowledge of the language \emph{a priori}.
This is contradictory and therefore impossible to achieve.

When abstracting a term, it may be useful to add informations about the
term to define it properly. That is why most formal system require a
\emph{definition} of each term using a formula. This definition is the
main piece of semantic information on a term and is used when needing to
evaluate a term in a different abstraction level. However, this is
causing yet another problem.

\hypertarget{circularity}{%
\subsection{Circularity}\label{circularity}}

\begin{quote}
\textbf{circularity} (n.d.): \emph{Defining one word in terms of another
that is itself defined in terms of the first word.}\footnote{American
  Heritage Dictionary
  (\protect\hyperlink{ref-americanheritagedictionary_circularity_2011}{2011}\protect\hyperlink{ref-americanheritagedictionary_circularity_2011}{b})}
\end{quote}

Defining a term requires using a formula in the host language to express
the abstracted properties of the generalization. The problem is that
most terms will have \emph{circular} definitions. Like stated this means
that every knowledge system will have some circularity in their
definitions. This means that it is impossible to have a complete
definition of a formal system without needing another formal system to
describe its base.

\emph{Example}: Using definitions from the American Heritage Dictionary
(\protect\hyperlink{ref-americanheritagedictionary_circularity_2011}{2011}\protect\hyperlink{ref-americanheritagedictionary_circularity_2011}{b}),
we can find that the term ``word'' is defined using the word ``meaning''
that is in turn defined using the term ``word''. Such circularity can
happen using an arbitrarily long chain of definition that will form a
cycle in the dependancies.

This problem is very important as it is overlooked in most fondations of
mathematics. This also makes Russel's paradox unavoidable in such
systems since there are always an infinite descent in the definition
relation. Also, since a formalization cannot fully be self defined,
another host language is generally used, sometimes without being
acknoledged. This causes cycles in the dependancies of languages and
theories in mathematics.

The only practical way to make some of this circularity disapear is to
base a fondation of mathematics using natural language as host language
for defining the most basic terms. This allows to aknowledge the problem
in an instinctive way while being aware of it while building the theory.

\hypertarget{fonctional-theory-of-mathematics}{%
\section{Fonctional theory of
mathematics}\label{fonctional-theory-of-mathematics}}

Using that last approach, we aim to find the smalest possible set of
axioms allowing to describe a fondation of mathematics. The following
theory is a proposition for a possible fondation that takes into account
the previously described constraints.

\hypertarget{tbl:functions}{}
\begin{table}\footnotesize
\centering

\caption{\label{tbl:functions}List of classical symbols for functions.}

\begin{tabular}{@{}ll@{}}
\toprule

\textbf{Symbol} & \textbf{Description} \\\midrule

\(\gtrdot\) & Null function. \\
\(=, \neq\) & Equal and not equal. \\
\(\to\) Mapp & ing function. \\
\(\bowtie, \vartriangle, \triangledown\) & Combination, superposition
and complement functions. \\
\(\circ, \bullet\) & Composition and inverse functions. \\
\(|f|\) & Arity (or order) of a functions. \\

\bottomrule
\end{tabular}

\end{table}

In this part, as an introduction to the fundamentals of maths and logic,
we propose another view of that fondation based on functions. The unique
advantage of it lays in its explicit structure that have emergent
reflexive properties. It also holds a coherent algebra that have a
strong expressive power. This approach is loosly based on category
theory. However it differs in its priorities and formulation.

Our theory is axiomatic, meaning it is based on fundamental logical
proposition called axioms. Those are accepted without any need for
proof. The following axiom is the mandatory undefinable notion created
from natural language. It is the explicit base of the formalism.

\begin{axiom}[Function]\label[axiom]{axi:function}

Lets be functions that associates a unique function to each function.

\end{axiom}

That axioms states that the formalism is based on \emph{functions}. Any
function can be used in anyway as long as it has a single argument and
value which are also functions.

Next we need to lay off the base definitions of the formalism.

\hypertarget{formalism-definition}{%
\subsection{Formalism definition}\label{formalism-definition}}

This functional algebra at the base of our fondation is inspired by
\emph{operator algebra}. The problem with the operator algebra is that
it supposes vectors and real numbers to work properly.

Since any formalism has circular definition we will have two way of
definining each notions: using natural language as host language or by
using notions that will be defines later. The main definitions we will
express are using natural language but we'll add the formula using
future notion as side notes.

Here we define the basic notions of our functional algebra that dictates
the rules of the formalism we are defining.

\begin{definition}[Application]\label[definition]{def:application}

When using a function \(f\) on an argument \(x\), the application of
\(f(x)\) gives the function \(y\) which is the value that \(f\)
associates with \(x\)\footnote{\(y = f(x)\)}.

\end{definition}

We call \emph{partial application} the application using an insufficient
number of arguments to any function \(f\). This results in a function
that takes less arguments with the first ones being fixed by the partial
application.

\textbf{TODO}: Example or formalization

The next few definition is required to have a valid syntax for formal
function definitions.

\begin{definition}[Currying]\label[definition]{def:currying}

Currying is the opperation named after the mathematician Haskell Brooks
Curry that allows multiple arguments functions in a simpler monoidal
formalism. A monome is a function that takes only one argument and has
only one value, as in our main axiom.

The opperation of currying is a function \(c\) that associates to each
function \(f\) another function that recursively partially applies \(f\)
with one argument at a time.\footnote{\(c = f \to (x \to c(f(x)))\)}

If we take a function \(h\) such that when applied to \(x\) gives the
function \(g\) that takes an argument \(y\), we can create a function
\(f\) by \emph{uncurrying} \(h\) and \(g\) so that \(f(x,y)\) behaves
the same way as \(g(y)\) and \(h(x)(y)\)\footnote{\(f(x, y) = h(x)(y)\)
  and \(g = y \to (x \to y)\)}. The function \(c\) associates \(f\) to
\(h\).

\end{definition}

From now on we will note \(f(x, y, z, …)\) any function that takes
multiple arguments but will suppose that they are implicitly curryied.
If a function only takes two arguments, we can also use the infix
notation e.g.~\(x f y\) for its application.

\begin{definition}[Mapping]\label[definition]{def:mapping}

Lets be the mapping function \(\to\) such that, for any two function
\(x\) and \(f(x)\), \(x \to f(x)\) has \(f\) as value.\footnote{\(f = x \to f(x)\)}

\end{definition}

\begin{definition}[Null]\label[definition]{def:null}

The \emph{null function} is the function between nothing and nothing. We
note it \(\gtrdot\). It is also recursively defined as
\(\gtrdot \to \gtrdot\).\footnote{\(\gtrdot = \gtrdot \to \gtrdot\)}

\end{definition}

\begin{definition}[Identity]\label[definition]{def:identity}

The \emph{identity function} is the function that associates any
function to itself noted \(=\).\footnote{\(= = x \to x\)}

\end{definition}

\hypertarget{literals-and-variables}{%
\subsection{Literals and Variables}\label{literals-and-variables}}

In our formalism, we use the null function to define notions of
variables and literals.

\begin{definition}[Literal]\label[definition]{def:literal}

A literal is a function that associates nothing with itself. This
consists of any fonction \(l\) writen as \(\gtrdot \to l\).\footnote{\(l = \gtrdot \to l\)}
This means that the function have only itself as an immutable value.

\end{definition}

A good example of that would be the yet to be defined natural numbers.
We can define the literal \(3\) as \(3 = \gtrdot \to 3\). We call
\emph{constants} function that have no arguments and have as value
either another constant or a literal.

\begin{definition}[Variable]\label[definition]{def:variable}

A variable is a function that associates itself to nothing. This
consists of any fonction \(x\) writen as \(x \to \gtrdot\).\footnote{\(x = x \to \gtrdot\)}
This means that the function have only itself as an immutable value.

\end{definition}

This definition means that a variable requires an input that can then be
hold as a possible value when evaluating.

An interesting property of this notation is that \(\gtrdot\) is both a
variable and a constant. When defining currying, we annotated with the
formalism \(c = f \to (x \to c(f(x)))\). The obvious issue is the
absence of stopping condition in that recursive expression. While the
end of the recursion doesn't technically happens, in practice from the
way variables and literals are defined, the recursion chain either ends
up becoming a variable or a constant. Also when currying the definition
of \(\gtrdot\) we obtains an interesting function.

\begin{definition}[Valuation]\label[definition]{def:valuation}

The valuation function turns any formula of function into its simplest
value. We note the valuation of a function \([[x]]\) and define it as
\(x \to (\gtrdot \to x)\).\footnote{\([[]] = x \to (\gtrdot \to x)\)}.

\end{definition}

If we compute its \emph{inverse} (see \textbf{LATER}) it becomes the
\emph{lifting function} (notted \(]][[\)) that turns any function into a
variable. The currying of the definition of \(\gtrdot\) can be noted as
\(]][[\gtrdot]][[\).

\hypertarget{functional-algebra}{%
\subsection{Functional algebra}\label{functional-algebra}}

Inspired by relational algebra and by category theory, we present an
highly expressive and compact functional algebra. In the
\cref{fig:function} we illustrate the different operators of this
algebra and their properties.

\begin{figure}
\hypertarget{fig:function}{%
\centering
\includegraphics{graphics/todo.svg}
\caption{Illustration of basic functionnal operators and their
properties.}\label{fig:function}
}
\end{figure}

The first operator of this algebra allows to combine several functions
into one.

\begin{definition}[Combination]\label[definition]{def:combination}

The \emph{combination function} is the function that associates any two
function \(f_1\) and \(f_2\) to a new function that associates any
functions associated by \textbf{either} functions.\footnote{\(f_1 \bowtie f_2 = x \to y\)
  with \(f_1(x) = y\) \textbf{or} \(f_2(x) = y\).}

\end{definition}

\textbf{TODO} Example

\begin{definition}[Superposition]\label[definition]{def:superposition}

The \emph{superposition function} is the function that associates any
two functions \(f_1\) and \(f_2\) to a new function that associates any
function associated by \textbf{both} functions.\footnote{\(f_1 \vartriangle f_2 = x \to y\)
  with \(f_1(x) = f_2(x) = y\).}

\end{definition}

\begin{definition}[Complement]\label[definition]{def:complement}

The \emph{complement function} is the function \(\triangledown\) that
associates any functions \(f\) to a new function \(\triangledown(f)\)
that associates any fonction \textbf{not} associated by the function
\(f\).\footnote{\(\triangledown f = x \to y\) with
  \(y \triangledown(=) x\).}

We can also use this function using the infix notation in the following
way : \(f_1 \triangledown f_2 = f_1 \vartriangle \triangledown(f_2)\) in
which case we call it \emph{subposition function}.

\end{definition}

From now on we will also note \(\neq = \triangledown(=)\). We can also
note a few properties of these functions:

\begin{itemize}
\tightlist
\item
  \(f \vartriangle f = f\)
\item
  \(f \vartriangle \gtrdot = \gtrdot\)
\item
  \(f_1 \vartriangle f_2 = r_2 \vartriangle f_1\)
\item
  \(f \triangledown f = \gtrdot\)
\item
  \(f \triangledown \gtrdot = f\)
\end{itemize}

Intuitively these fonctions are the fonctional equivalent of the union,
intersection and difference from set theory. In our formalism we will
define the set operations from these.

The following operators are the classical operations on functions.

\begin{definition}[Composition]\label[definition]{def:composition}

The \emph{composition function} is the function that associates any two
functions \(f_1\) and \(f_2\) to a new function such that:
\(f_1 \circ f_2 = x \to f_1(f_2(x))\).

\end{definition}

\begin{definition}[Inverse]\label[definition]{def:inverse}

The \emph{inverse function} is the function that associates any function
to its inverse such that if \(y = f(x)\) then \(x = \bullet(f)(y)\).

We can also use an infix version of it with the composition of
functions: \(f_1 \bullet f_2 = f_1 \circ \bullet(f_2)\).

\end{definition}

These properties are akin to multiplication and division in arithmetics.

\begin{itemize}
\tightlist
\item
  \(f \circ \gtrdot = \gtrdot\) (\(\gtrdot\) is the absorbing element)
\item
  \(f \circ = = f\) (\(=\) is the neutral element)
\item
  \(\bullet(\gtrdot) = \gtrdot\) and \(\bullet(=) = =\)
\item
  \(f_1 \circ f_2 \neq f_2 \circ f_1\)
\end{itemize}

From now on we will use numbers and classical arithmetics as we had
defined them. However, we consider defining them from a fondation point
of view, later using set theory and Peano's axioms.

In classical mathematics, the inferse of a fonction \(f\) is often wrote
as \(f^{-1}\). Therefore we can define the transitivity of the \(n\)th
degree as the power of a fonction such that:

\begin{itemize}
\tightlist
\item
  \(f^{-1} = \bullet f\)
\item
  \(f^0 = =\)
\item
  \(f^1 = f\)
\item
  \(f^n = f^{n-1} \circ f\)
\end{itemize}

We also call \emph{arity} the number of arguments (or the currying
order) of a function noted \(|f|\).

This fondation is now ready to define other fields of mathematics. We
start with logic as it is a very basic formalism in mathematics. Then we
redefine the ZFC set theory using our formalism as a base. And finally
we will present derived mathematical tools to represent data structures
and their properties.

\hypertarget{first-order-logic}{%
\section{First Order Logic}\label{first-order-logic}}

\hypertarget{tbl:logic}{}
\begin{table}\footnotesize
\centering

\caption{\label{tbl:logic}List of classical symbols for logic.}

\begin{tabular}{@{}ll@{}}
\toprule

\textbf{Symbol} & \textbf{Description} \\\midrule

\(e : ?(e)\) & The colon is a separator to be read as ``such that''.
Also used for typing. \\
\(\top, \bot\) & Top and bottom symbols used as true and false
respectively. \\
\(?(e)\) & Predicate over \(e\). \\
\(\lnot, \land, \lor, \veeonwedge\) & Negation (not), conjunction (and),
disjunction (logical or) and either. \\
\(\vdash\) & Entails, used for logical implication and consequence. \\
\(\forall, \exists, \exists!, \nexists, \textsection\) & Universal,
existential, uniqueness, exclusive and solution quantifiers. \\
\([?(e)]\) & Iverson's brackets: \([\bot]=0\) and \([\top]=1\). \\

\bottomrule
\end{tabular}

\end{table}

In this section, we present First Order Logic (FOL). All notations are
presented in \cref{tbl:logic}. FOL is based on boolean logic with the
two literals \(\top\) \emph{true} and \(\bot\) \emph{false}.

A function \(?\) that have as only values either \(\top\) or \(\bot\) is
called a \textbf{predicate}.\footnote{\(\cal{D}(\bullet ?) = \{\bot, \top\}\)}

We define the classical logic \emph{entailment} as
\(\vdash = (\bot, x \to \top) \bowtie (\top, x \to x)\)

Then we define the classical boolean operators \(\lnot\) \emph{not},
\(\land\) \emph{and} and \(\lor\) \emph{or} as:

\begin{itemize}
\tightlist
\item
  \(\lnot = (\bot \to \top) \bowtie (\top \to \bot)\)
\item
  \(\land = x \to ((\top \to x) \bowtie (\bot \to \bot))\), the
  conjonction is true when all its arguments are simultaneously true.
\item
  \(\lor = x \to ((\top \to \top) \bowtie (\bot \to x))\), the
  disjonction is true if all its arguments are not false.
\end{itemize}

The last two operators are curried function and can take any number of
arguments as necessary and recursively apply their definition.

Functions that takes an expression as parameter are called
\emph{modifiers}. FOL introduce a useful kind of modifer used to
modalize expressions: \emph{quantifiers}. Quantifiers takes an
expression and a variable as arguments. Classical quantifiers are also
predicates: they restrict the values that the variable can take.

The classical quantifiers are:

\begin{itemize}
\tightlist
\item
  The \emph{universal quantifier} \(\forall\) meaning \emph{``for
  all''}.\footnote{\(\textsection \forall = \triangledown \gtrdot\)}
\item
  The \emph{existential quantifier} \(\exists\) meaning \emph{``it
  exists''}.\footnote{\(\textsection \exists \neq \gtrdot\)}
\end{itemize}

They are sometimes extended with :

\begin{itemize}
\tightlist
\item
  The \emph{uniqueness quantifier} \(\exists!\) meaning \emph{``it
  exists a unique''}.\footnote{\(\textsection \exists!x = \{x\}\)}
\item
  The \emph{exclusive quantifier} \(\nexists\) meaning \emph{``it
  doesn't exist''}.\footnote{\(\textsection \nexists = \gtrdot\)}
\end{itemize}

An another exotic quantifier that isn't a predicate can be proven useful
(Hehner \protect\hyperlink{ref-hehner_practical_2012}{2012}):

\begin{itemize}
\tightlist
\item
  The \emph{solution quantifier} \(\textsection\) meaning
  \emph{``those''}.\footnote{\(\textsection x = x\)}
\end{itemize}

The last three quantifiers are optional in FOL but will be conducive
later on. It is interesting to note that most quantified expression can
be expressed using the set builder notation discussed in the following
section.

\hypertarget{set-theory}{%
\section{Set Theory}\label{set-theory}}

\hypertarget{tbl:set}{}
\begin{table}\footnotesize
\centering

\caption{\label{tbl:set}List of classical symbols and syntax for sets.}

\begin{tabular}{@{}ll@{}}
\toprule

\textbf{Symbol} & \textbf{Description} \\\midrule

\(\emptyset\) & Empty set, also noted \(\{\}\). \\
\(e \in \cal{S}\) & Element \(e\) is a member of set \(\cal{S}\). \\
\(\subset, \cup, \cap, \setminus, \times\) & Set inclusion, union,
intersection, difference and cartesian product. \\
\(|\cal{S}|\) & Cardinal (number of elements) of set \(\cal{S}\). \\
\(\{e : ?(e)\}\) & Set builder notation, set of all \(e\) such that
\(?(e)\) is true. \\
\(\wp(\cal{S})\) & Powerset: set of all subsets of \(\cal{S}\). \\

\bottomrule
\end{tabular}

\end{table}

Since we need to represent knowledge, we will handle more complex data
than simple booleans. At the beginning of his funding work on set
theory, Cantor wrote:

\begin{quote}
``\emph{A set is a gathering together into a whole of definite, distinct
objects of our perception or of our thought--which are called elements
of the set.}''\footnote{George Cantor
  (\protect\hyperlink{ref-cantor_beitrage_1895}{1895})}
\end{quote}

For Cantor, a set is a collection of concepts and percepts. In our case
both notions are grouped in what we call \emph{objects}, \emph{entities}
that are all ultimately \emph{functions} in our formalism.

\hypertarget{base-definitions}{%
\subsection{Base Definitions}\label{base-definitions}}

We define a set using the notations in \cref{tbl:set}.

\begin{definition}[Set]\label[definition]{def:set}

A collection of \emph{distinct} objects considered as an object in its
own right. We define a set one of two ways (always using braces):

\begin{itemize}
\tightlist
\item
  In extension by listing all the elements in the set: \(\{0,1,2,3,4\}\)
\item
  In intension by specifying the rule that all elements follow:
  \(\{n : ?(n)\}\)
\end{itemize}

\end{definition}

The function \(:\) is to be red as ``such as'' is used for the \emph{set
builder notation} (see \textbf{LATER}) and for typing (see
\textbf{LATER}).

\textbf{TODO}: Find a good formalization for it.

Using our fonctional fondation, we can define any set as a predicate
\(\cal{S} = e \to \top\) with \(e\) being a member of \(\cal{S}\). This
allows us to define the member function noted \(e \in \cal{S}\) to
indicate that \(e\) is an element of \(\cal{S}\).\footnote{\(\in = e, \cal{S} \to \cal{S}(e)\)}

Another, useful definition using sets is the \emph{domain} of a function
\(f\) as the set of all arguments for which the function is defined. We
call \emph{co-domain} the domain of the inverse of a function. We can
note them \(f: \cal{D}(f) \mapsto \cal{D}(\bullet f)\). In the case of
our functional version of sets, they are their own domain.

\hypertarget{set-operations}{%
\subsection{Set Operations}\label{set-operations}}

We note
\(\cal{S} \subset \cal{T} \vdash ((e \in \cal{S} \vdash e\in \cal{T}) \land \cal{S} \neq \cal{T})\),
that a set \(\cal{S}\) is a proper subset of a more general set
\(\cal{T}\).

We also define the union, intersection and difference as following:

\begin{itemize}
\tightlist
\item
  \(\cal{S} \cup \cal{T} = \{e : e \in \cal{S} \lor e \in \cal{T} \}\)
\item
  \(\cal{S} \cap \cal{T} = \{e : e \in \cal{S} \land e \in \cal{T} \}\)
\item
  \(\cal{S} \setminus \cal{T} = \{e : e \in \cal{S} \land e \notin \cal{T} \}\)
\end{itemize}

An interesting way to visualize relationsips with sets is by using Venn
diagrams. In \cref{fig:venn} we present the classical set operations.

\begin{figure}
\hypertarget{fig:venn}{%
\centering
\includegraphics{graphics/venn.svg}
\caption{Example of Venn diagram to illustrate operations on
sets.}\label{fig:venn}
}
\end{figure}

These diagrams have a lack of expressivity regarding complex operations
on sets. Indeed, from their plannar form it is complicated to express
numerous sets having intersection and disjunctions. One exemple is the
cartesian product that is defined as
\(\cal{S} \times \cal{T} = \{\langle e_{\cal{S}}, e_{\cal{T}} \rangle : e_{\cal{S}} \in \cal{S} \land e_{\cal{T}}\in \cal{T}\}\).
This is the set equivalent of currying as
\(\cal{S} \times \cal{T} = e_{\cal{S}}, e_{\cal{T}} \to \cal{S}(e_{\cal{S}}) \land \cal{T}(e_{\cal{T}})\).
The \(\langle\rangle\) notation is used for tuples, that are another
view on currying by replacing several arguments using a single one as an
ordered list.

From the cartesian product we can also define the set power recursively
by \(\cal{S}^1 = \cal{S}\) and
\(\cal{S}^n = \cal{S} \times \cal{S}^{n-1}\).

\hypertarget{the-zfc-theory}{%
\subsection{The ZFC Theory}\label{the-zfc-theory}}

The most common axiomatic set theory is ZFC. In that definition of sets
there are a few notions that comes from its axioms. By being able to
distinguish elements in the set from one another we assert that elements
have an identity and we can derive equality from there:

\begin{axiom}[Extensionality]\label[axiom]{axi:extensionality}

\(\forall\cal{S} \forall\cal{T} : \forall e((e\in\cal{S})=(e\in\cal{T})) \vdash \cal{S}=\cal{T}\)

\end{axiom}

Another axiom of ZFC that is crucial in avoiding Russel's paradox
(\(\cal{S} \in \cal{S}\)) is the following:

\begin{axiom}[Foundation]\label[axiom]{axi:fondation}

\(\forall \cal{S} : (\cal{S} \neq \emptyset \vdash \exists \cal{T}\in \cal{S},(\cal{T}\cap \cal{S}=\emptyset))\)

\end{axiom}

This axiom uses the empty set \(\emptyset\) (also noted \(\{\}\)) as the
set with no elements. Since two sets are equals if and only if they have
precisely the same elements, the empty set is unique.

The definition by intention uses the set builder notation to define a
set. It is composed of an expression and a predicate \(?\) that will
make any element \(e\) in a set \(\cal{T}\) satisfying it part of the
resulting set \(\cal{S}\), or as formulated in ZFC:

\begin{axiom}[Specification]\label[axiom]{axi:specification}

\(\forall ? \forall \cal{T} \exists \cal{S} : \left(\forall e \in \cal{S} : (e \in \cal{T} \land ?(e)) \right)\)

\end{axiom}

The last axiom of ZFC we use is to define the power set \(\wp(\cal{S})\)
as the set containing all subsets of a set \(\cal{S}\):

\begin{axiom}[Power set]\label[axiom]{axi:powerset}

\(\wp(\cal{S}) = \{\cal{T} : \cal{T} \subseteq \cal{S}\}\)

\end{axiom}

With the symbol
\(\cal{S} \subseteq \cal{T} \vdash (\cal{S} \subset \cal{T} \lor \cal{S} = \cal{T})\).
These symbols have an interesting property as they are often used as a
partial order over sets.

\hypertarget{graphs}{%
\section{Graphs}\label{graphs}}

\hypertarget{tbl:graph}{}
\begin{table}\footnotesize
\centering

\caption{\label{tbl:graph}List of classical symbols and syntax for
graphs.}

\begin{tabular}{@{}ll@{}}
\toprule

\textbf{Symbol} & \textbf{Description} \\\midrule

\(g=(V,E)\) & Graph \(g\) with set of vertices \(V\) and edges \(E\). \\
\(\phi^{\pm|n|*}(e|v)\) & Incidence (edge) and adjacence (vertex)
function for graphs: \\
\(\phi\) & • A tuple or set representing the edge or all adjacent edges
of a vertex. \\
\(\phi^-\) & • Source vertex (subject) or set of all incoming edges of a
vertex. \\
\(\phi^+\) & • Target vertex (object) or set of all outgoing edges of a
vertex. \\
\(\phi^0\) & • Label of edges and vertex : property of a statement or
cause of a causal link. \\
\(\chi(g)^+\) & Transitive closure of graph \(g\). \\
\(\div\) & Graph quotient. \\

\bottomrule
\end{tabular}

\end{table}

With set theory, it is possible to introduce all of standard
mathematics. A field of interest for this thesis is the study of the
structure of data. Most of these structures uses graphs and isomorphic
derivatives.

\begin{definition}[Graph]\label[definition]{def:graph}

A graph is a mathematical structure \(g\) which is defined by its
\emph{connectivity function} \(\chi\) that is a combination of the
classical adjacency and incidency functions of the graph. We can define
it by self reference in the following way:

\begin{itemize}
\tightlist
\item
  \emph{Adjacency}: \(\chi_a = v \to \{ e: v \in \chi(e) \}\)\footnote{Also:
    \(\chi_a = \bullet \chi_i\)}
\item
  \emph{Incidency}: \(\chi_i = e \to \{ v: e \in \chi(v) \}\)
\end{itemize}

\[\chi = \chi_a \bowtie \chi_i\]

We usually note a graph \(g=(V,E)\) with the set of vertices \(V\) (also
called nodes) and edges \(E\) (arcs) that links two vertices together.
Each edge is basically a pair of vertices ordered or not depending on if
the graph is directed or not.\footnote{\(E= \sigma\wp(V^2)\)} The
incidency can then be defined using the set definition of the edges:
\(\chi_i = e \to e\)

\end{definition}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{figure}
\hypertarget{fig:transitive}{%
\centering
\includegraphics{graphics/transitivity.svg}
\caption{Example of the recursive application of the transitive cover to
a graph.}\label{fig:transitive}
}
\end{figure}

A graph is often represented with lines or arrows linking points
together like illustrated in \cref{fig:transitive}. In that figure, the
vertices \(v_1\) and \(v_2\) are connected through an undirected edge.
Similarly \(v_3\) connects to \(v_4\) but not the opposite since they
are bonded with a directed edge. The vertex \(v_8\) is also connected to
itself.

From that definition, some other relations are needed to express most
properties of graphs. In the following, the signed symbol only applies
to directed graphs.

We provide graphs with an adjacence function \(\phi\) over any vertex
\(v \in V\) such that:

\begin{itemize}
\tightlist
\item
  \(\phi(v) = \{ e : e\in E \land v \in e \}\)
\item
  \(\phi^+(v) = \{ \langle v \rightarrow v' \rangle \in E : v' \in V \}\)
  and
  \(\phi^-(v) = \{ \langle v' \rightarrow v \rangle \in E : v' \in V \}\)
\end{itemize}

This relation gives the set of incoming or outgoing edges from any
vertex. In non directed graphs, the relation gives edges adjacent to the
vertex. For example: in \cref{fig:transitive},
\(\phi(v_1) = \{ \langle v_1, v_2 \rangle \}\). In that example, using
directed graph notation we can note
\(\phi^+(v_3) = \{ \langle v_3 \rightarrow v_4 \rangle \}\).

Using types, it is possible to reuse the same symbol to define an
incidence function over any edges \(e = \langle v, v' \rangle\) such
that:

\begin{itemize}
\tightlist
\item
  \(\phi(e) = \langle v, v' \rangle\)
\item
  \(\phi^-(e) = v\) and \(\phi^+(e) = v'\)
\end{itemize}

Most of the intrinsic information of a graph is contained within its
structure. Exploring its properties require to study the ``shape'' of a
graph and to find relationships between vertices. That is why graph
properties are easier to explain using the \emph{transitive cover}
\(\chi^+\) of any graph \(g = (V,e)\) defined as follows:

\begin{itemize}
\tightlist
\item
  \(\chi(g) = (V,e') : e' = e \cup \{ \langle v_1, v_3 \rangle : \{ \langle v_1, v_2 \rangle, \langle v_2, v_3 \rangle \} \subset e \}\)
\item
  \(\chi^+ = \chi^\infty\)
\end{itemize}

This transitive cover will create another graph in which two vertices
are connected through an edge if and only if it exists a path between
them in the original graph \(g\). We illustrate this process in
\cref{fig:transitive}. Note how there is no edge in \(\chi(g)\) between
\(v_5\) and \(v_6\) and the one in \(\chi^2(g)\) is directed towards
\(v_5\) because there is no path back to \(v_6\) since the edge between
\(v_3\) and \(v_4\) is directed.

\begin{definition}[Path]\label[definition]{def:path}

We say that vertices \(v_1\) and \(v_2\) are \emph{connected} if it
exists a path from one to the other. Said otherwise, there is a path
from \(v_1\) to \(v_2\) if and only if
\(\langle v_1, v_2 \rangle \in E_{\chi^+(g)}\).

\end{definition}

The notion of connection can be extended to entire graphs. An undirected
graph \(g\) is said to be \emph{connected} if and only if
\(\forall e \in V^2 ( e \in E_{\chi^+(g)})\).

Similarly we define \emph{cycles} as the existence of a path from a
given vertex to itself. For example, in \cref{fig:transitive}, the
cycles of the original graph are colored in blue. Some graphs can be
strictly acyclical, enforcing the absence of cycles.

A \textbf{tree} is a special case of a graph. A tree is an acyclical
connected graph. If a special vertex called a \emph{root} is chosen we
call the tree a \emph{rooted tree}. It can then be a directed graph with
all edge pointing away from the root. When progressing away from the
root, we call the current vertex \emph{parent} of all exterior
\emph{children} vertices. Vertex with no children are called
\emph{leaves} of the tree and the rest are called \emph{branches}.

An interesting application of trees to FOL is called \emph{and/or trees}
where each vertex has two sets of children: one for conjunction and the
other for disjunction. Each vertex is a logic formula and the leaves are
atomic logic propositions. This is often used for logic problem
reduction. In \cref{fig:andor} we illustrate how and/or trees are often
depicted.

\begin{figure}
\hypertarget{fig:andor}{%
\centering
\includegraphics{graphics/and-or.svg}
\caption{Example of and/or tree.}\label{fig:andor}
}
\end{figure}

Another notion often used for reducing big graphs is the quotiening as
illustrated in \cref{fig:quotient}.

\begin{definition}[Graph Quotient]\label[definition]{def:quotient}

A quotient over a graph is the act of reducing a subgraph into a node
while preserving the external connections. All internal structure
becomes ignored and the subgraph now acts like a regular node. We note
it \(\div_f(g)= (\pi_f(V), \{ \pi_f(e) : e \in E\})\) with \(f\) being a
function that maps any vertex either toward itself or toward its
quotiened vertex.

\end{definition}

We can also combine several graphs into one using fusion:
\(g_1 + g_2 = (V_1 \cup V_2, E_1 \cup E_2)\).

\begin{figure}
\hypertarget{fig:quotient}{%
\centering
\includegraphics{graphics/quotient.svg}
\caption{Example of graph quotient.}\label{fig:quotient}
}
\end{figure}

\hypertarget{hypergraphs}{%
\subsection{Hypergraphs}\label{hypergraphs}}

A generalization of graphs are \textbf{hypergraphs} where the edges are
allowed to connect to more than two vertices. They are often represented
using Venn-like representations but can also be represented with edges
``gluing'' several vertex like in \cref{fig:hypergraph}.

An hypergraph is said to be \emph{\(n\)-uniform} if the edges are
restricted to connect to only \(n\) vertices together. In that regard,
classical graphs are 2-uniform hypergraphs.

\begin{figure}
\hypertarget{fig:hypergraph}{%
\centering
\includegraphics{graphics/hypergraph.svg}
\caption{Example of hypergraph with total freedom on the edges
specification.}\label{fig:hypergraph}
}
\end{figure}

Hypergraphs have a special case where \(E \subset V\). This means that
edges are allowed to connect to other edges. In \cref{fig:hypergraph},
this is illustrated by the edge \(e_3\) connecting to three other edges.
Information about these kinds of structures for knowledge representation
is hard to come by and rely mostly on a form of ``folk wisdom'' within
the mathematics community where knowledge is rarely published and mostly
transmitted orally during lessons. One of the closest information
available is this forum post (Kovitz
\protect\hyperlink{ref-kovitz_terminology_2018}{2018}) that associated
this type of graph to port graphs (Silberschatz
\protect\hyperlink{ref-silberschatz_port_1981}{1981}). Additional
information was found in the form of a contribution of Vepstas
(\protect\hyperlink{ref-vepstas_hypergraph_2008}{2008}) on an
encyclopedia article about hypergraphs. In that contribution, he says
that a generalization of hypergraph allowing for edge-to-edge
connections violate the
\namecref{axi:fondation} of \nameref{axi:fondation} of ZFC by allowing
edge-loops. Indeed, like in \cref{fig:hypergraph}, an edge
\(e_9 = \{e_{10}\}\) can connect to another edge \(e_{10} = \{ e_9 \}\)
causing an infinite descent inside the \(\in\) relation in direct
contradiction with ZFC.

This shows the limits of standard mathematics especially on the field of
knowledge representation. Some structures needs higher dimensions than
allowed by the one-dimensional structure of ZFC and FOL. However, it is
important not to be mistaken: such non-standard set theories are more
general than ZFC and therefore contains ZFC as a special case. All is a
matter of restrictions.

\hypertarget{sheaf}{%
\section{Sheaf}\label{sheaf}}

\hypertarget{tbl:sheaf}{}
\begin{table}\footnotesize
\centering

\caption{\label{tbl:sheaf}List of symbols and syntax for sheaves.}

\begin{tabular}{@{}ll@{}}
\toprule

\textbf{Symbol} & \textbf{Description} \\\midrule

\(\textbullet, \varstar, \multimap\) & Germ, seed and connector. \\
\(\cal{F}\) & Sheaf (from French \emph{faisceau}). \\

\bottomrule
\end{tabular}

\end{table}

In order to understand sheaves, we need to present a few auxiliary
notions. Most of these definitions are adapted from (Vepštas
\protect\hyperlink{ref-vepstas_sheaves_2008}{2008}). The first of which
is a seed.

\begin{figure}
\hypertarget{fig:seed}{%
\centering
\includegraphics{graphics/seed_section_stalk.svg}
\caption{Example of a seed, a section and a stalk.}\label{fig:seed}
}
\end{figure}

\begin{definition}[Seed]\label[definition]{def:seed}

A seed corresponds to a vertex along with the set of adjacent edges.
Formally we note a seed
\(\varstar = (\textbullet, \phi_g(\textbullet))\) that means that a seed
build from the vertex \(\textbullet\) in the graph \(g\) contains a set
of adjacent edges \(\phi_g(\textbullet)\). We call the vertex
\(\textbullet\) the \emph{germ} of the seed. The edges in a seed does
not connect to the other vertices but keep the information and are able
to match the correct vertices through typing (often a type of a single
individual). We call the edges in a seed \emph{connectors}.

\end{definition}

Seeds are extracts of graphs that contains all information about a
vertex. Illustrated in the \cref{fig:seed}, seeds have a central germ
(represented with discs) and connectors leading to a typed vertex
(outlined circles). Those external vertices are not directly contained
in the seed but the information about what vertex can fit in them is
kept. It is useful to represent connectors like jigsaw puzzle pieces:
they can match only a restricted number of other pieces that match their
shape.

From there, it is useful to build a kind of partial graph from seeds
called sections.

\begin{definition}[Section]\label[definition]{def:section}

A section is a set of seeds that have their common edges connected. This
means that if two seeds have an edge in common connecting both germs,
then the seeds are connected in the section and the edges are merged. We
note \(g_\varstar = (●, ⌕)\) the graph formed by the section.

\end{definition}

In \cref{fig:seed}, a section is represented. It is a connected section
composed of seeds along with the additional seeds of any vertices they
have in common. They are very similar to subgraph but with an additional
border of typed connectors. This tool was originally mostly meant for
big data and categorization over large graphs. As graph quotient is
often used in that domain, it was ported to sections instead of graphs
allows us to define stalks.

\begin{definition}[Stalk]\label[definition]{def:stalk}

Given a projection function \(f:●\to ●'\) over the germs of a section
\(\varstar\), the stalk above the vertex \(\textbullet' \in ●'\) is the
quotient of all seeds that have their germ follow
\(f(\textbullet) = \textbullet'\).

\end{definition}

The quotienning is used in stalks for their projection. Indeed, as shown
in \cref{fig:seed}, the stalks are simply a collection of seeds with
their germs quotiened into their common projection. The projection can
be any process of transformation getting a set of seeds in one side and
gives object in any base space called the image. Sheaves are a
generalization of this concept to sections.

\begin{figure}
\hypertarget{fig:sheaf}{%
\centering
\includegraphics{graphics/sheaf.svg}
\caption{Example of sheaves.}\label{fig:sheaf}
}
\end{figure}

\begin{definition}[Sheaf]\label[definition]{def:sheaf}

A sheaf is a collection of sections, together with a projection. We note
it \(\cal{F} = \langle G_{\varstar}, \pi_g \rangle\) with the function
\(g\) being the gluing axioms that the projection should respect
depending on the application. The projected sheaf graph is noted
\(g_{\cal{F}} = \sum_{g_{\varstar} \in G_{\varstar}}\div_g(g_{\varstar})\)
as the fusion of all quotiened sections.

\end{definition}

By merging common vertices into a section, we can build stack fields.
These fields are simply a subcategory of sheaves. Illustrated in
\cref{fig:sheaf}, a sheaf is a set of section with a projection
relation.

\hypertarget{knowledge-representation}{%
\chapter{Knowledge Representation}\label{knowledge-representation}}

Knowledge representation is at the intersection of maths, logic,
language and computer sciences. Knowledge description systems rely on
syntax to interoperate systems and users to one another. The base of
such languages comes from the formalization of automated grammars by
Chomsky (\protect\hyperlink{ref-chomsky_three_1956}{1956}). It mostly
consists of a set of hierarchical rules aiming to deconstruct an input
string into a sequence of terminal symbols. This deconstruction is
called parsing and is a common operation in computer science. More tools
for the characterization of computer language emerged soon after thanks
to Backus (\protect\hyperlink{ref-backus_syntax_1959}{1959}) while
working on a programming language at IBM. This is how the Backus-Naur
Form (BNF) metalanguage was created on top of Chomsky's formalization.

A similar process happened in the 1970's, when logic based knowledge
representation gained popularity among computer scientists (Baader
\protect\hyperlink{ref-baader_description_2003}{2003}). Systems at the
time explored notions such as rules and networks to try and organize
knowledge into a rigorous structure. At the same time other systems were
built based on First Order Logic (FOL). Then, around the 1990's, the
research began to merge in search of common semantics in what led to the
development of Description Logics (DL). This domain is expressing
knowledge as a hierarchy of classes containing individuals.

From there and with the advent of the world wide web, engineers were on
the lookout for standardization and interoperability of computer
systems. One such standardization took the name of ``semantic web'' and
aimed to create a widespread network of connected services sharing
knowledge between one another in a common language. At the beginning of
the 21\textsuperscript{st} century, several languages were created, all
based on the World Wide Web Consortium (W3C) specifications called
Resource Description Framework (RDF) (Klyne and Carroll
\protect\hyperlink{ref-klyne_resource_2004}{2004}). This language is
based on the notion of statements as triples. Each can express a unit of
knowledge. All the underlying theoretical work of DL continued with it
and created more expressive derivatives. One such derivative is the
family of languages called Web Ontology Language (OWL) (Horrocks
\emph{et al.} \protect\hyperlink{ref-horrocks_shiq_2003}{2003}).
Ontologies and knowledge graphs are more recent names for the
representation and definition of categories (DL classes), properties and
relation between concepts, data and entities.

Nowadays, when designing a knowledge representation, one usually starts
with existing framework or generic knowledge database systems. The most
popular in practice is certainly the classical relational database,
followed closely by more novel methods for either big data or more
expressive solutions like ontologies.

In our case we need a tool that is more expressive than ontologies while
remaining efficient. Of course, this will lead to compromises, but can
also have some interesting properties.

\hypertarget{grammar-and-parsing}{%
\section{Grammar and Parsing}\label{grammar-and-parsing}}

Grammar is an old tool that used to be dedicated to linguists. With the
funding works by Chomsky and his Contex-Free Grammars (CFG), these tools
became available to mathematicians and shortly after to computer
scientists.

A CFG is a formal grammar that aims to generate a formal language given
a set of hierarchical rules. Each rule is given a symbol as a name. From
any finite input of text in a given alphabet, the grammar should be able
to determine if the input is part of the language it generates.

\hypertarget{bnf}{%
\subsection{BNF}\label{bnf}}

In computer science, popular metalanguage called BNF was created shortly
after Chomsky's work on CFG. The syntax is of the following form :

\begin{verbatim}
<rule> ::= <other_rule> | <terminal_symbol> | "literals"
\end{verbatim}

A terminal symbol is a rule that does not depend on any other rule. It
is possible to use recursion, meaning that a rule will use itself in its
definition. This actually allows for infinite languages. Despite its
expressive power, BNF is often used in one of its extended forms.

In this context, we present a widely used form of BNF syntax that is
meant to be human readable despite not being very formal. We add the
repetition operators \texttt{*} and \texttt{+} that respectively repeat
0 and 1 times or more the preceding expression. We also add the negation
operator \texttt{\textasciitilde{}} that matches only if the following
expression does not match. We also add parentheses for grouping
expression and brackets to group literals.

\hypertarget{dynamic-grammar}{%
\subsection{Dynamic Grammar}\label{dynamic-grammar}}

A regular grammar is static, it is set once and for all and will always
produce the same language. In order to be more flexible we need to talk
about dynamic grammars and their associated tools.

One of the main tools for both static and dynamic grammar is a parser.
It is the program that will interpret the input into whatever usage it
is meant for. Most of the time, a parser will transform the input into
another similarly structured language. It can be a storage inside
objects or memory, or compiled into another format, or even just for
syntax coloration. Since a lot of usage requires the same kind of
function, a new kind of tool emerged to make the creation of a parser
simpler. We call those tools parser or compiler generators (Paulson
\protect\hyperlink{ref-paulson_semanticsdirected_1982}{1982}). They take
a grammar description as input and gives the program of a parser of the
generated language as an output.

For dynamic grammar, these tools can get more complicated. There are a
few ways a grammar can become dynamic. The most straightforward way to
make a parser dynamic is to introduce code in the rule handling that
will tweak variables affecting the parser itself (Souto \emph{et al.}
\protect\hyperlink{ref-souto_dynamic_1998}{1998}). This allows for
handling context in CFG without needing to rewrite the grammar.

Another kind of dynamic grammar is grammar that can modify themselves.
In order to do this a grammar is valuated with reified objects
representing parts of itself (Hutton and Meijer
\protect\hyperlink{ref-hutton_monadic_1996}{1996}). These parts can be
modified dynamically by rules as the input gets parsed (Renggli \emph{et
al.} \protect\hyperlink{ref-renggli_practical_2010}{2010}; Alessandro
and Piumarta \protect\hyperlink{ref-alessandro_ometa_2007}{2007}). This
approach uses Parsing Expression Grammars (PEG)(Ford
\protect\hyperlink{ref-ford_parsing_2004}{2004}) with Packrat parsing
that Packrat parsing backtracks by ensuring that each production rule in
the grammar is not tested more than once against each position in the
input stream (Ford \protect\hyperlink{ref-ford_packrat_2002}{2002}).
While PEG is easier to implement and more efficient in practice than
their classical counterparts (Loff \emph{et al.}
\protect\hyperlink{ref-loff_computational_2018}{2018}; Henglein and
Rasmussen \protect\hyperlink{ref-henglein_peg_2017}{2017}), it offset
the computation load in memory making it actually less efficient in
general (Becket and Somogyi
\protect\hyperlink{ref-becket_dcgs_2008}{2008}).

Some tools actually just infer entire grammars from inputs and software
(Höschele and Zeller \protect\hyperlink{ref-hoschele_mining_2017}{2017};
Grünwald \protect\hyperlink{ref-grunwald_minimum_1996}{1996}). However,
these kinds of approaches require a lot of input data to perform well.
They also simply provide the grammar after expensive computations.

\hypertarget{description-logics}{%
\section{Description Logics}\label{description-logics}}

On of the most standard and flexible way of representing knowledge for
databases is by using ontologies. They are based mostly on the formalism
of Description Logics (DL). It is based on the notion of classes (or
types) as a way to make the knowledge hierarchically structured. A class
is a set of individuals that are called instances of the classes.
Classes got the same basic properties as sets but can also be
constrained with logic formula. Constraints can be on anything about the
class or its individuals. Knowledge is also encoded in relations that
are predicates over attributes of individuals.

It is common when using DLs to store statements into three boxes (Baader
\protect\hyperlink{ref-baader_description_2003}{2003}):

\begin{itemize}
\tightlist
\item
  The TBox for terminology (statements about types)
\item
  The RBox for rules (statements about properties) (Bürckert
  \protect\hyperlink{ref-burckert_terminologies_1994}{1994})
\item
  The ABox for assertions (statements about individual entities)
\end{itemize}

These are used mostly to separate knowledge about general facts
(intentional knowledge) from specific knowledge of individual instances
(extensional knowledge). The extra RBox is for ``knowhow'' or knowledge
about entity behavior. It restricts usages of roles (properties) in the
ABox. The terminology is often hierarchically ordered using a
subsumption relation noted \(\subseteq\). If we represent classes or
type as a set of individuals then this relation is akin to the subset
relation of set theory.

There are several versions and extensions of DL. They all vary in
expressivity. Improving the expressivity of DL system often comes at the
cost of less efficient inference engines that can even become
undecidable for some extensions of DL.

\hypertarget{ontologies-and-their-languages}{%
\section{Ontologies and their
Languages}\label{ontologies-and-their-languages}}

Most AI problem needs a way to represent data. The classical way to
represent knowledge has been more and more specialized for each AI
community. Each their Domain Specific Language (DSL) that neatly fit the
specific use it is intended to do. There was a time when the branch of
AI wanted to unify knowledge description under the banner of the
``semantic web''. From numerous works, a repeated limitation of the
``semantic web'' seems to come from the languages used. In order to
guarantee performance of generalist inference engines, these languages
have been restricted so much that they became quite complicated to use
and quickly cause huge amounts of recurrent data to be stored because of
some forbidden representation that will push any generalist inference
engine into undecidability.

The most basic of these languages is perhaps RDF Turtle (Beckett and
Berners-Lee \protect\hyperlink{ref-beckett_turtle_2011}{2011}). It is
based on triples with an XML syntax and has a graph as its knowledge
structure (Klyne and Carroll
\protect\hyperlink{ref-klyne_resource_2004}{2004}). A RDF graph is a set
of RDF triples \(\langle sub, pro, obj \rangle\) which fields are
respectively called subject, property and object. It can also be seen as
a partially labeled directed graph \((V, E)\) with \(V\) being the set
of RDF nodes and \(E\) being the set of edges. This graph also comes
with an incomplete label \(\phi^0 : (V \cup E) \to L_{String}^{URI}\)
relation. Nodes without an URI are called blank nodes. It is important
that, while not named, blank nodes have a distinct internal identifier
from one another that allows to differentiate them.

Built on top of RDF, the W3C recommended another standard called OWL. It
adds the ability to have hierarchical classes and properties along with
more advanced description of their arrity and constraints. OWL is, in a
way, more expressive than RDF (Van Harmelen \emph{et al.}
\protect\hyperlink{ref-vanharmelen_handbook_2008}{2008}, 1,p825). It
adds most formalism used in knowledge representation and is widely used
and interconnected. OWL comes in three versions: OWL Lite, OWL DL and
OWL Full. The lite version is less advanced but its inference is
decidable, OWL DL contains all notions of DL and the full version
contains all features of OWL but is strongly undecidable.

The expressivity can also come from a lack of restriction. If we allow
some freedom of expression in RDF statements, its inference can quickly
become undecidable (Motik
\protect\hyperlink{ref-motik_properties_2007}{2007}). This kind of
extremely permissive language is better suited for specific usage for
other branches of AI. Even with this expressivity, several works still
deem existing ontology system as not expressive enough, mostly due to
the lack of classical constructs like lists, parameters and quantifiers
that don't fit the triple representation of RDF.

One of the ways which have been explored to overcome these limitations
is by adding a 4\textsuperscript{th} field in RDF. This field is meant
for context and annotations. This field is used for information about
any statement represented as a triple, such as access rights, beliefs
and probabilities, or most of the time the source of the data (Tolksdorf
\emph{et al.} \protect\hyperlink{ref-tolksdorf_semantic_2004}{2004}).
One of the other uses of the fourth field of RDF is to reify statements
(Hernández \emph{et al.}
\protect\hyperlink{ref-hernandez_reifying_2015}{2015}). Indeed by
identifying each statement, it becomes possible to efficiently for
statements about statements.

A completely different approach is done by Hart and Goertzel
(\protect\hyperlink{ref-hart_opencog_2008}{2008}) in his framework for
Artificial General Intelligence (AGI) called OpenCog. The structure of
the knowledge is based on a rhizome, a collection of trees, linked to
one another. This structure is called Atomspace. Each vertex in the tree
is an atom, leaf-vertexes are nodes, the others are links. Atoms are
immutable, indexed objects. They can be given values that can be dynamic
and, since they are not part of the rhizome, are an order of magnitude
faster to access. Atoms and values alike are typed.

The goal of such a structure is to be able to merge concepts from widely
different domains of AI. The major drawback being that the whole system
is very slow compared to pretty much any domain specific software.

\hypertarget{self}{%
\section{Self}\label{self}}

\hypertarget{tbl:self}{}
\begin{table}\footnotesize
\centering

\caption{\label{tbl:self}List of classical symbols and syntax for self.}

\begin{tabular}{@{}ll@{}}
\toprule

\textbf{Symbol} & \textbf{Description} \\\midrule

\(\cal{D}, P, \bb{Q}, S, T, \bb{U}\) & Sets for domains, properties,
quantifiers, statements, types and universe. \\
\(\mu^\pm\) & Meta-relation for abstraction (\(+\)) and reification
(\(\minus\)). \\
\(\nu\) & Name relation. \\
\(\rho\) & Parameter relation. \\

\bottomrule
\end{tabular}

\end{table}

As we have seen, most existing knowledge description systems have a
common drawback: they are static. This means that they are either too
inefficient or too specific. To fix this issue, a new knowledge
representation system must be presented. The goal is to make a minimal
language framework that can adapt to its use to become as specific as
needed. If it becomes specific is must start from a generic base. Since
that base language must be able to evolve to fit the most cases
possible, it must be neutral and simple.

To summarize, that framework must maximize the following criteria:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Neutral}: Must be independent from preferences and be
  localization.
\item
  \textbf{Permissive}: Must allow as many data representation as
  possible.
\item
  \textbf{Minimalist}: Must have the minimum number of base axioms and
  as little native notions as possible.
\item
  \textbf{Adaptive}: Must be able to react to user input and be as
  flexible as possible.
\end{enumerate}

In order to respect these requirements, we developed a framework for
knowledge description. This Structurally Expressive Language Framework
(SELF) is our answer to these criteria. SELF is inspired by RDF Turtle
and Description Logic.

\hypertarget{knowledge-structure}{%
\subsection{Knowledge Structure}\label{knowledge-structure}}

SELF extends the RDF graphs by adding another label to the edges of the
graph to uniquely identify each statement. This basically turns the
system into a quadruple storage even if this forth field is transparent
to the user.

\begin{axiom}[Structure]\label[axiom]{axi:structure}

A SELF graph is a set of statements that transparently include their own
identity. The closest representation of the underlying structure of SELF
is as follows: \[
g_{\bb{U}} = (\bb{U}, S) :
  S = \left\{ s = \langle sub,pro,obj \rangle: 
    s \in \cal{D} \vdash s \land \cal{D} \right\}
\]

with:

\begin{itemize}
\tightlist
\item
  \(sub, obj \in \bb{U}\) being entities representing the \emph{subject}
  and \emph{object} of the \emph{statement} \(s\),
\item
  \(pro \in P\) being the \emph{property} of the statement \(s\),
\item
  \(\cal{D} \subset S\) is the \emph{domain} of the \emph{world}
  \(g_{\bb{U}}\),
\item
  \(S, P \subset \bb{U}\) with \(S\) the set of statements and \(P\) the
  set of properties,
\end{itemize}

\end{axiom}

This means that the world \(g_{\bb{U}}\) is a graph with the set of
entities \(\bb{U}\) as vertices and the set of statements \(S\) as
edges. This model also suppose that every statement \(s\) must be true
if they belong to the domain \(\cal{D}\). This graph is a directed
3-uniform hypergraph.

Since sheaves are a representation of hypergraphs, we can encode the
structure of SELF into a sheaf-like form. Each seed is a statement, the
germ being the statement vertex. It is always accompanied of an incoming
connector (its subject), an outgoing connector (its object) and a
non-directed connector (its property). The sections are domains and must
be coherent. Each statement, along with its property, makes a stalk as
illustrated in \cref{fig:selfgraph}.

\begin{figure}
\hypertarget{fig:selfgraph}{%
\centering
\includegraphics{graphics/self_graph.svg}
\caption{Projection of a statement from the SELF to RDF
space.}\label{fig:selfgraph}
}
\end{figure}

The difference with a sheaf is that the projection function is able to
map the pair statement-property into a labeled edge in its projection
space. We map this pair into a classical labeled edge that connects the
subject to the object of the statement in a directed fashion. This
results in the projected structure being a correct RDF graph.

\hypertarget{consequences}{%
\subsubsection{Consequences}\label{consequences}}

The base knowledge structure is more than simply convenience. The fact
that statements have their own identity, changes the degrees of freedom
of the representation. RDF has a way to represent reified statements
that are basically blank nodes with properties that are related to
information about the subject, property and object of a designated
statement. The problem is that such statements are very differently
represented and need 3 regular statements just to define. Using the
fourth field, it becomes possible to make statements about \emph{any}
statements. It also becomes possible to express modal logic about
statements or to express, various traits like the probability or the
access rights of a statement.

The knowledge structure holds several restrictions on the way to express
knowledge. As a direct consequence, we can add several theorems to the
logic system underlying SELF. The
\namecref{axi:structure} of \nameref{axi:structure} is the only axiom of
the system.

\begin{theorem}[Identity]\label[theorem]{theo:identity}

Any entity is uniquely distinct from any other entity.

\end{theorem}

This theorem comes from the
\namecref{axi:extensionality} of \nameref{axi:extensionality} of ZFC.
Indeed it is stated that a set is a unordered collection of distinct
objects. Distinction is possible if and only if intrinsic identity is
assumed. This notion of identity entails that a given entity cannot
change in a way that would alter its identifier.

\begin{theorem}[Consistency]\label[theorem]{theo:consistency}

Any statement in a given domain is consistent with any other statements
of this domain.

\end{theorem}

Consistency comes from the need for a coherent knowledge database and is
often a requirement of such constructs. This theorem also is a
consequence of the \namecref{axi:structure} of \nameref{axi:structure}:
\(s \in \cal{D} \vdash s \land \cal{D}\).

\begin{theorem}[Uniformity]\label[theorem]{theo:uniformity}

Any object in SELF is an entity. Any relations in SELF are restricted to
\(\bb{U}\).

\end{theorem}

This also means that all native relations are closed under \(\bb{U}\).
This allows for a uniform knowledge database.

\hypertarget{native-properties}{%
\subsubsection{Native properties}\label{native-properties}}

\cref{theo:identity} lead to the need for two native properties in the
system : \emph{equality} and \emph{name}.

The \textbf{equality relation} \(= : \bb{U} \to \bb{U}\), behaves like
the classical operator. Since the knowledge database will be expressed
through text, we also need to add an explicit way to identify entities.
This identification is done through the \textbf{name relation}
\(\nu: \bb{U} \to L_{String}\) that affects a string literal to some
entities. This lead us to introduce literals into SELF that is also
entities that have a native value.

The \namecref{axi:structure} of \nameref{axi:structure} puts a type
restriction on property. Since it compartments \(\bb{U}\) using various
named subsets, we must adequately introduce an explicit type system into
SELF. That type system requires a \textbf{type relation} (named using
the colon) \(: : \bb{U} \to T\). That relation is complete as all
entities have a type. \cref{theo:uniformity} causes the set of entities
to be universal. Type theory, along with Description Logic (DL),
introduces a \textbf{subsumption relation} \(\subseteq : T \to T\) as a
partial ordering relation to the types. Since types can be seen as sets
of instances we simply use the the subset relation from set theory. In
our case, the entity type is the greatest element of the lattice formed
by the set of types with the subsumption relation \((T, \subseteq)\).

The \cref{theo:uniformity} also allows for some very interesting
meta-constructs. That is why we also introduce a signed \textbf{Meta
relation} \(\mu^+: \bb{U} \to D\) with \(\mu^- = (\mu^+)^{-1}\). This
allows to create domain from certain entities and to encapsulate domains
into entities. \(\mu^-\) is for reification and \(\mu^+\) is for
abstraction. This Meta relation also allows to express value of
entities, like lists or various containers.

To fulfill the principle of adaptability and in order to make the type
system more useful, we introduce the \textbf{parameter relation}
\(\rho: \bb{U} \to \bb{U}\). This relation affects a list of parameters,
using the meta relation, to some parameterized entities. This also
allows for variables in statements.

Since \namecref{axi:structure} of \nameref{axi:structure} gives the
structure of SELF a hypergraph shape, we must port some notions of graph
theory into our framework. Introducing the \textbf{statement relation}
\(\phi : S \to \bb{U}\) reusing the same symbol as for the adjacency and
incidence relation of graphs. This isn't a coincidence as this relation
has the same properties. For example, \(\phi^-(s)\) gives the subject of
a statement \(s\). Respectively, \(\phi^+\) and \(\phi^0\) give the
object and property of any statement. For adjacencies, \(\phi^-\) and
\(\phi^+\) can give the set of statements any entity is respectively the
object and subject of. For any property \(pro\), the notation
\(\phi^0(pro)\) gives the set of statements using this property. This
allows us to port all the other notions of graphs using this relation as
a base.

In \cref{fig:typerel}, we present all the native relations along with
their domains and most subsets of \(\bb{U}\).

\begin{figure}
\hypertarget{fig:typerel}{%
\centering
\includegraphics{graphics/self_structure.svg}
\caption{Venn diagram of subsets of \(\bb{U}\) along with their
relations. Dotted lines mean that the sets are defined a subset of the
wider set.}\label{fig:typerel}
}
\end{figure}

\hypertarget{syntax}{%
\subsection{Syntax}\label{syntax}}

Since we need to respect the requirements of the problem, the RDF syntax
cannot be used to express the knowledge. Indeed, RDF states native
properties as English nodes with a specific URI that isn't neutral. It
also isn't minimalist since it uses an XML syntax so verbose that it is
not used for most examples in the documents that defines RDF because it
is too confusing and complex (W3C
\protect\hyperlink{ref-w3c_rdf_2004a}{2004}\protect\hyperlink{ref-w3c_rdf_2004a}{a};
W3C
\protect\hyperlink{ref-w3c_rdf_2004}{2004}\protect\hyperlink{ref-w3c_rdf_2004}{b}).
The XML syntax is also quite restrictive and cannot evolve dynamically
to adapt to the usage.

So we need to define a new language that is minimalist and neutral. At
the same time the language must be permissive and dynamic. These two
goals are incompatible and will end up needing different solutions. So
the solution to the problem is to actually define two languages that fit
the criteria : one minimalist and one adaptive. The issue is that we
need not make a user learn two languages and the second kind of language
must be very specific and that violates the principle of neutrality we
try to respect.

The only solution is to make a mechanism to adapt the language as it is
used. We start off with a simple framework that uses a grammar.

The description of \(\bb{g}_0\) is pretty straightforward: it mostly is
just a triple representation separated by whitespaces. The goal is to
add a minimal syntax consistent with the
\namecref{axi:structure} of \nameref{axi:structure}. In
\cref{lst:grammar}, we give a simplified version of \(\bb{g}_0\). It is
written in a pseudo-BNF fashion, which is extended with the classical
repetition operators \texttt{*} and \texttt{+} along with the negation
operator \texttt{\textasciitilde{}}. All tokens have names in uppercase.
We also add the following rule modifiers:

\begin{itemize}
\tightlist
\item
  \texttt{\textless{}\textasciitilde{}name\textgreater{}} are ignored
  for the parsing. However, the tokens are consumed and therefore acts
  like separators for the other rules.
\item
  \texttt{\textless{}?name\textgreater{}} are inferred rules and tokens.
  They play a key role for the process of derivation explained in
  \cref{sec:derivation}.
\end{itemize}

\begin{codelisting}

\caption{Simplified pseudo-BNF description for basic SELF.}

\hypertarget{lst:grammar}{%
\label{lst:grammar}}%
\begin{verbatim}
<~COMMENT: <INLINE: "//" (~["\n", "\r"])*>
| <BLOCK: "/*" (~["*/"])*> > //Ignored
<~WHITE_SPACE: " "|"\t"|"\n"|"\r"|"\f">
<LITERAL: <INT> | <FLOAT> | <CHAR> | <STRING>> //Java definition$\label[line]{line:literal}$
<ID: <TYPE: <UPPERCASE>(<LETTERS>|<DIGITS>)* > $\label[line]{line:uppercase}$
| <ENTITY:  <LOWERCASE>(<LETTERS>|<DIGITS>)*> 
| <SYMBOL: (~[<LITERALS>, <LETTERS>, <DIGITS>])*>>

<worselfld> ::= <first> <statement>* <EOF>
<first> ::= <subject> <?EQUAL> <?SOLVE> <?EOS> $\label[line]{line:first}$
<statement> ::= <subject> <property> <object> <EOS> $\label[line]{line:statement}$
<subject> ::= <entity>
<property> ::= <ID> | <?meta_property>
<object> ::= <entity>
<entity> ::= <ID> | <LITERAL> | <?meta_entity>
\end{verbatim}

\end{codelisting}

In order to respect the principle of neutrality, the language must not
suppose of any regional predisposition of the user. There are few
exceptions for the sake of convenience and performance. The first
exception is that the language is meant to be read from left to right
and have an occidental biased \texttt{subject\ verb\ object} triple
description. Another exception is for liberals that use the same grammar
as in classical Java. This means that the decimal separator is the dot
(\texttt{.}). This could be fixed in later version using dynamic
definitions (see \cref{sec:peano}).

Even if sticking to the ASCII subset of characters is a good idea for
efficiency, SELF can work with UTF-8 and exploits the Unicode Character
Database (UCD) for its token definitions (Unicode Consortium
\protect\hyperlink{ref-unicodeconsortium_unicode_2018}{2018}\protect\hyperlink{ref-unicodeconsortium_unicode_2018}{b}).
This means that SELF comes keywords free and that the definition of each
symbol is left to the user. Each notion and symbol is inferred (with the
exception of the first statement which is closer to an imposed
configuration file).

In \(\bb{g}_0\), the first two token definitions are ignored. This means
that comments and white-spaces will act as separation and won't be
interpreted. Comments are there only for convenience since they do not
serve any real purpose in the language. It was arbitrarily decided to
use Java-style comments. White-spaces are defined against UCD's
definition of the separator category \texttt{Z\&} (see Unicode
Consortium
\protect\hyperlink{ref-unicodeconsortium_unicode_2018a}{2018}\protect\hyperlink{ref-unicodeconsortium_unicode_2018a}{a},
chap. 4).

\cref{line:literal} uses the basic Java definition for liberals. In
order to keep the independence from any natural language, boolean
laterals are not natively defined (since they are English words).

Another aspect of that language independence is found starting at
\cref{line:uppercase} where the definitions of
\texttt{\textless{}UPPERCASE\textgreater{}},
\texttt{\textless{}LOWERCASE\textgreater{}},
\texttt{\textless{}LETTERS\textgreater{}} and
\texttt{\textless{}DIGITS\textgreater{}} are defined from the UCD
(respectively categories \texttt{Lu}, \texttt{Ll}, \texttt{L\&},
\texttt{Nd}). This means that any language's upper case can be used in
that context. For performance and simplicity reasons we will only use
ASCII in our examples and application.

The rule at \cref{line:first} is used for the definition of three tokens
that are important for the rest of the input.
\texttt{\textless{}EQUAL\textgreater{}} is the symbol for equality and
\texttt{\textless{}SOLVE\textgreater{}} is the symbol for the
\emph{solution quantifier} (and also the language pendant of \(\mu^-\)).
The most useful token \texttt{\textless{}EOS\textgreater{}} is used as a
statement delimiter. This rule also permits the inclusion of other files
if a string literal is used as a subject. The underlying logic of this
first statement will be presented in \cref{sec:quantifier}.

At \cref{line:statement}, we can see one of the most defining features
of \(\bb{g}_0\): statements. The input is nothing but a set of
statements. Each component of the statements are entities. We defined
two specific rules for the subject and object to allow for eventual
runtime modifications. The property rule is more restricted in order to
guarantee the non-ambiguity of the grammar.

\hypertarget{sec:derivation}{%
\subsection{Dynamic Grammar}\label{sec:derivation}}

The syntax we described is only valid for \(\bb{g}_0\). As long as the
input is conforming to these rules, the framework keeps the minimal
behavior. In order to access more features, one needs to break a rule.
We add a second outcome to handling with violations :
\textbf{derivation}. There are several kinds of possible violations that
will interrupt the normal parsing of the input :

\begin{itemize}
\tightlist
\item
  Violations of the \texttt{\textless{}first\textgreater{}} statement
  rule : This will cause a fatal error.
\item
  Violations of the \texttt{\textless{}statement\textgreater{}} rule :
  This will cause a derivation if an unexpected additional token is
  found instead of \texttt{\textless{}EOS\textgreater{}}. If not enough
  tokens are present, a fatal error is triggered.
\item
  Violations of the secondary rules
  (\texttt{\textless{}subject\textgreater{}},
  \texttt{\textless{}entity\textgreater{}}, \ldots) : This will cause a
  fatal error except if there is also an excess of token in the current
  statement which will cause derivation to happen.
\end{itemize}

Derivation will cause the current input to be analyzed by a set of
meta-rules. The main restriction of these rules is given in
\(\bb{g}_0\): each statement must be expressible using a triple
notation. This means that the goal of the meta-rules is to find an
interpretation of the input that is reducible to a triple and to augment
\(\bb{g}_0\) by adding an expression to any
\texttt{\textless{}meta\_*\textgreater{}} rules. If the input has fewer
than 3 entities for a statement then the parsing fails. When there is
extra input in a statement, there is a few ways the infringing input can
be reduced back to a triple.

\hypertarget{containers}{%
\subsubsection{Containers}\label{containers}}

The first meta-rule is to infer a container. A container is delimited by
at least a left and right delimiter (they can be the same symbol) and an
optional middle delimiter. We infer the delimiters using the
\cref{alg:container}.

\begin{algorithm}\caption{Container meta-rule}\label[algorithm]{alg:container}\begin{algorithmic}[1]\Function{container}{Token current}
  \State \Call{lookahead}{current, EOS} \Comment{Populate all tokens of the statement}
  \ForAll{token in horizon}
    \If{token is a new symbol}
      delimiters.\Call{append}{token}
    \EndIf
  \EndFor
  \If{\Call{length}{delimiters} <2 }
    \If{\Call{coherentDelimiters}{horizon, delimiters[0]} }
      \State \Call{inferMiddle}{delimiters[0]} \Comment{New middle delimiter in existing containers}
      \State \Return Success
    \EndIf
    \State \Return Failure
  \EndIf
  \While{\Call{length}{delimiters} > 0}
    \ForAll{(left, middle, right) in \Call{sortedDelimiters}{delimiters}}\label[line]{line:sorteddelim}
      \If{\Call{coherentDelimiters}{horizon, left, middle, right} }\label[line]{line:coherentdelim}
        \State \Call{inferDelimiter}{left, right}
        \State \Call{inferMiddle}{middle} \Comment{Ignored if null}
        \State delimiters.\Call{remove}{left, middle, right}
        \Break
      \EndIf
    \EndFor
    \If{\Call{length}{delimiters} stayed the same }
      \Return Success
    \EndIf
  \EndWhile
  \State \Return Success
\EndFunction\end{algorithmic}\end{algorithm}

The function sortedDelimiters at \cref{line:sorteddelim} is used to
generate every ordered possibility and sort them using a few criteria.
The default order is possibilities grouped from left to right. All
coupled delimiters that are mirrors of each other following the UCD are
preferred to other possibilities.

Checking the result of the choice is very important. At
\cref{line:coherentdelim} a function checks if the delimiters allow for
triple reduction and enforce restrictions. For example, a property
cannot be wrapped in a container (except if part of parameters). This is
done in order to avoid a type mismatch later in the interpretation.

Once the inference is done, the resulting calls to inferDelimiter will
add the rules listed in \cref{lst:container} to \(\bb{g}_0\). This
function will create a \texttt{\textless{}container\textgreater{}} rule
and add it to the definition of
\texttt{\textless{}meta\_entity\textgreater{}}. Then it will create a
rule for the container named after the UCD name of the left delimiter
(searching in the \texttt{NamesList.txt} file for an entry starting with
``left'' and the rest of the name or defaulting to the first entry).
Those rules are added as a conjunction list to the rule
\texttt{\textless{}container\textgreater{}}. It is worthy to note that
the call to inferMiddle will add rules to the token
\texttt{\textless{}MIDDLE\textgreater{}} independently from any
container and therefore, all containers share the same pool of middle
delimiters.

\begin{codelisting}

\caption{Rules added to the current grammar for handling the new container for parenthesis}

\hypertarget{lst:container}{%
\label{lst:container}}%
\begin{verbatim}
<meta_entity> ::= <container>
<container> :: = <parenthesis> | …
<parentesis> ::= "(" [<naked_entity>] (<?MIDDLE> <naked_entity>)* ")"
<naked_entity> ::= <statement> | <entity>$\label{line:meta_statement}$
\end{verbatim}

\end{codelisting}

The rule at \cref{line:meta_statement} is added once and enables the use
of meta-statements inside containers. It is the language pendant of the
\(\mu^+\) relation, allowing to wrap abstraction in a safe way.

\hypertarget{parameters}{%
\subsubsection{Parameters}\label{parameters}}

If the previous rule didn't fix the parsing of the statement, we
continue with the following meta-rule. Parameters are extra containers
that are used after an entity. Every container can be used as
parameters. We detail the analysis in \cref{alg:parameter}.

\begin{algorithm}\caption{Parameter meta-rule}\label[algorithm]{alg:parameter}\begin{algorithmic}[1]\Function{parameter}{Entity[] statement}
  \State reduced = statement
  \While{\Call{length}{reduced} >3}
    \For{i from 0 to \Call{length}{reduced} - 1}
      \If{\Call{name}{reduced[i]} not null and \\ 
      \Call{type}{reduced[i+1]} = Container and \\
      \Call{coherentParameters}{reduced, i}}
        \State param = \Call{inferParameter}{reduced[i], reduced[i+1]}
        \State reduced.\Call{remove}{reduced[i], reduced[i+1]}
        \State reduced.\Call{insert}{param, i} \Comment{Replace parameterized entity}
        \Break
      \EndIf
    \EndFor
    \If{\Call{length}{statement} stayed the same }
      \Return Success
    \EndIf
  \EndWhile
  \State \Return Failure
\EndFunction\end{algorithmic}\end{algorithm}

The goal is to match extra containers with the preceding named entity.
The container is then combined with the preceding entity into a
parameterized entity.

The call to inferParameter will add the rule in \cref{lst:parameter},
replacing \texttt{\textless{}?container\textgreater{}} with the name of
the container used (for example
\texttt{\textless{}parenthesis\textgreater{}}).

\begin{codelisting}

\caption{Rules added to the current grammar for handling parameters}

\hypertarget{lst:parameter}{%
\label{lst:parameter}}%
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{<meta_entity> ::= <ID> <?container>}
\NormalTok{<meta_property> ::= <ID> <?container>}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\hypertarget{operators}{%
\subsubsection{Operators}\label{operators}}

A shorthand for parameters is the operator notation. It allows to affect
a single parameter to an entity without using a container. It is most
used for special entities like quantifiers or modificators. This is why,
once used, the parent entity takes a polymorphic type, meaning that type
inference will not issue errors for any usage of them. Details of the
way the operators are reduced is exposed in \cref{alg:operator}.

\begin{algorithm}\caption{Operator meta-rule}\label[algorithm]{alg:operator}\begin{algorithmic}[1]\Function{operator}{Entity[] statement}
  \State reduced = statement
  \While{\Call{length}{reduced} >3}
    \For{i from 0 to \Call{length}{reduced} - 1}
      \If{\Call{$\nu$}{reduced[i]} not null and \\ 
      \Call{$\nu$}{reduced[i+1]} not null and \\
      (\Call{$\nu$}{reduced[i]} is a new symbol or \\
      reduced[i] has been parameterized before) and \\
      \Call{coherentOperator}{reduced, i}}
        \State op = \Call{inferOperator}{reduced[i], reduced[i+1]}
        \State reduced.\Call{remove}{reduced[i], reduced[i+1]}
        \State reduced.\Call{insert}{op, i} \Comment{Replace parameterized entity}
        \Break
      \EndIf
    \EndFor
    \If{\Call{length}{statement} stayed the same }
      \Return Success
    \EndIf
  \EndWhile
  \State \Return Failure
\EndFunction\end{algorithmic}\end{algorithm}

From the call of inferOperator, comes new rules explicated in
\cref{lst:operator}. The call also adds the operator entity to an
inferred token \texttt{\textless{}OP\textgreater{}}.

\begin{codelisting}

\caption{Rules added to the current grammar for handling operators}

\hypertarget{lst:operator}{%
\label{lst:operator}}%
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{<meta_entity> ::= <?OP> <ID>}
\NormalTok{<meta_property> ::= <?OP> <ID>}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

If all meta-rules fail, then the parsing fails and returns an error to
the user.

\hypertarget{contextual-interpretation}{%
\subsection{Contextual Interpretation}\label{contextual-interpretation}}

While parsing another important part of the processing is done after the
success of a grammar rule. The grammar in SELF is valuated, meaning that
each rule has to return an entity. A set of functions are used to then
populate the database with the right entities or retrieve an existing
one that correspond to what is being parsed.

When parsing, the rules \texttt{\textless{}entity\textgreater{}} and
\texttt{\textless{}property\textgreater{}} will ask for the creation or
retrieval of an entity. This mechanism will use the name of the entity
and its type to retrieve an entity with the same name in a given scope.

\hypertarget{naming-and-scope}{%
\subsubsection{Naming and Scope}\label{naming-and-scope}}

When parsing an entity by name, the system will first request for an
existing entity with the same name. If such an entity is retrieved, it
is returned instead of creating a new one. The validity of a name is
limited by the notion of scope.

A scope is the reach of an entity's direct influence. It affects the
naming relation by removing variable's names. Scopes are delimited by
containers and statements. This local context is useful when wanting to
restrict the scope of the declaration of an entity. The main goal of
such restriction is to allow for a similar mechanism as the RDF
namespaces. This also makes the use of variable (RDF blank nodes)
possible.

The scope of an entity has three special values :

\begin{itemize}
\tightlist
\item
  Variable: This scope restricts the scope of the entity to only the
  other entities in its scope.
\item
  Local: This scope means that the parsing is still populating the scope
  of the entity. Its scope is limited to the currently parsing
  statement.
\item
  Global: This scope means the name has no scope limitation.
\end{itemize}

The scope of an entity also contains all its parent entities, meaning
all containers or statement the entity is part of. This is used when
choosing between the special values of the scope. The process is
detailed in \cref{alg:scope}.

\begin{algorithm}\caption{Determination of the scope of an entity}\label[algorithm]{alg:scope}\begin{algorithmic}[1]\Function{inferScope}{Entity $e$}
  \State Entity[] reach = []
  \If{$:(e) = S$}
    \ForAll{$i \in \phi(e)$}
      reach.\Call{append}{\Call{inferVariable}{$i$}} \Comment{Adding scopes nested in statement $e$}
    \EndFor
  \EndIf
  \ForAll{$i \in \mu^-(e)$}
    reach.\Call{append}{\Call{inferVariable}{$i$}} \Comment{Adding scopes nested in container $e$}
  \EndFor
  \If{$\exists \rho(e)$}
    \State Entity[] param = \Call{inferScope}{$\rho(e)$}
    \ForAll{$i \in $ param}
      param.\Call{remove}{\Call{inferScope}{$i$}} \Comment{Remove duplicate scopes from parameters}
    \EndFor
    \ForAll{$i \in $ param}
      reach.\Call{append}{\Call{inferVariable}{$i$}} \Comment{Adding scopes from paramters of $e$}
    \EndFor
  \EndIf
  \State $\Call{scope}{e} \gets $ reach
  \If{GLOBAL $\notin \Call{scope}{e}$}
    \Call{scope}{$e$} $\gets$ \Call{scope}{$e$} $\cup \{$LOCAL$\}$
  \EndIf
  \Return reach
\EndFunction

\Function{inferVariable}{Entity $e$}
  \State Entity[] reach = []
  \If{LOCAL $\in$ \Call{scope}{$e$}}
    \ForAll{$i \in$ \Call{scope}{$e$}}
      \If{$\exists e^+ \in \bb{U} : \rho(p) = i$} \Comment{$e$ is already a parameter of another entity $e^+$}
        \State \Call{scope}{$e$} $\gets$ \Call{scope}{$e$} $\setminus \{$LOCAL$\}$
        \State \Call{scope}{$e^+$} $\gets$ \Call{scope}{$e^+$} $\cup$ \Call{scope}{$e$}
        \State \Call{scope}{$e$} $\gets$ \Call{scope}{$e$} $\cup \{$VARIABLE$, p\}$
      \EndIf
    \EndFor
    \State reach.\Call{append}{$e$}
    \State reach.\Call{append}{\Call{scope}{$e$}}
  \EndIf
  \Return reach
\EndFunction\end{algorithmic}\end{algorithm}

The process happens for each entity created or requested by the parser.
If a given entity is part of any other entity, the enclosing entity is
added to its scope. When an entity is enclosed in any entity while
already being a parameter of another entity, it becomes a variable.

\hypertarget{instanciation-identification}{%
\subsubsection{Instanciation
identification}\label{instanciation-identification}}

When a parameterized entity is parsed, another process starts to
identify if a compatible instance already exists. From
\cref{theo:identity}, it is impossible for two entities to share the
same identifier. This makes mandatory to avoid creating an entity that
is equal to an existing one. Given the order of which parsing is done,
it is not always possible to determine the parameter of an entity before
its creation. In that case a later examination will merge the new entity
onto the older one and discard the new identifier.

\hypertarget{structure-as-a-definition}{%
\subsection{Structure as a Definition}\label{structure-as-a-definition}}

The derivation feature on its own does not allow to define most of the
native properties. For that, one needs a light inference mechanism. This
mechanism is part of the default inference engine. This engine only
works on the principle of structure as a definition. Since all names
must be neutral from any language, that engine cannot rely on regular
mechanisms like configuration files with keys and values or predefined
keywords.

To use SELF correctly, one must be familiar with the native properties
and their structure or implement their own inference engine to override
the default one.

\hypertarget{sec:quantifier}{%
\subsubsection{Quantifiers}\label{sec:quantifier}}

What are quantifiers? In SELF they differ from their mathematical
counterparts. The quantifiers are special entities that are meant to be
of a generic type that matches any entities including quantifiers. There
are infinitely many quantifiers in SELF but they are all derived from a
special one called the \emph{solution quantifier}. We mentioned it
briefly during the definition of the grammar \(\bb{g}_0\). It is the
language pendant of \(\mu^-\) and is used to extract and evaluate
reified knowledge.

For example, the statement
\texttt{bob\ is\ \textless{}SOLVE\textgreater{}(x)} will give either a
default container filled with every value that the variable \texttt{x}
can take or if the value is unique, it will take that value. If there is
no value it will default to \texttt{\textless{}NULL\textgreater{}}, the
exclusion quantifier.

How are these other quantifiers defined? We use a definition akin to
Lindstöm quantifiers
(\protect\hyperlink{ref-lindstrom_first_1966}{1966}) which is a
generalization of counting quantifiers (Gradel \emph{et al.}
\protect\hyperlink{ref-gradel_twovariable_1997}{1997}). Meaning that a
quantifier is defined as a constrained range over the quantified
variable. We suppose five quantifiers as existing in SELF as native
entities.

\begin{itemize}
\tightlist
\item
  The \textbf{solution quantifier}
  \texttt{\textless{}SOLVE\textgreater{}} noted \(\textsection\) in
  classical mathematics, makes a query that turns the expression into
  the possible range of its variable.
\item
  The \textbf{universal quantifier}
  \texttt{\textless{}ALL\textgreater{}} behaves like \(\forall\) and
  forces the expression to affect every possible value of its variable.
\item
  The \textbf{existential quantifier}
  \texttt{\textless{}SOME\textgreater{}} behaves like \(\exists\) and
  forces the expression to match \emph{at least one} value for its
  variable.
\item
  The \textbf{uniqueness quantifier}
  \texttt{\textless{}ONE\textgreater{}} behaves like \(!\exists\) and
  forces the expression to match \emph{exactly one} value for its
  variable.
\item
  The \textbf{exclusion quantifier}
  \texttt{\textless{}NULL\textgreater{}} behaves like \(\lnot \exists\)
  and forces the expression to match none of the value of its variable.
\end{itemize}

The last four quantifiers are inspired from Aristotle's square of
opposition (D'Alfonso
\protect\hyperlink{ref-dalfonso_generalized_2011}{2011}).

\begin{figure}
\hypertarget{fig:aristotle}{%
\centering
\includegraphics{graphics/aristotle_square.svg}
\caption{Aristotle's square of opposition}\label{fig:aristotle}
}
\end{figure}

In SELF, quantifiers are not always followed by a quantified variable
and can be used as a value. In that case the variable is simply
anonymous. We use the exclusion quantifier as a value to indicate that
there is no value, sort of like \texttt{null} or \texttt{nil} in
programming languages.

In \cref{lst:lang}, we present an example file that is meant to define
most of the useful native properties along with default quantifiers.

\begin{codelisting}

\caption{The default lang.w file.}

\hypertarget{lst:lang}{%
\label{lst:lang}}%
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{* =? ;$\textbackslash{}label\{line:first\}$}
\NormalTok{?(x) = x; }\CommentTok{//Optional definition}
\NormalTok{?~ = \{ \};}
\NormalTok{?_ ~(=) ~;}
\NormalTok{?!_ = \{_\};$\textbackslash{}label\{line:endquantifier\}$}

\NormalTok{(*e, !T) : (e :: T); *T : (T :: }\BuiltInTok{Type}\NormalTok{);$\textbackslash{}label\{line:typing\}$}
\NormalTok{*T : (}\BuiltInTok{Entity}\NormalTok{ / T);$\textbackslash{}label\{line:subsumption\}$}

\NormalTok{:: :: }\FunctionTok{Property}\NormalTok{(}\BuiltInTok{Entity}\NormalTok{, }\BuiltInTok{Type}\NormalTok{);}
\NormalTok{(___) :: }\BuiltInTok{Statement}\NormalTok{;}
\NormalTok{(~, !, _, *) :: Quantifier;}
\NormalTok{( )::}\BuiltInTok{Group}\NormalTok{;}
\NormalTok{\{ \}::}\BuiltInTok{Set}\NormalTok{;}
\NormalTok{[ ]::}\BuiltInTok{List}\NormalTok{;}
\NormalTok{< >::Tuple;}
\BuiltInTok{Collection}\NormalTok{/(}\BuiltInTok{Set}\NormalTok{,}\BuiltInTok{List}\NormalTok{,Tuple);}
\DecValTok{0}\NormalTok{ :: }\BuiltInTok{Integer}\NormalTok{; }\FloatTok{0.}\DecValTok{0}\NormalTok{::}\BuiltInTok{Float}\NormalTok{;}
\NormalTok{'\textbackslash{}}\DecValTok{0}\NormalTok{'::}\BuiltInTok{Character}\NormalTok{; }\StringTok{""}\NormalTok{::}\BuiltInTok{String}\NormalTok{;}
\NormalTok{Literal/(}\BuiltInTok{Boolean}\NormalTok{, }\BuiltInTok{Integer}\NormalTok{, }\BuiltInTok{Float}\NormalTok{, }\BuiltInTok{Character}\NormalTok{, }\BuiltInTok{String}\NormalTok{);}

\NormalTok{(*e, !(s::}\BuiltInTok{String}\NormalTok{)) : (e named s);$\textbackslash{}label\{line:naming\}$}
\NormalTok{(*}\FunctionTok{e}\NormalTok{(p), !p) : (e param p);$\textbackslash{}label\{line:param\}$}
\NormalTok{*(s p o):(((s p o) subject s),((s p o) property p),((s p o) object o));$\textbackslash{}label\{line:incidence\}$}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

At \cref{line:first}, we give the first statement that defines the
solution quantifier's symbol. The reason this first statement is shaped
like this is that global statements are always evaluated to be a true
statement. This means that anything equaling the solution quantifier at
this level will be evaluated as a domain. If it is a string literal,
then it must be either a file path or URL or a valid SELF expression. If
it is a single entity then it becomes synonymous to the entire SELF
domain and therefore contains everything. We can infer that it becomes
the universal quantifier.

All statements up to \cref{line:endquantifier} are quantifiers
definitions. On the left side we got the quantifier symbol used as a
parameter to the solution quantifier using the operator notation. On the
right we got the domain of the quantifier. The exclusive quantifier has
as a range the empty set. For the existential quantifier we have only a
restriction of it not having an empty range. At last, the uniqueness
quantifier got a set with only one element matching its variable (noting
that anonymous variables doesn't match necessarily other anonymous
variables in the same statement).

In that file the type hierarchy can be illustrated by the
\cref{fig:hierarchy}. It consist of entities that are either
parameterized or not and that have a value or not.

\begin{figure}
\hypertarget{fig:hierarchy}{%
\centering
\includegraphics{graphics/todo.svg}
\caption{Hierarchy of types in SELF}\label{fig:hierarchy}
}
\end{figure}

\hypertarget{inferring-native-properties}{%
\subsubsection{Inferring Native
Properties}\label{inferring-native-properties}}

All native properties can be inferred by structure using quantified
statements. Here is the structural definition for each of them:

\begin{itemize}
\tightlist
\item
  \(=\) (at \cref{line:first}) is the equality relation given in the
  first statement.
\item
  \(\subseteq\) (at \cref{line:subsumption}) is the first property to
  relate a particular type relative to all types. That type becomes the
  entity type.
\item
  \(\mu^-\) (at \cref{line:first}) is the solution quantifier discussed
  above given in the first statement.
\item
  \(\mu^+\) is represented using containers.
\item
  \(\nu\) (at \cref{line:naming}) is the first property affecting a
  string literal uniquely to each entity.
\item
  \(\rho\) (at \cref{line:param}) is the first property to effect to all
  entities a possible parameter list.
\item
  \(:\) (at \cref{line:typing}) is the first property that matches every
  entity to a type.
\item
  \(\phi\) (at \cref{line:incidence}) is the first property to match for
  all statements:

  \begin{itemize}
  \tightlist
  \item
    \(\phi^-\) its subject,
  \item
    \(\phi^0\) its property,
  \item
    \(\phi^+\) its object.
  \end{itemize}
\end{itemize}

We limit the inference to one symbol to make it much simpler to
implement and to retrieve but, except for false positives, there are no
reason it should not be possible to define several notations for each
relation.

\hypertarget{extended-inference-mechanisms}{%
\subsection{Extended Inference
Mechanisms}\label{extended-inference-mechanisms}}

In this section we present the default inference engine. It has only a
few functionalities. It isn't meant to be universal and the goal of SELF
is to provide a framework that can be used by specialists to define and
code exactly what tools they need.

Inference engines need to create new knowledge but this knowledge
shouldn't be simply merged with the explicit domain. Since this
knowledge is inferred, it is not exactly part of the domain but must
remain consistent with it. This knowledge is stored in a special scope
dedicated to each inference engine. This way, inference engines can use
defeasible logic or have dynamic inference from any knowledge insertion
in the system.

\hypertarget{type-inference}{%
\subsubsection{Type Inference}\label{type-inference}}

Type inference works on matching types in statements. The main mechanism
consists in inferring the type of properties in a restrictive way.
Properties have a parameterized type with the type of their subject and
object. The goal is to make that type match the input subject and
object.

For that we start by trying to match the types. If the types differ, the
process tries to reduce the more general type against the lesser one
(subsumption-wise). If they are incompatible, the inference uses some
light defeasible logic to undo previous inferences. In that case the
types are changed to the last common type in the subsumption tree.

However, this may not always be possible. Indeed, types can be
explicitly specified as a safeguard against mistakes. If that's the
case, an error is raised and the parsing or knowledge insertion is
interrupted.

\hypertarget{instanciation}{%
\subsubsection{Instanciation}\label{instanciation}}

Another inference tool is instantiation. Since entities can be
parameterized, they can also be defined against their parameters. When
those parameters are variables, it allows entities to be instantiated
later.

This is a complicated process because entities are immutable. Indeed,
parsing happens from left to right and therefore an entity is often
created before all the instantiation information are available. Even
harder are completion of definition in several separate statements. In
all cases, a new entity is created and then the inference realize that
it is either matching a previous definition and will need to be merged
with the older entity or it is a new instance and needs all properties
duplicated and instantiated.

This gives us two mechanisms to take into account: merging and
instanciating.

Merging is pretty straightforward: the new entity is replaced with the
old one in all of the knowledge graph. containers, parameterized
entities, quantifiers and statements must be duplicated with the correct
value and the original destroyed. This is a heavy and complicated
process but seemingly the only way to implement such a case with
immutable entities.

Instanciating is similar to merging but even more complicated. It starts
with computing a relation that maps each variable that needs replacing
with their grounded value. Then it duplicates all knowledge about the
parent entity while applying the replacement map.

\hypertarget{general-planning-framework}{%
\chapter{General Planning Framework}\label{general-planning-framework}}

When making artificial intelligence systems, an important feature is the
ability to make decisions and act accordingly. To act, one should plan
ahead. This is why the field of automated planning is being actively
researched in order to find efficient algorithms to find the best course
of action in any given situation. The previous chapter allowed to lay
the bases of knowledge representation. How knowledge about the planning
domains are represented is a main factor to take into account in order
to conceive any planning algorithm.

Automated planning really started being formally investigated after the
creation of the Stanford Research Institute Problem Solver (STRIPS) by
Fikes and Nilsson (\protect\hyperlink{ref-fikes_strips_1971}{1971}).
This is one of the most influential planner, not because of its
algorithm but because of its input language. Any planning system needs a
way to express the information related to the input problem. Any
language done for this purpose is called an \emph{action language}.
STRIPS will be mostly remembered for its eponymous action language that
is at the base of any modern derivatives.

All action language is based on mainly two notions: \emph{actions} and
\emph{states}. A state is a set of \emph{fluents} that describe aspects
of the world modelized by the domain. Each action has a logic formula
over states that allows its correct execution. This requirement is
called \emph{precondition}. The mirror image of this notion are possible
\emph{effects} which are logic formula that are enforced on the current
state after the action is executed. The domain is completed with a
problem, most of the time specified in a separate file. The problem
basically contains two states: the \emph{initial} and \emph{goal}
states.

\hypertarget{sec:plan_example}{%
\section{Illustration}\label{sec:plan_example}}

To illustrate how automated planners works, we introduce a typical
planning problem called \textbf{block world}. In this example, a robotic
grabing arm tries to stack blocks on a table in a specific order. The
arm is only capable of handling one block at a time. We suppose that the
table is large enough so that all the blocks can be put on it without
any stacks. \Cref{fig:blockworld} illustrates the setup of this domain.

\begin{figure}
\hypertarget{fig:blockworld}{%
\centering
\includegraphics{graphics/blockworld.svg}
\caption{The block world domain setup.}\label{fig:blockworld}
}
\end{figure}

The possible actions are \texttt{pickup}, \texttt{putdown},
\texttt{stack} and \texttt{unstack}. There are at least three fluents
needed:

\begin{itemize}
\tightlist
\item
  one to state if a given block is \texttt{down} on the table,
\item
  one to specify which block is \texttt{held} at any moment and
\item
  one to describe which block is stacked \texttt{on} which block.
\end{itemize}

We also need a special block to state when \texttt{noblock} is held or
on top of another block. This block is a constant.

The knowledge we just described is called \emph{planning domain}.

In that example, the initial state is described as stacks and a set of
blocks directly on the table. The goal state is usually the
specification of one or many stacks that must be present on the table.
This part of the description is called \emph{planning problem}.

In order to solve it we must find a valid sequence of actions called a
\emph{plan}. If this plan can be executed in the initial state and
result in the goal state it is called a \emph{solution} of the planning
problem. To be executed, each action must be done in a state satisfying
its precondition and will alter that state according to its effects. A
plan can be executed if all its action can be executed in the sequence
of the plan. For example, in the block world domain we can have an
initial state with the \(blockB\) ontop of \(blockA\) and the \(blockC\)
being on the table. In \cref{fig:plan}, we give a plan solution to the
problem consisting of having the stack
\(\langle blockA, blockB, blockC \rangle\) from that initial state.

\begin{figure}
\hypertarget{fig:plan}{%
\centering
\includegraphics{graphics/plan1.svg}
\caption{An example of a solution to a planning problem with a goal
requiring three block stacked in alphabetical order.}\label{fig:plan}
}
\end{figure}

Every automated planner aims to find at least one such solution in any
way shape or form in the least amount of time with the best plan
quality. The quality of a plan is often measured by how hard it is to
execute, whether by its execution time or by the ressources needed to
accomplish it. This metric is often called \emph{cost} of a plan and is
often simply the sum of the cost of its actions.

Automated planning is very diverse. A lot of paradigms shifts the
definition of domain, actions and even plan to widely varying extents.
This is the reason why making a general planning formalism was deamed so
hard or even impossible:

\begin{quote}
``\emph{It would be unreasonable to assume there is one single compact
and correct syntax for specifying all useful planning problems.}''
\hfill ({\textbf{???}})
\end{quote}

Indeed, the block world example domain we give is mostly theoritical
since there are infinitely more subtelty into this problem such as
mechatronic engineering, balancing issues and partial ability to observe
the environement and predict its evolution as well as failure in the
execution. In our example, we didn't mentioned the misplaced \(blockD\)
that could very well interefer with any exectution in unpredicted ways.
This is why so many planning paradigms exists and why they are all so
diverse: they try to address an infinitely complex problem, one
sub-problem at a time. In doing so we lose the general view of the
problem and by simply stating that this is the only way to resolve it we
close ourself to other approaches that can become successful. Like once
said:

\begin{quote}
``\emph{The easiest way to solve a problem is to deny it exists.}''
\hfill Asimov (\protect\hyperlink{ref-asimov_gods_1973}{1973})
\end{quote}

However, In the next section we aim to create such a general planning
formalism. The main goal is to provide the automated planning comunity
with a general unifying framework it so badly need.

\hypertarget{formalism}{%
\section{Formalism}\label{formalism}}

In this section, a general formalism of automated planning is proposed.
The goal is to explain what is planning and how it works. First we must
express the knowledge domain formalism, then we describe how problems
are represented and lastly how a general planning algorithm can be
envisioned.

\hypertarget{planning-domain}{%
\subsection{Planning domain}\label{planning-domain}}

\hypertarget{tbl:planning}{}
\begin{table}\footnotesize
\centering

\caption{\label{tbl:planning}List of classical symbols and syntax for
planning.}

\begin{tabular}{@{}ll@{}}
\toprule

\textbf{Symbol} & \textbf{Description} \\\midrule

\(F, \square, A\) & Sets of fluents, states and actions. \\
\(\otimes, \odot^\pm\) & Sets of flaws and signed resolvers. Flaws have
variants: \\
\(\otimes^\downarrowbarred\) & • unsupported subgoal. \\
\(\otimes^\dagger\) & • causal threat to an existing causal link. \\
\(\otimes^\circlearrowleft\) & • cycle in the plan. \\
\(\otimes^\ast\) & • decomposition of a compound action. \\
\(\otimes^\curvearrowrightminus\) & • alternative to an existing
action. \\
\(\otimes^{\bar{o}}\) & • orphan action in the plan. \\
\(\bb{\Pi}, \bb{S}\) & Sets of plans and search space. \\
\(\cal{l} \downdasharrow a\) & Partial support of action \(a\) by the
causal link \(\cal{l}\). \\
\(\bb{\pi} \downarrow a\) & Full support of action \(a\) by plan
\(\bb{\pi}\) \\
\(\prec, \succ\) & Precedence and succession relation used for order. \\
\(\Mapsto^*\) & General shortest path algorithm. \\
\(h\) & Search heuristic. \\
\(\bb{p}\) & Planning problem. \\
\(\gamma\) & Constraints on the action. \\
\(¢\) & Cost of an action. \\
\(d\) & Duration of an action. \\
\(\omega\) & Root operator. \\

\bottomrule
\end{tabular}

\end{table}

In order to concieve a general formalism for planning domains, we base
its definition on the formalism of SELF. This means that all part of the
domain must be a member of the universe of discourse \(\bb{U}\).

\hypertarget{fluents}{%
\subsubsection{Fluents}\label{fluents}}

First, we need to define the smallest unit of knowledge in planning, the
fluents.

\begin{definition}[Fluent]

A planning fluent is a predicate \(f(arg_1, arg_2, …, arg_n) \in F\).

Fluents are signed. Negative fluents are noted \(\neg f\) and behave as
a logical complement. We do not use the closed world hypothesis: fluents
are only satisfied when another compatible fluent is provided.

\end{definition}

The name ``fluent'' comes from their fluctuating value. Indeed the truth
value of a fluent is meant to vary with time and specifically by acting
on it. In this formalism we represent fluents using either parameterized
entities of using statements for binary fluents.

\emph{Example}: In our example we have four predicates. They can form
countless fluents like \(held(no-block)\), \(on(blockA, blockB)\) or
\(\neg down(blockA)\). Their when expressing a fluent we suppose its
truth value is \(\top\) and denote falsehood using the negation
\(\neg\).

When expressing states, we need a formalism to express sets of fluents
as formulaes.

\begin{definition}[State]

A state is a set of fluent. It is provided with a truth value like a
fluent and can behave like one. The truth value is the
\emph{conjunction} of all fluents within the state
\(\smwhtsquare \vdash \bigwedge_{f \in \smwhtsquare} f\) denoted by a
small square \(\smwhtsquare\). States can contain other states in which
case their truth value is the \emph{disjunction} of the member's truth
value:
\(\smwhtsquare \vdash \bigvee_{\smwhtsquare' \in \smwhtsquare} \smwhtsquare'\).
This creates an and/or tree with the branches being all \emph{or
vertices} except for the ones connecting to fluents that becomes
\emph{and vertices}. All leaves of the tree are fluents.

\end{definition}

\emph{Example}: In the domain block world, we can express a couple of
states as set of fluents:

\begin{itemize}
\tightlist
\item
  \(\smwhtsquare_1 = \{held(noblock), on(blockA, blockB), down(blockC)\}\)
\item
  \(\smwhtsquare_2 = \{held(blockC), down(blockA), down(blockB)\}\)
\end{itemize}

In such a case, both state \(\smwhtsquare_1\) and \(\smwhtsquare_2\)
have their truth value being the conjunction of all their fluents. In
order to express a disjunction, one has to combine states in the
following way: \(\smwhtsquare_3 = \{\smwhtsquare_1, \smwhtsquare_2\}\).
In that case \(\smwhtsquare_3\) is the root of the and/or tree and all
its direct children are or vertices. The states \(\smwhtsquare_1\) and
\(\smwhtsquare_2\) have their children as \emph{and vertices} since they
are fluents.

\begin{figure}
\hypertarget{fig:state_tree}{%
\centering
\includegraphics{graphics/and-or-state.svg}
\caption{Example of a state encoded as an and/or
tree.}\label{fig:state_tree}
}
\end{figure}

When planning, we need to make operations on states and fluents. Namely,
we need to be able to \emph{verify} them, \emph{unify} them and
\emph{apply} them. These three operations are defined the following way
:

\begin{itemize}
\tightlist
\item
  \textbf{Verify}: \(\smwhtsquare \vdash f = \smwhtsquare \cup \{f\}\)
\item
  \textbf{Unify}:
  \(\smwhtsquare \sqcup f = \rho_i(f) \mapsto \rho_i(f') : f' \in \smwhtsquare \land f' \vdash f\)
\item
  \textbf{Apply}:
  \(f(\smwhtsquare) = \smwhtsquare \cup \{(\smwhtsquare \sqcup f)(f)\}\)
\end{itemize}

Another important part of the behavior of fluents is their ability to
match and to being unified.

Matching is the relation \(f_1 :_\smwhtsquare f_2\) that affects any
pair of fluents \(\langle f_1, f_2 \rangle\) along with a context state
\(\smwhtsquare\) to another state containing the context augmented with
unification constraints or \(\emptyset\) if the two fluents are
contradicting one another given the context. If the two fluents are
equal or have a different function, they do not influence the context.
Opposite fluents will always contradict. The quantified or variable
arguments may cause contradiction or add a constraint into the context.

The actual unification relation \(f_1 \vdash_\smwhtsquare f_2\) bind any
matching fluents to another grounded fluent giving the constraints of
the given context.

Both relations are generalized to states by merging the result of the
following way:

\[\colonvdash(\smwhtsquare_r, \smwhtsquare_f) = \bigcup_{f_r \in \smwhtsquare_f, f_f \in \smwhtsquare_f} \colonvdash(f_r, f_f, \smwhtsquare_a)\]

with, \(\smwhtsquare_r\) being the reference state and
\(\smwhtsquare_f\) being the formula state. The state \(\smwhtsquare_a\)
is an accumulator that is result of the partial union of the previous
matching iterations. This formula is the same for \(:\) and \(\vdash\).
It is interesting to note that the relations doesn't need an additional
context and share the same definition domain \(\square^2 \to \square\)
when taking states as arguments.

\emph{Example}: Using previously defined example states
\(\smwhtsquare_{1,2,3}\), and adding the following:

\begin{itemize}
\tightlist
\item
  \(\smwhtsquare_4(x) = \{held(noblock), down(x)\}\) and
\item
  \(\smwhtsquare_5(y) = \{held(y), \neg down(y)\}\),
\end{itemize}

We can express a few example of fluent matching:

\begin{itemize}
\tightlist
\item
  \(held(noblock) : held(x) = \{x=noblock\}\)
\item
  \(\neg held(x) : held(x) = \emptyset\)
\item
  \(held(blockA) :_{\smwhtsquare_1} held(x) = \emptyset\)
\item
  \(down(blockD) :_{\smwhtsquare_1} down(x) = \smwhtsquare_1 \cup \{x=blockD\}\)
\item
  \(down(blockC) :_{\smwhtsquare_1} down(blockC) = \smwhtsquare_1\)
\end{itemize}

Unification of fluents goes quite simply:

\begin{itemize}
\tightlist
\item
  \(held(noblock) \vdash held(x) = held(noblock)\)
\item
  \(\neg held(x) \vdash held(x) = \emptyset\)
\item
  \(held(blockC) \vdash_{\smwhtsquare_5(y)} held(y) = \{held(blockC), \neg down(blockC)\}\)
\end{itemize}

We also can present matching on states:

\begin{itemize}
\tightlist
\item
  \(\smwhtsquare_1 : \smwhtsquare_2 = \emptyset\)
\item
  \(\smwhtsquare_1 : \smwhtsquare_4(x) = \smwhtsquare_1 \cup \{x=blockC\}\)
\end{itemize}

And unification too:

\begin{itemize}
\tightlist
\item
  \(\smwhtsquare_1 \vdash \smwhtsquare_4(x) = \smwhtsquare_1\)
\end{itemize}

\textbf{FIXME: Type check and see if application is different.}

All these relations are used to check and apply actions.

\hypertarget{actions}{%
\subsubsection{Actions}\label{actions}}

Actions are the main mecanism beind automated planning, they describe
what can be done and how it can be done.

\begin{definition}[Action]\label[definition]{def:action}

An action is a parametrized tuple
\(a(args)=\langle :, \vdash, \gamma, ¢, d, \bb{P}, \bb{\Pi} \rangle\)
where:

\begin{itemize}
\tightlist
\item
  \(:\) and \(\vdash\) are states that are respectively the
  \textbf{preconditions and the effects} of the action.
\item
  \(\gamma\) is the state representing the \textbf{constraints}.
\item
  \(¢\) is the intrisic \textbf{cost} of the action.
\item
  \(\delta\) is the intrinsic \textbf{duration} of the action.
\item
  \(\bb{P}\) is the prior \textbf{probability} of the action succeeding.
\item
  \(\bb{\Pi}\) is a set of \textbf{methods} that decompose the action
  into smaller simpler ones.
\end{itemize}

\end{definition}

Operators take many names in difference planning paradigm : actions,
steps, tasks, etc. In our case we call operators, all fully lifted
actions and actions are all the instances possibles (including
operators).

In order to be more generalistic, we allow in the constraints
description, any time constraints, equalities or inequalities, as well
as probabilistic distributions. These constratints can also express
derived predicates. It is even possible to place arbitrary constraints
on order and selection of actions.

Actions are often represented as state operators that can be applied in
given state to alter it. The application of actions is done by using the
actions as relations \(a : \square \to \square\) defined as follow:
\(a(\smwhtsquare) = \smwhtsquare \vdash_{\smwhtsquare : a} a\)

\[a(\smwhtsquare) = 
\begin{cases}
  \emptyset,& \text{if } \smwhtsquare : a =\emptyset\\
  \smwhtsquare \vdash a,& \text{otherwise}
\end{cases}\]

\emph{Example}: A useful action we can define from previously defined
states is the following:

\[pickup(x) = \langle \smwhtsquare_4(x), \smwhtsquare_5(x), (x: Block), 1.0¢, 3.5s, 75\%, \emptyset \rangle\]

That action can pickup a block \(x\) in \(3.5\) seconds using a cost of
\(1.0\) with a prior success probability of \(75\%\).

\hypertarget{domain}{%
\subsubsection{Domain}\label{domain}}

\begin{figure}
\hypertarget{fig:color}{%
\centering
\includegraphics{graphics/color_structure.svg}
\caption{Venn diagram extended from the one from SELF to add all
planning knowledge representation.}\label{fig:color}
}
\end{figure}

The domain specifies the allowed operators that can be used to plan and
all the fluents they use as preconditions and effects.

\begin{definition}[Domain]

A domain \(\cal{D}\) is a set of \textbf{operators} which are fully
lifted \emph{actions}, along with all the relations and entities needed
to describe their preconditions and effects.

\end{definition}

\emph{Example}: In the previous examples the domain was named block
world. It consists in four actions: \(pickup, putdown, stack\) and
\(unstack\). Usually the domain is self contained, meaning that all
fluents, types, constants and operators are contained in it.

\hypertarget{planning-problem}{%
\subsection{Planning problem}\label{planning-problem}}

The aim of an automatted planner is to find a plan satisfying the goal.
This plan can be of multiple forms, and there can even be multiple plans
that meet the demand of the problem.

\hypertarget{solution}{%
\subsubsection{Solution}\label{solution}}

\begin{definition}[Partial Plan / Method]

A partially ordered plan is an \emph{acyclic} directed graph
\(\bb{\pi} = (A_{\bb{\pi}}, E)\), with:

\begin{itemize}
\tightlist
\item
  \(A_{\bb{\pi}}\) the set of \textbf{steps} of the plan as vertices. A
  step is an action belonging in the plan. \(A_{\bb{\pi}}\) must contain
  an initial step \(a_{\bb{\pi}}^0\) and goal step \(a_{\bb{\pi}}^*\) as
  convinience for certain planning paradigms.
\item
  \(E\) the set of \textbf{causal links} of the plan as edges. We note
  \(l = a_s \xrightarrow{\smwhtsquare} a_t\) the link between its source
  \(a_s\) and its target \(a_t\) caused by the set of fluents
  \(\smwhtsquare\). If \(\smwhtsquare = \emptyset\) then the link is
  used as an ordering constraint.
\end{itemize}

\end{definition}

This definition can express any kind of plans, either temporal, fully or
partially ordered or even lose hierarchical plans (using the methods of
the actions). It can even express diverse planning results.

In our framework, \emph{ordering constraints} are defined as the
transitive cover of causal links over the set of steps. We note ordering
constraints: \(a_a \succ a_s\), with \(a_a\) being \emph{anterior} to
its \emph{successor} \(a_s\). Ordering constraints cannot form cycles,
meaning that the steps must be different and that the successor cannot
also be anterior to its anterior steps:
\(a_a \neq a_s \land a_s \not \succ a_a\). If we need to enforce order,
we simply add a link without specifying a cause. The use of graphs and
implicit order constraints help to simplify the model while maintaining
its properties. Totally ordered plans are done by specifying links
between all successive actions of the sequence.

\emph{Example}: In the \cref{sec:plan_example}, we described a classical
fully ordered plan, illustrated in \cref{fig:plan}. A partially ordered
plan has a tree-like structure except that it also meet in a ``sink''
vertex (goal step). We explicit this structure in \cref{fig:poplan}.

\begin{figure}
\hypertarget{fig:poplan}{%
\centering
\includegraphics{graphics/poplan.svg}
\caption{Example of the structure of a partially ordered
plan.}\label{fig:poplan}
}
\end{figure}

\hypertarget{problem}{%
\subsubsection{Problem}\label{problem}}

With this formalism, the problem is very simplified but still general.

\begin{definition}[Problem]

The planning problem is defined as the \textbf{root operator} \(\omega\)
which methods are potential solutions of the problem. Its preconditions
and effects are respectively used as initial state and goal description.

\end{definition}

As actions are very general, it is interesting to make the problem and
domain space homogenous by using an action to describe any problem.

Most of the specific notions of this framework are optionnal. Any
planner using it will probably define what features it supports when
compiling input domains and problems.

All notions explained so far are represented in the \cref{fig:color}
adding to the SELF Venn diagram.

\hypertarget{planning-algorithm}{%
\subsection{Planning algorithm}\label{planning-algorithm}}

The general planning algorithm can be described as a guided exploration
of a search space. The detailed structure of the search space as well as
search iterators are dependant on the planning paradigm used.

\hypertarget{search-space}{%
\subsubsection{Search space}\label{search-space}}

\begin{definition}[Planner]

A planning algorithm, often called planner, is an exploration of a
search space \(\bb{S}\) partially ordered by an iterator
\(\phi^+_{\bb{S}}\) guided by a heuristic \(h\). From any problem
\(\bb{p}\) every planner can derive two informations imediately:

\begin{itemize}
\tightlist
\item
  the starting point \(\bb{s}_0 \in \bb{S}\) and
\item
  the solution predicate \(?_{\bb{s}^*}\) that gives the validity of any
  potential solution in the search space.
\end{itemize}

Formally the problem can be exprimed as a pathfinding problem in the
dirrected graph \(g_{\bb{S}}\) formed by the vertex set \(\bb{S}\) and
the adjacence function \(\phi^+_{\bb{S}}\). The set of solutions is
therefore expressed as:

\[\bb{S}^* = 
\left \{ \bb{s}^* : 
  \langle \bb{s}_0, \bb{s}^* \rangle \in E_{\chi^+(g_{\bb{S}})} 
  \land ?_{\bb{s}^*})  
\right \}\]

\end{definition}

In automated planning there are also other considerations about the
search.

\hypertarget{diversity}{%
\subsubsection{Diversity}\label{diversity}}

Sometimes, it is necessary to find alternatives. Since re-planning from
scratch is computationally demanding, it is better to find several
solutions at once. This approach is called \emph{diverse planning}. It
aims to find \(k\) solutions that deviates from one another
significantly. This simply make the process return when either it found
\(k\) solutions or when it determined that \(k > |\bb{S}^*|\).

\hypertarget{temporality}{%
\subsubsection{Temporality}\label{temporality}}

Another aspect of planning lies in its timming. Indeed sometimes acting
needs to be done before a dealine and planning is useful only durring a
finite timeframe. We add a predicate that specifies time constraints
over algorithms \(t : \bb{A} \to \bb{A}\). This constraint has three
main type of application:

\begin{itemize}
\tightlist
\item
  \(t_\acidfree\): Optimal search without time limitation, finding the
  best solution everytime.
\item
  \(t_CHRONO\): Anytime search, finding a solution and improving its
  quality until stopped.
\item
  \(t_TIMEOUT\): Real-time search, being able to give a solution in a
  given time even if it is an approximation.
\end{itemize}

\hypertarget{general-planner}{%
\subsubsection{General planner}\label{general-planner}}

A general planner
\(\bb{C}^*[\bb{S},\phi^+_{\bb{S}},h,\to](\cal{D},\bb{p})\) is an
algorithm that can plan any formalism of automated planning. It takes
two set of parameters:

\begin{itemize}
\tightlist
\item
  \textbf{Formalism dependant parameters}

  \begin{itemize}
  \tightlist
  \item
    \(\bb{S}\) the search space
  \item
    \(\phi^+_{\bb{S}}\) the search iterator
  \item
    \(h\) the heuristic
  \item
    \(\to\) the problem transformation function
  \end{itemize}
\item
  \textbf{Domain depedant parameters}

  \begin{itemize}
  \tightlist
  \item
    \(\cal{D}\) the \emph{planning domain}
  \item
    \(\bb{p}\) the \emph{planning problem}
  \end{itemize}
\end{itemize}

The heuristic \(h(\bb{s})\) gives off the shortest predicted distance to
any point of the solution space. The exploration is guided by it by
minimizing its value.

Other informations must be added to any problem \(\bb{p}\) in the form
of constraints:

\[\left( t(\bb{C}) \veeonwedge |\bb{\Pi}(\omega)| = k \right) \in c(\omega)\]

The value for \(k\) is extracted from the problem and the temporality is
expressed using either \(\land\), \(\lor\) or \(\land \bot \lor\)
instead of \(\veeonwedge\).

The transformation function
\(\bb{p} \to \langle \bb{s}_0, ?_{\bb{s}^*} \rangle\) gives the starting
point \(\bb{s}_0\) and the solution predicate \(?_{\bb{s}^*}\). This
predicate is derived from the problem description and its contraints.

For the algorithm itself, we simply use a parameterized instance of the
K* algorithm (Aljazzar and Leue
\protect\hyperlink{ref-aljazzar_heuristic_2011}{2011}, alg. 1). This
algorithm uses the classical algorithm A* to explore the graph while
using Dijkstra on some sections to find the \(k\) shortest paths. The
parameters are as follow:
\(K^*(g_{\bb{S}}, \bb{s}_0, ?_{\bb{s}^*}, h)\). The solution predicate
contains the expression of the restriction of \(k\) solutions,
therefore, it superseeds the need for the \(k\) parameter. We also add
the heuristic \(h\) to guide the A* part of the algorithm.

Of course this algorithm is merely an example of a general planner
algorithm. Its efficiency hasn't been tested.

\begin{figure}
\hypertarget{fig:gplanner}{%
\centering
\includegraphics{graphics/general_planner.svg}
\caption{Venn diagram extended with general planning
formalism.}\label{fig:gplanner}
}
\end{figure}

\hypertarget{classical-formalisms}{%
\section{Classical Formalisms}\label{classical-formalisms}}

One of the most comprehensive work on sumarizing the automated planning
domain was done by Ghallab \emph{et al.}
(\protect\hyperlink{ref-ghallab_automated_2004}{2004}). This book
explains the different planning paradigm of its time and gives formal
description of some of them. This work has been updated later (Ghallab
\emph{et al.} \protect\hyperlink{ref-ghallab_automated_2016}{2016}) to
reflect the changes occuring in the planning community.

\hypertarget{state-transition-planning}{%
\subsection{State-transition planning}\label{state-transition-planning}}

The most classical representation of automated planning is using the
state transition approach: actions are operators on the set of states
and a plan is a finite-state automaton. We can also see that problem
description as either a graph exploration problem or even a constraint
satisfaction problem. In any way that problem is isomorph to its
original formulation and most efficient algorithms use a derivative of
A* exploration techniques on the state space.

This makes this kind of planning quite simple to instantiate from the
general planner:
\[\bb{C}_{state} = \bb{C}^* \left[ \square, \bigcup_{a \in A} a(\bb{s}), h, :(\omega), \vdash(\omega) \right]\]

For this formalism, we often set \(k=1\) and
\(t(\bb{C}_{state}) = t_\acidfree\) as is customary in classical
planning. It can also be specified as a backward search by inverting the
application of \(a\) with \(a^{-1}\) and having the starting state as
\(:(\omega)\) and solution predicate as \(\vdash(\omega)\).

State based planning usually supose total knowledge of the state space
and action behavior. No concurency or time constraints are expressed and
the state and action space must be finite as well as the resulting state
graph. This process is also deterministic and doesn't allow uncertainty.
The result of such a planning is a totally ordered sequence of actions
called a plan. The total order needs to be enforced even if it is
unecessary.

All those features are important in practice and lead to other planning
paradigms that are more complex than classical state based planning.

\hypertarget{plan-space-planning}{%
\subsection{Plan space planning}\label{plan-space-planning}}

Plan Space Planning (PSP) is a form of planning that use plan space as
its search space. It starts with an empty plan and try to iteratively
refine that plan into a solution.

\[\bb{C}_{psp} = \bb{C}^* \left[ \bb{\Pi}, \bigcup_{f \in \otimes(\bb{s})}^{r \in \odot^\pm_f} r(\bb{s}), h, \left(\{a_{\bb{\pi}}^0, a_{\bb{\pi}}^*\},\{a_{\bb{\pi}}^0\rightarrow a_{\bb{\pi}}^*\} \right), \otimes(\bb{s}) = \emptyset \right]\]

with \(a_{\bb{\pi}}^0\) and \(a_{\bb{\pi}}^*\) being the intial and goal
steps of the plan \(\bb{s}_0\) such that
\(\vdash(a_{\bb{\pi}}^0) = :(\omega)\) and
\(:(a_{\bb{\pi}}^*) = \vdash(\omega)\). The iterator is all the possible
resolutions of all flaws on any plan \(\bb{s}\) and the solution
predicate is true when the plan has no more flaws.

Details about flaws, resolver and the overall Partial Order Causal Links
(POCL) algorithm will be presented \textbf{LATER}.

This approach usually can give a partial plan if we set
\(t(\bb{C}_{psp}) < t(\exists \bb{\pi} \in \bb{\Pi} \land ?_{\bb{s}}(\bb{\pi}))\).
This plan is not a solution but can eventually be engineered into having
approximative properties relative to a solution.

\hypertarget{case-based-planning}{%
\subsection{Case based planning}\label{case-based-planning}}

Another plan oriented planning is called Case-Based Planning (CBP). This
kind of planning relies on a library of already complete plans and try
to find the most appropriate one to repair.

\[\bb{C}_{cbp} = \bb{C}^* \left[ \cal{L}^{\bb{\pi}}, \odot(\bb{s}), h, \sigma^*(\cal{L}^{\bb{\pi}}, \omega), \bb{s}(:(\omega)) \Downarrow \vdash(\omega) \right ]\]

with \(\cal{L}^{\bb{\pi}}\) being the plan library. The planner selects
efficiently a plan that fit the best with the intial and goal state of
the problem. This plan is then repaired and validated iteratively. The
problem with this approach is that it may be unable to find a valid plan
or might need to populate and maintain a good plan library. For such
case an auxiliary planner is used (preferably diverse with \(k>1\)).

\hypertarget{probabilistic-planning}{%
\subsection{Probabilistic planning}\label{probabilistic-planning}}

\hypertarget{tbl:proba}{}
\begin{table}\footnotesize
\centering

\caption{\label{tbl:proba}List of classical symbols and syntax for
probabilities.}

\begin{tabular}{@{}ll@{}}
\toprule

\textbf{Symbol} & \textbf{Description} \\\midrule

\(\bb{P}(\cal{e})\) & Probability of event \(\cal{e}\). \\
\(\cal{O}\) & Set of observations. \\
\(\rupee\) & Reward function. \\

\bottomrule
\end{tabular}

\end{table}

\textbf{FIXME} Reward using rupee symbol \(\rupee\)

Probabilistic planning tries to deal with uncertainty by generating a
policy instead of a plan. The initial problem holds probability laws
that govern the execution of any actions. It is sometimes accompagnated
with a reward function instead of a deterministic goal.

\[\bb{C}_{prob} = \bb{C}^* \left[ \square \times A, \bb{s}_{+1} = \bigcup_{a \in A}^{a(\smwhtsquare) \neq \emptyset} \langle \pi_{\langle \smwhtsquare', a' \rangle \to a'(\smwhtsquare')}\sigma(\bb{s}_{+1}), a \rangle, h_\rupee, \bigcup_{a \in A}^{a(:(\omega)) \neq \emptyset} \langle :(\omega), a \rangle , \smwhtsquare \vdash \omega \right]\]

The state \(\smwhtsquare\) is a state chosen from the frontiere. The
frontiere is updated at each iteration with the application of a
non-deterministically chosen pair of the last policy insertion. The
search stops when all elements in the frontiere are goal states.

\hypertarget{hierarchical-planning}{%
\subsection{Hierarchical planning}\label{hierarchical-planning}}

Hierarchical Task Networks (HTN) are a totally different kind of
planning paradigm. Instead of a goal description, HTN uses a root task
that needs to be decomposed. The task decomposition is an operation that
replaces a task (action) by one of its methods \(\Pi\).

\[\bb{C}_{htn} = \bb{C}^* = \left [ \bb{\Pi}, \left(\sigma\{a\in A_{\bb{s}} : \bb{\Pi}_a \neq \emptyset\} \to\sigma(\bb{\Pi}_a)\right)(\bb{s}), h, \omega, \forall a \in A_{\bb{s}} : \bb{\Pi}(a) = \emptyset \right ]\]

\hypertarget{existing-languages-and-frameworks}{%
\section{Existing Languages and
Frameworks}\label{existing-languages-and-frameworks}}

\hypertarget{classical}{%
\subsection{Classical}\label{classical}}

After STRIPS, one of the first language to be introduced to express
planning domains like ADL (Pednault
\protect\hyperlink{ref-pednault_adl_1989}{1989}). That formalism adds
negation and conjunction into literals to STRIPS. It also drops the
closed world hypothesis for an open world one: anything not stated in
conditions (initial or action effects) is unknown.

The current standard was strongly inspired by Penberthy \emph{et al.}
(\protect\hyperlink{ref-penberthy_ucpop_1992}{1992}) and his UCPOP
planner. Like STRIPS, UCPOP had a planning domain language that was
probably the most expressive of its time. It differs from ADL by merging
the add and delete lists in effects and to change both preconditions and
effects of actions into logic formula instead of simple states.

The PDDL language was created for the first major automated planning
competition hosted by AIPS in 1998 ({\textbf{???}}). It came along with
a syntax and solution checker writen in Lisp. It was introduced as a way
to standardize notation of planning domains and problems so that
libraries of standard problems can be used for benchmarks. The main goal
of the language was to be able to express most of the planning problems
of the time.

With time, the planning competitions became known under the name of
International Planning Competitions (IPC) regularly hosted by the ICAPS
conference. With each installment, the language evolved to address
issues encountered the previous years. The current version of PDDL is
3.1 (Kovacs \protect\hyperlink{ref-kovacs_bnf_2011}{2011}). Its syntax,
goes similarly as described in \cref{lst:pddl_syntax}.

\begin{codelisting}

\caption{Simplified explanation of the syntax of PDDL.}

\hypertarget{lst:pddl_syntax}{%
\label{lst:pddl_syntax}}%
\begin{verbatim}
(define (domain <domain-name>)
  (:requirements :<requirement-name>)
  (:types <type-name>)
  (:constants <constant-name> - <constant-type>)
  (:predicates (<predicate-name> ?<var> - <var-type>))
  (:functions   (<function-name> ?<var> - <var-type>) - <function-type>)

  (:action <action-name>
         :parameters (?<var> - <var-type>)
         :precondition (and (= (<function-name> ?<var>) <value>) (<predicate-name> ?<var>))
         :effect
         (and (not (<predicate-name> ?<var>))
           (assign (<function-name> ?<var>) ?<var>)))
\end{verbatim}

\end{codelisting}

PDDL uses the functional notation style of LISP. It defines usually two
files: one for the domain and one for the problem instance. The domain
describes constants, fluents and all possible actions. The problem lays
the initial and goal states description.

For example, consider the classic block world domain expressed in
\cref{lst:block_pddl}. It uses a predicate to express whether a block is
on the table because several blocks can be on the table at once. However
it uses a 0-ary function to describe the one block allowed to be held at
a time. The description of the stack of blocks is done with an unary
function to give the block that is on top of another one. To be able to
express the absence of blocks it uses a constant named
\texttt{no-block}. All the actions described are pretty straightforward:
\texttt{stack} and \texttt{unstack} make sure it is possible to add or
remove a block before doing it and \texttt{pick-up} and
\texttt{put-down} manages the handling operations.

\begin{codelisting}

\caption{Classical PDDL 3.0 definition of the domain Block world}

\hypertarget{lst:block_pddl}{%
\label{lst:block_pddl}}%
\begin{verbatim}
(define (domain BLOCKS-object-fluents)
  (:requirements :typing :equality :object-fluents)
  (:types block)
  (:constants no-block - block)
  (:predicates (on-table ?x - block))
  (:functions   (in-hand) - block
        (on-block ?x - block) - block) ;;what is in top of block ?x

  (:action pick-up
         :parameters (?x - block)
         :precondition (and (= (on-block ?x) no-block) (on-table ?x) (= (in-hand) no-block))
         :effect
         (and (not (on-table ?x))
           (assign (in-hand) ?x)))

  (:action put-down
         :parameters (?x - block)
         :precondition (= (in-hand) ?x)
         :effect
         (and (assign (in-hand) no-block)
           (on-table ?x)))

  (:action stack
         :parameters (?x - block ?y - block)
         :precondition (and (= (in-hand) ?x) (= (on-block ?y) no-block))
         :effect
         (and (assign (in-hand) no-block)
          (assign (on-block ?y) ?x)))

  (:action unstack
         :parameters (?x - block ?y - block)
         :precondition (and (= (on-block ?y) ?x) (= (on-block ?x) no-block) (= (in-hand) no-block))
         :effect
         (and (assign (in-hand) ?x)
          (assign (on-block ?y) no-block))))
\end{verbatim}

\end{codelisting}

However, PDDL is far from an universal standard. Some efforts have been
made to try and standardize the domain of automated planning in the form
of optional requirements. The latest of the PDDL standard is the version
3.1 (Kovacs \protect\hyperlink{ref-kovacs_bnf_2011}{2011}). It has 18
atomic requirements as represented in \cref{fig:pddl_req}. Most
requirements are parts of PDDL that either increase the complexity of
planning significantly or that require extra implementation effort to
meet.

\begin{figure}
\hypertarget{fig:pddl_req}{%
\centering
\includegraphics{graphics/pddl_requirements.svg}
\caption{Dependancies and grouping of PDDL
requirements.}\label{fig:pddl_req}
}
\end{figure}

Even with that flexibility, PDDL is unable to cover all of automated
planning paradigms. This caused most subdomains of automated planning to
be left in a state similar to before PDDL: a zoo of languages and
derivatives that aren't interoperable. The reason for this is the fact
that PDDL isn't expressive enough to encode more than a limited
variation in action and fluent description.

Another problem is that PDDL isn't made to be used by planners to help
with their planning process. Most planners will totally separate the
compilation of PDDL before doing any planning, so much so that most
planners of the latest IPC used a framework that translates PDDL into a
useful form before planning, adding computation time to the planning
process. The list of participating planners and their use of language is
presented in \cref{tbl:ipc}.

\hypertarget{tbl:ipc}{}
\begin{table}\footnotesize
\centering

\caption{\label{tbl:ipc}Planners participating in the Classic track of
the 2018 International Planning Competition (IPC). The table states
whether the planner used a translation and a preprocessing system to
handle PDDL. Most of the planners are based on FastDownward directly.}

\begin{tabular}{@{}llllll@{}}
\toprule

Name & Trans & Pre & Lang & Base & Rank \\\midrule

Delfi & Yes & Yes & C++ & FD & 1 \\
Complementary & Yes & Yes & C++ & FD & 2 \\
Planning-PDBs & Yes & Yes & C++ & FD & 3 \\
Scorpion & Yes & Yes & C++ & FD & 4 \\
FDMS & Yes & Yes & C++ & FD & 5 \\
DecStar & Yes & Yes & C++ & LAMA & 6 \\
Metis & Yes & Yes & C++ & FD & 7 \\
MSP & Yes & Yes & Lisp & FD & 8 \\
Symple & Yes & Yes & C++ & FD & 9 \\
Ma-plan & No & Yes & C & None & 10 \\

\bottomrule
\end{tabular}

\end{table}

The domain is so diverse that attempts to unify it haven't succeeded so
far. The main reason behind this is that some paradigms are vastly
different from the classical planning description. Sometimes just adding
a seamingly small feature like probabilities or plan reuse can make for
a totally different problem. In the next section we describe planning
paradigms and how they differ from classical planning along with their
associated languages.

\hypertarget{temporality-oriented}{%
\subsection{Temporality oriented}\label{temporality-oriented}}

When planning, time can become a sensitve constraint. Some critical
tasks may required to be completed within a certain time. Actions with
durations are already a feature of PDDL 3.1. However, PDDL might not
provide support for external events (i.e.~events occuring independant
from the agent). To do this one must use another language.

\hypertarget{pddl}{%
\subsubsection{PDDL+}\label{pddl}}

PDDL+ is an extension of PDDL 2.1 that handles process and events (Fox
and Long \protect\hyperlink{ref-fox_pddl_2002}{2002}). It can be viewed
as similar to PDDL 3.1 continious effects but it differs on the
expressivity. A process can have an effect on fluents at any time. They
can happen either from the agent's own doing or being purely
environemental. It might be possible in certain cases to modelize this
using the durative actions, continious effects and timed initial
literals of PDDL 3.1.

In \cref{lst:pddl_plus}, we reproduce an example from Fox and Long
(\protect\hyperlink{ref-fox_pddl_2002}{2002}). It shows the syntax of
durative actions in PDDL+. The timed precondition are also available in
PDDL 3.1, but the \texttt{increase} and \texttt{decrease} rate of
fluents is an exclusive feature of PDDL+.

\begin{codelisting}

\caption{Example of PDDL+ durative action from Fox's paper.}

\hypertarget{lst:pddl_plus}{%
\label{lst:pddl_plus}}%
\begin{verbatim}
(:durative-action downlink
    :parameters (?r - recorder ?g - groundStation)
    :duration (> ?duration 0)
    :condition (and (at start (inView ?g))
                    (over all (inView ?g))
                    (over all (> (data ?r) 0)))
    :effect (and (increase (downlinked)
                      (* #t (transmissionRate ?g)))
                 (decrease (data ?r)
                      (* #t (transmissionRate ?g)))))
\end{verbatim}

\end{codelisting}

The main issue with durative actions is that time becomes a continous
ressource that may change the values of fluents. The search for a plan
in that context has a higher complexity than regular planning.

\hypertarget{probabilistic}{%
\subsection{Probabilistic}\label{probabilistic}}

Sometimes, acting can become unpredictable. An action can fail for many
reasons, from logical errors down to physical constraints. This call for
a way to plan using probabilities with the ability to recover from any
predicted failures. PDDL doesn't support using probabilities. That is
why all IPC's tracks dealing with it always used another language than
PDDL.

\hypertarget{ppddl}{%
\subsubsection{PPDDL}\label{ppddl}}

PPDDL is such a language. It was used during the 4\textsuperscript{th}
and 5\^{}th IPC for its probabilistic track (Younes and Littman
\protect\hyperlink{ref-younes_ppddl_2004}{2004}). It allows for
probabilistic effects as demonstrated in \cref{lst:ppddl}. The planner
must take into account the probability when chosing an action. The plan
must be the most likely to succeed. But even with the best plan, failure
can occur. This is why probabilistic planning often gives policies
instead of a plan. A policy dictates the best choice in any given state,
failure or not. While this allows for much more resilient execution,
computation of policies are exponentially harder than classical
planning. Indeed the planner needs to take into account every outcome of
every action in the plan and react accordingly.

\begin{codelisting}

\caption{Example of PPDDL use of probabilistic effects from Younes' paper.}

\hypertarget{lst:ppddl}{%
\label{lst:ppddl}}%
\begin{verbatim}
(define (domain bomb-and-toilet)
    (:requirements :conditional-effects :probabilistic-effects)
    (:predicates (bomb-in-package ?pkg) (toilet-clogged)
                  (bomb-defused))
    (:action dunk-package
             :parameters (?pkg)
             :effect (and (when (bomb-in-package ?pkg)
                                (bomb-defused))
                          (probabilistic 0.05 (toilet-clogged)))))
\end{verbatim}

\end{codelisting}

\hypertarget{rddl}{%
\subsubsection{RDDL}\label{rddl}}

Another language used by the 7\textsuperscript{th} IPC's uncertainty
track is RDDL ({\textbf{???}}). This language has been choosen because
of its ability to express problems that are hard to encode in PDDL or
PPDDL. Indeed, RDDL is capable of expressing Partially Observable
Markovian Decision Process (POMDP) and Dynamic Bayesian Networks (DBN)
in planning domains. This along with complex probability laws allows for
easy implementation of most probabilistic planning problems. Its syntax
differs greatly from PDDL, and seems closer to scala or C++. An example
is provided in \cref{lst:rddl} from ({\textbf{???}}). In it, we can see
that actions in RDDL doesn't need preconditions or effects. In that case
the reward is the closest information to the classical goal and the
action is simply a parameter that will influence the probability
distribution of the events that conditioned the reward.

\begin{codelisting}

\caption{Example of RDDL syntax by Sanner.}

\hypertarget{lst:rddl}{%
\label{lst:rddl}}%
\begin{verbatim}
////////////////////////////////////////////////////////////////////////
// A simple propositional 2-slice DBN (variables are not parameterized).
//
// Author: Scott Sanner (ssanner [at] gmail.com)
////////////////////////////////////////////////////////////////////////
domain prop_dbn {
    
    requirements = { reward-deterministic };
        
    pvariables { 
        p : { state-fluent,  bool, default = false };
        q : { state-fluent,  bool, default = false };
        r : { state-fluent,  bool, default = false }; 
        a : { action-fluent, bool, default = false }; 
    };
  
    cpfs {
        // Some standard Bernoulli conditional probability tables
        p´ = if (p ^ r) then Bernoulli(.9) else Bernoulli(.3);
                        
        q´ = if (q ^ r) then Bernoulli(.9) 
                        else if (a) then Bernoulli(.3) else Bernoulli(.8);

        // KronDelta is like a DiracDelta, but for discrete data (boolean or int)
        r´ = if (~q) then KronDelta(r) else KronDelta(r <=> q);                                     
    };

    // A boolean functions as a 0/1 integer when a numerical value is needed    
    reward = p + q - r; // a boolean functions as a 0/1 integer when a numerical value is needed
}
        
instance inst_dbn {

    domain = prop_dbn;  
    init-state { 
        p = true;  // could also just say 'p' by itself 
        q = false; // default so unnecessary, could also say '~q' by itself
        r;         // same as r = true
    };
  
    max-nondef-actions = 1;
    horizon  = 20;
    discount = 0.9;
}
\end{verbatim}

\end{codelisting}

\hypertarget{multi-agent}{%
\subsection{Multi-agent}\label{multi-agent}}

Planning can also be a collective effort. In some cases, a system must
account for other agents trying to either cooperate or compete in
achieving similar goals. The problem that arise is coordination. How to
make a plan meant to be executed with several agents concurently ?
Several multi-agent action languages have been proposed to answer that
question.

\hypertarget{mapl}{%
\subsubsection{MAPL}\label{mapl}}

Another extension of PDDL 2.1, MAPL was introduced to handle
synchronization of actions (Brenner
\protect\hyperlink{ref-brenner_multiagent_2003}{2003}). This is done
using modal operators over fluents. In that regard, MAPL is closer to
the PDDL+ extension proposed earlier. It introduce durative actions that
will later be integrated into the PDDL 3.0 standard. MAPL also introduce
a synchronization mechanism using speech as a comunication vector. This
seems very specific as explicit comunication isn't a requirement of
collaborative work. \cref{lst:mapl} is an example of the syntax of MAPL
domains. PDDL 3.0 seems to share a similar syntax.

\begin{codelisting}

\caption{Example of MAPL syntax by Brenner.}

\hypertarget{lst:mapl}{%
\label{lst:mapl}}%
\begin{verbatim}
(:state-variables
  (pos ?a - agent) - location
  (connection ?p1 ?p2 - place) - road
  (clear ?r - road) - boolean)
(:durative-action Move
  :parameters (?a - agent ?dst - place)
  :duration (:= ?duration (interval 2 4))
  :condition
    (at start (clear (connection (pos ?a) ?dst)))
  :effect (and
    (at start (:= (pos ?a) (connection (pos ?a) ?dst)))
    (at end (:= (pos ?a) ?dst))))
\end{verbatim}

\end{codelisting}

\hypertarget{ma-pddl}{%
\subsubsection{MA-PDDL}\label{ma-pddl}}

Another aspect of multi-agent planning is the ability to affect tasks
and to manage interactions between agents efficiently. For this MA-PDDL
seems more adapted than MAPL. It is an extension of PDDL 3.1, that makes
easier to plan for a team of heteroneous agents (Kovács
\protect\hyperlink{ref-kovacs_multiagent_2012}{2012}). In the example in
\cref{lst:ma-pddl}, we can see how action can be affected to agents.
While it makes the representation easier, it is possible to obtain
similar effect by passing an agent object as parameter of an action in
PDDL 3.1. More complex expressions are possible in MA-PDDL, like
referencing the action of other agents in the preconditions of actions
or the ability to affect different goals to different agents. Later on,
MA-PDDL was extended with probabilistic capabilities inspired by PPDDL
(Kovács and Dobrowiecki
\protect\hyperlink{ref-kovacs_converting_2013}{2013}).

\begin{codelisting}

\caption{Example of MA-PDDL syntax by Kovacs.}

\hypertarget{lst:ma-pddl}{%
\label{lst:ma-pddl}}%
\begin{verbatim}
(define (domain ma-lift-table)
(:requirements :equality :negative-preconditions 
               :existential-preconditions :typing :multi-agent) 
(:types agent) (:constants table)
(:predicates (lifted (?x - object) (at ?a - agent ?o - object))
(:action lift :agent ?a - agent :parameters ()
:precondition (and (not (lifted table)) (at ?a table) 
              (exists (?b - agent)
               (and (not (= ?a ?b)) (at ?b table) (lift ?b)))) 
:effect (lifted table)))
\end{verbatim}

\end{codelisting}

\hypertarget{hierarchical}{%
\subsection{Hierarchical}\label{hierarchical}}

Another approach to planning is using Hierarchical Tasks Networks (HTN)
to resolve some planning problem. Instead of searching to satisfy a
goal, HTNs try to find a decomposition to a root task that fit the
initial state requirements and that generate an executable plan.

\hypertarget{umcp}{%
\subsubsection{UMCP}\label{umcp}}

One of the first planner to support HTN domains was UCMP by Erol
\emph{et al.} (\protect\hyperlink{ref-erol_umcp_1994}{1994}). It uses
Lisp like most of the early planning systems. Apparently PDDL was in
part inspired by UCMP's syntax. Like for PDDL, the domain file describes
action (called operators here) and their preconditions and effects
(called postconditions). The syntax is demonstrated in \cref{lst:ucmp}.
The interesting part of that language is the way decomposition is
handled. Each task is expressed as a set of methods. Each method has an
expansion expression that specifies how the plan should be constructed.
It also has a pseudo precondition with modal operators on the
temporality of the validity of the literals.

\begin{codelisting}

\caption{Example of the syntax used by UCMP.}

\hypertarget{lst:ucmp}{%
\label{lst:ucmp}}%
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(constants a b c table) }\CommentTok{; declare constant symbols}
\NormalTok{(predicates on clear) }\CommentTok{; declare predicate symbols}
\NormalTok{(compound-tasks move) }\CommentTok{; declare compound task symbols}
\NormalTok{(primitive-tasks unstack dostack restack) }\CommentTok{; declare primitive task symbols}
\NormalTok{(variables x y z) }\CommentTok{; declare variable symbols}

\NormalTok{(operator unstack(x y)}
\NormalTok{          :pre ((clear x)(on x y))}
\NormalTok{          :post ((~on x y)(on x table)(clear y)))}
\NormalTok{(operator dostack (x y)}
\NormalTok{          :pre ((clear x)(on x table)(clear y))}
\NormalTok{          :post ((~on x table)(on x y)(~clear y)))}
\NormalTok{(operator restack (x y z)}
\NormalTok{          :pre ((clear x)(on x y)(clear z))}
\NormalTok{          :post ((~on x y)(~clear z)(clear y)(on x z)))}

\NormalTok{(declare-method move(x y z)}
\NormalTok{                :expansion ((n restack x y z))}
\NormalTok{                :formula (}\KeywordTok{and}\NormalTok{ (}\KeywordTok{not}\NormalTok{ (veq y table))}
\NormalTok{                              (}\KeywordTok{not}\NormalTok{ (veq x table))}
\NormalTok{                              (}\KeywordTok{not}\NormalTok{ (veq z table))}
\NormalTok{                              (before (clear x) n)}
\NormalTok{                              (before (clear z) n)}
\NormalTok{                              (before (on x y) n)))}

\NormalTok{(declare-method move(x y z)}
\NormalTok{                :expansion ((n dostack x z))}
\NormalTok{                :formula (}\KeywordTok{and}\NormalTok{ (veq y table)}
\NormalTok{                              (before (clear x) n)}
\NormalTok{                              (before (on x y) n)))}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\hypertarget{shop2}{%
\subsubsection{SHOP2}\label{shop2}}

The next HTN planner is SHOP2 by Nau \emph{et al.}
(\protect\hyperlink{ref-nau_shop2_2003}{2003}). It remains to this day,
one of the reference implenentation of an HTN planner. The SHOP2
formalism is quite similar to UCMP's: each method has a signature, a
precondition formula and eventually a decomposition description. This
decomposition is a set of methods like in UCMP. The methods can be also
partially ordered allowing more expressive plans. An example of the
syntax of a method is given in \cref{lst:shop2}.

\begin{codelisting}

\caption{Example of method in the SHOP2 language.}

\hypertarget{lst:shop2}{%
\label{lst:shop2}}%
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(:method}
  \CommentTok{; head}
\NormalTok{    (transport-person ?p ?c2)}
  \CommentTok{; precondition}
\NormalTok{    (}\KeywordTok{and}
\NormalTok{      (at ?p ?c1)}
\NormalTok{      (aircraft ?a)}
\NormalTok{      (at ?a ?c3)}
\NormalTok{      (different ?c1 ?c3))}
  \CommentTok{; subtasks}
\NormalTok{    (:ordered}
\NormalTok{      (move-aircraft ?a ?c1)}
\NormalTok{      (board ?p ?a ?c1)}
\NormalTok{      (move-aircraft ?a ?c2)}
\NormalTok{      (debark ?p ?a ?c2)))}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\hypertarget{hddl}{%
\subsubsection{HDDL}\label{hddl}}

A more recent example of HTN formalism comes from the PANDA framework by
Bercher \emph{et al.}
(\protect\hyperlink{ref-bercher_hybrid_2014}{2014}). This framework is
considered the current standard of HTN planning and allows for great
flexibility in domain description. PANDA takes previous formalism and
generalize them into a new language exposed in \cref{lst:hddl}. That
language was called HDDL after its most used file extention.

\begin{codelisting}

\caption{Example of HDDL syntax as used in the PANDA framework.}

\hypertarget{lst:hddl}{%
\label{lst:hddl}}%
\begin{verbatim}
(define (domain transport)
  (:requirements :typing :action-costs)
  (:types
        location target locatable - object
        vehicle package - locatable
        capacity-number - object
  )
  (:predicates 
     (road ?l1 ?l2 - location)
     (at ?x - locatable ?v - location)
     (in ?x - package ?v - vehicle)
     (capacity ?v - vehicle ?s1 - capacity-number)
     (capacity-predecessor ?s1 ?s2 - capacity-number)
  )

  (:task deliver :parameters (?p - package ?l - location))
  (:task unload :parameters (?v - vehicle ?l - location ?p - package))

  (:method m-deliver
    :parameters (?p - package ?l1 ?l2 - location ?v - vehicle)
    :task (deliver ?p ?l2)
     :ordered-subtasks (and
      (get-to ?v ?l1)
      (load ?v ?l1 ?p)
      (get-to ?v ?l2)
      (unload ?v ?l2 ?p))
  )
  (:method m-unload
    :parameters (?v - vehicle ?l - location ?p - package ?s1 ?s2 - capacity-number)
    :task (unload ?v ?l ?p)
    :subtasks (drop ?v ?l ?p ?s1 ?s2)
  )

  (:action drop
    :parameters (?v - vehicle ?l - location ?p - package ?s1 ?s2 - capacity-number)
    :precondition (and
        (at ?v ?l)
        (in ?p ?v)
        (capacity-predecessor ?s1 ?s2)
        (capacity ?v ?s1)
      )
    :effect (and
        (not (in ?p ?v))
        (at ?p ?l)
        (capacity ?v ?s2)
        (not (capacity ?v ?s1))
      )
  )
)
\end{verbatim}

\end{codelisting}

\hypertarget{hpddl}{%
\subsubsection{HPDDL}\label{hpddl}}

A very recent language proposition was done by RAMOUL
(\protect\hyperlink{ref-ramoul_mixedinitiative_2018}{2018}). He proposes
HPDDL with a simple syntax similar to the one of UCMP. In
\cref{lst:hpddl} we give an example of HPDDL method. Its expressive
power seems similar to that of UCMP and SHOP. Except for a possible
commercial integration with PDDL4J (Pellier and Fiorino
\protect\hyperlink{ref-pellier_pddl4j_2017}{2017}), there doesn't seem
to have any advantages compared to earlier works.

\begin{codelisting}

\caption{Example of HPDDL syntax as described by Ramoul.}

\hypertarget{lst:hpddl}{%
\label{lst:hpddl}}%
\begin{verbatim}
(:method do_navigate
  :parameters(?x - rover ?from ?to - waypoint)
  :expansion((tag t1 (navigate ?x ?from ?mid))
             (tag t2 (visit ?mid))
             (tag t3 (do_navigate ?x ?mid ?to))
             (tag t4 (unvisited ?mid)))
  :constraints((before (and (not (can_traverse ?x ?from ?to)) (not (visited ?mid))
                            (can_traverse ?x ?from ?mid)) t1)))
\end{verbatim}

\end{codelisting}

\hypertarget{ontological}{%
\subsection{Ontological}\label{ontological}}

Another old idea was to merge automated planning and other artificial
intelligence fields with knowledge representation and more specifically
ontologies. Indeed, since the ``semantic web'' is already widespread for
service description, why not make planning compatible with it to ease
service composition ?

\hypertarget{webpddl}{%
\subsubsection{WebPDDL}\label{webpddl}}

This question finds it first answer in 2002 with WebPDDL. This language,
explicited in \cref{lst:webpddl}, is meant to be compatible with RDF by
using URI identifiers for domains (McDermott and Dou
\protect\hyperlink{ref-mcdermott_representing_2002}{2002}). The syntax
is inspired by PDDL, but axioms are added as constraints on the
knowledge domain. Actions also have a return value and can have
variables that aren't dependant on their parameters. This allows for
greater expressivity than regular PDDL, but can be partially emulated
using PDDL 3.1 constraints and object fluents.

\begin{codelisting}

\caption{Example of WebPDDL syntax by Mc Dermott.}

\hypertarget{lst:webpddl}{%
\label{lst:webpddl}}%
\begin{verbatim}
(define (domain www-agents)
  (:extends (uri "http://www.yale.edu/domains/knowing")
            (uri "http://www.yale.edu/domains/regression-planning")
            (uri "http://www.yale.edu/domains/commerce"))
  (:requirements :existential-preconditions :conditional-effects)
  (:types Message - Obj Message-id - String)
  (:functions  (price-quote ?m - Money)
               (query-in-stock ?pid - Product-id)
               (reply-in-stock ?b - Boolean) - Message)
  (:predicates (web-agent ?x - Agent)
               (reply-pending a - Agent id - Message-id msg - Message)
               (message-exchange ?interlocutor - Agent
                                 ?sent ?received - Message
                                 ?eff - Prop)
               (expected-reply a - Agent sent expect-back - Message))
  (:axiom
      :vars (?agt - Agent ?msg-id - Message-id ?sent ?reply - Message)
      :implies (normal-step-value (receive ?agt ?msg-id) ?reply)
      :context (and (web-agent ?agt)
                    (reply-pending ?agt ?msg-id ?sent)
                    (expected-reply ?agt ?sent ?reply)))
  (:action send
      :parameters (?agt - Agent ?sent - Message)
      :value (?sid - Message-id)
      :precondition (web-agent ?agt)
      :effect (reply-pending ?agt ?sid ?sent))
  (:action receive
    :parameters (?agt - Agent ?sid - Message-id)
    :vars (?sent - Message ?eff - Prop)
    :precondition (and (web-agent ?agt) (reply-pending ?agt ?sid ?sent))
    :value (?received - Message)
    :effect (when (message-exchange ?agt ?sent ?received ?eff) ?eff)))
\end{verbatim}

\end{codelisting}

\hypertarget{opt}{%
\subsubsection{OPT}\label{opt}}

This previous work was updated by McDermott
(\protect\hyperlink{ref-mcdermott_opt_2005}{2005}). The new version is
called OPT and allows for some further expressivity. It can express
hierarchical domains with links between actions and even advanced data
structure. The syntax is mostly an update of WebPDDL. In \cref{lst:opt},
we can see that the URI were replaced by simpler names, the action
notation was simplified to make the parameter and return value more
natural. Axioms were replaced by facts with a different notation.

\begin{codelisting}

\caption{Example of the updated OPT syntax as described by Mc Dermott.}

\hypertarget{lst:opt}{%
\label{lst:opt}}%
\begin{verbatim}
(define (domain www-agents)
  (:extends knowing regression-planning commerce)
  (:requirements :existential-preconditions :conditional-effects)
  (:types Message - Obj Message-id - String )
  (:type-fun (Key t) (Feature-type (keytype t)))
  (:type-fun (Key-pair t) (Tup (Key t) t))
  (:functions (price-quote ?m - Money)
              (query-in-stock ?pid - Product-id)
              (reply-in-stock ?b - Boolean) - Message)
  (:predicates (web-agent ?x - Agent)
               (reply-pending a - Agent id - Message-id msg - Message)
               (message-exchange ?interlocutor - Agent
                                 ?sent ?received - Message
                                 ?eff - Prop)
               (expected-reply a - Agent sent expect-back - Message))
  (:facts
    (freevars (?agt - Agent ?msg-id - Message-id
               ?sent ?reply - Message)
      (<- (and (web-agent ?agt)$\iffalse>\fi$
               (reply-pending ?agt ?msg-id ?sent)
               (expected-reply ?agt ?sent ?reply))
               (normal-value (receive ?agt ?msg-id) ?reply))))
  (:action (send ?agt - Agent ?sent - Message) - (?sid - Message-id)
    :precondition (web-agent ?agt)
    :effect (reply-pending ?agt ?sid ?sent))
  (:action (receive ?agt - Agent ?sid - Message-id) - (?received - Message)
    :vars (?sent - Message ?eff - Prop)
    :precondition (and (web-agent ?agt)
                       (reply-pending ?agt ?sid ?sent))
    :effect (when (message-exchange ?agt ?sent ?received ?eff) ?eff)))
\end{verbatim}

\end{codelisting}

\hypertarget{color-and-general-planning-representation}{%
\section{Color and general planning
representation}\label{color-and-general-planning-representation}}

From the general formalism of planning proposed earlier, it is possible
to create an instanciation of the SELF language for expressing planning
domains. This extension was the primary goal of creating SELF and uses
almost all features of the language.

\hypertarget{framework}{%
\subsection{Framework}\label{framework}}

In order to describe this planning framework into SELF, we simply put
all fields of the actions into properties. Entities are used as fluents,
and the entire knwoledge domain as constraints. We use parameterized
types as specified \textbf{BEFORE}.

\begin{codelisting}

\caption{Content of the file "planning.w"}

\hypertarget{lst:planning}{%
\label{lst:planning}}%
\begin{Shaded}
\begin{Highlighting}[]
\StringTok{"lang.w"}\NormalTok{ = ? ; }\CommentTok{//include default language file.}
\NormalTok{Fluent = }\BuiltInTok{Entity}\NormalTok{;}
\BuiltInTok{State}\NormalTok{ = (}\BuiltInTok{Group}\NormalTok{(Fluent), }\BuiltInTok{Statement}\NormalTok{);}
\NormalTok{BooleanOperator = (&,|);}
\NormalTok{(pre,eff, constr)::}\FunctionTok{Property}\NormalTok{(}\BuiltInTok{Action}\NormalTok{,}\BuiltInTok{State}\NormalTok{);$\textbackslash{}label\{line:preeff\}$}
\NormalTok{(costs,lasts,probability) ::}\FunctionTok{Property}\NormalTok{(}\BuiltInTok{Action}\NormalTok{,}\BuiltInTok{Float}\NormalTok{);$\textbackslash{}label\{line:attributes\}$}
\NormalTok{Plan = }\BuiltInTok{Group}\NormalTok{(}\BuiltInTok{Statement}\NormalTok{);$\textbackslash{}label\{line:plan\}$}
\NormalTok{-> ::}\FunctionTok{Property}\NormalTok{(}\BuiltInTok{Action}\NormalTok{,}\BuiltInTok{Action}\NormalTok{); }\CommentTok{//Causal links$\textbackslash{}label\{line:causallinks\}$}
\NormalTok{methods ::}\FunctionTok{Property}\NormalTok{(}\BuiltInTok{Action}\NormalTok{,Plan);}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

The file presented in \cref{lst:planning}, gives the definition of the
syntax of fluents and actions in SELF. The first line includes the
default syntax file using the first statement syntax. The fluents are
simply typed as entities. This allows them to be either parameterized
entities or statements. States are either a set of fluent or a logical
statement between states or fluents. When a state is represented as a
set, it represent the conjonction of all fluents in the set.

Then at \cref{line:preeff}, we define the preconditions, effects and
constraints formalism. They are represented as simple properties between
actions and states. This allows for simple expression of commonly
expressed formalism like the ones found in PDDL. \cref{line:attributes}
expresses the other attributes of actions like cost, duration and prior
probability of success.

Plans are needed to be represented in the files, especially for case
based and hierarchical paradims. They are expressed using statements for
causal link representation. The property \texttt{-\textgreater{}} is
used in these statements and the causes are either given explicitely as
parameters of the property or they can be infered by the planner. We add
a last property to express methods relative to their actions.

\hypertarget{example-domain}{%
\subsection{Example domain}\label{example-domain}}

Using the classical example domain used earlier we can write the
following file in \cref{lst:block_world}.

\begin{codelisting}

\caption{Blockworld writen in SELF to work with Color}

\hypertarget{lst:block_world}{%
\label{lst:block_world}}%
\begin{Shaded}
\begin{Highlighting}[]
\StringTok{"planning.w"}\NormalTok{ = ? ; }\CommentTok{//include base terminology $\textbackslash{}label\{line:include\}$}

\NormalTok{(! on !, }\FunctionTok{held}\NormalTok{(!), }\FunctionTok{down}\NormalTok{(_)) :: Fluent;$\textbackslash{}label\{line:arity\}$}

\FunctionTok{pickUp}\NormalTok{(x) }\FunctionTok{pre}\NormalTok{ (~ on x, }\FunctionTok{down}\NormalTok{(x), }\FunctionTok{held}\NormalTok{(~));$\textbackslash{}label\{line:pickup\}$}
\FunctionTok{pickUp}\NormalTok{(x) }\FunctionTok{eff}\NormalTok{ (~(}\FunctionTok{down}\NormalTok{(x)), }\FunctionTok{held}\NormalTok{(~));}

\FunctionTok{putDown}\NormalTok{(x) }\FunctionTok{pre}\NormalTok{ (}\FunctionTok{held}\NormalTok{(x));}
\FunctionTok{putDown}\NormalTok{(x) }\FunctionTok{eff}\NormalTok{ (}\FunctionTok{held}\NormalTok{(~), }\FunctionTok{down}\NormalTok{(x));}

\FunctionTok{stack}\NormalTok{(x, y) }\FunctionTok{pre}\NormalTok{ (}\FunctionTok{held}\NormalTok{(x), ~ on y);}
\FunctionTok{stack}\NormalTok{(x, y) }\FunctionTok{eff}\NormalTok{ (}\FunctionTok{held}\NormalTok{(~), x on y);}

\FunctionTok{unstack}\NormalTok{(x, y) }\FunctionTok{pre}\NormalTok{ (}\FunctionTok{held}\NormalTok{(~), x on y);}
\FunctionTok{unstack}\NormalTok{(x, y) }\FunctionTok{eff}\NormalTok{ (}\FunctionTok{held}\NormalTok{(x), ~ on y);}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

At line \cref{line:include}, We need to include the file defined in
\cref{lst:planning}. After that \cref{line:arity} defines the allowed
arrity of each relation/function used by fluents. This restricts
eventually the cardinality between parameters (one to many, many to one,
etc).

\cref{line:pickup} encodes the action \(pickup\) defined earlier. It is
interesting to note that instead of using a constant to denote the
absence of block, we can use an anonymous exclusive quantifier to make
sure no block is held. This is quite useful to make concice domains that
stays expressive and intuitive.

\hypertarget{differences-with-pddl}{%
\subsection{Differences with PDDL}\label{differences-with-pddl}}

SELF+Color is more consice than PDDL. It will infere most types and
declaration. Variables are also infered if they are used more than once
in a statement and also part of parameters.

While PDDL uses a fixed set of extensions to specify the capabilities of
the domain, SELF uses inclusion of other files to allow for greater
flexibility. In PDDL, everything must be declared while in SELF, type
inference allows for usage without definition. It is interesting to note
that the use of variables names \texttt{x} and \texttt{y} are arbitrary
and can be changed for each statement and the domain will still be
functionally the same. The line 3 in \cref{lst:block_pddl} is a specific
feature of SELF that is absent in PDDL. It is possible to specify
constraints on the cardinality of properties. This limits the number of
different combination of values that can be true at once. This is
typically done in PDDL using several predicate or constraints.

Most of the differences can be sumarized saying that `SELF do it once,
PDDL needs it twice'. This doesn't only mean that SELF is more compact
but also that the expressivity allows for a drastic reduction of the
search space if taken into account. Thiébaux \emph{et al.}
(\protect\hyperlink{ref-thiebaux_defense_2005}{2005}) advocate for the
recognition of the fact that expressivity isn't just a convinience but
is crucial for some problem and that treating it like an obstacle by
trying to compile it away only makes the problem worse. If a planner is
agnostic to the domain and problem, it cannot take advantages of clues
that the instanciation of an action or even its name can hold (Babli
\emph{et al.} \protect\hyperlink{ref-babli_use_2015}{2015}).

Whatever the time and work that an expert spend on a planning domain it
will always be incomplete and fixed. SELF allows for dynamical extension
and even adresses the use of reified actions as parameters. Such a
framework can be useful in multi-agent systems where agents can
commnicate composite actions to instruct another agent. It can also be
useful for macro-action learning that allows to learn hierarchical
domains from repeating observations. It can also be used in online
planning to repair a plan that failed. And at last this mechanism can be
used for explanation or inference by making easy to map two similar
domains together. (lots of \textbf{CITATION}).

Also another difference between SELF and PDDL is the underlying planning
framework. We presented the one of SELF but PDDL seems to suppose a more
classical state based formalism. For example the fluents are of two kind
depending if they are used as precondition or effects. In the first
case, the fluent is a formula that is evaluated like a predicate to know
if the action can be executed in any given state. Effects are formula
enforcing the values of existing fluent in the state. SELF just suppose
that the new knowledge is enforcing and that the fluents are of the same
kind since verification about the coherence of the actions are made
prior to its application in planning.

\hypertarget{online-and-flexible-planning-algorithms}{%
\chapter{Online and Flexible Planning
Algorithms}\label{online-and-flexible-planning-algorithms}}

Since automated planning comes in a variety of paradigms, so does the
planners algorithms. It amounts to such a number of planners that
listing all those that exists would be a terribly long endeavour.

In that chapter, we present planners and approaches to inverted planning
and intent recognition. To do that we must first have a performant
online planning algorithm that can take into account observed plans or
fluents and find the most likely plan being pursued by an external
agent.

Classical planning can be used for such a work but lacks flexibility
when needing to replan at high frequency. The planner must be either
able to reuse previously found plans or be able to give quickly plans
that are good approximation of the intended goal. We could use
probabilistic planning, especially Partially Observable Markovian
Decision Process (POMDP) to directly encode the intent recognition
problem but that approach have been explored in great detail already,
including numerous bayesian network approaches. We propose to create
planners fit for this use by deriving from POCL and HTN planning.

\hypertarget{existing-algorithms}{%
\section{Existing Algorithms}\label{existing-algorithms}}

In order to make such a planner, few paradigms are interesting and quite
useful. The first one is PSP, that allows to refine plans into
solutions.

\hypertarget{plan-space-planning-1}{%
\subsection{Plan Space Planning}\label{plan-space-planning-1}}

Every PSP algorithms works in a similar way: their search space is the
set of all plans and its iteration operation is plan refinement. This
means that every PSP planner searches for \emph{flaws} in the current
plan and then computes a set of \emph{resolvers} that potentially fix
it. The algorithm starts with an empty plan only having the initial and
goal step and recursively refine the plan until all flaws have been
solved. The PSP approach has two main advantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It is very flexible since it allows for custom definition for flaws in
  the plan and ways to fix them and
\item
  It allows delayed comitment and makes it possible to cut the search
  tree early on.
\end{enumerate}

It however has also several inconvinients:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Since it manipulates plans, the data structure operations are much
  more complex and therefore requires more overhead.
\item
  Backtracking is required in some cases and ammount to a drastically
  decreased efficiency.
\end{enumerate}

\hypertarget{existing-psp-planners}{%
\subsubsection{Existing PSP planners}\label{existing-psp-planners}}

Related works already tried to explore new ideas to make PSP an
attractive alternative to regular state-based planners like the
appropriately named ``Reviving partial order planning'' (Nguyen and
Kambhampati \protect\hyperlink{ref-nguyen_reviving_2001}{2001}) and
VHPOP (Younes and Simmons
\protect\hyperlink{ref-younes_vhpop_2003}{2003}). More recent efforts
(Coles \emph{et al.} \protect\hyperlink{ref-coles_popf2_2011}{2011};
Sapena \emph{et al.}
\protect\hyperlink{ref-sapena_combining_2014}{2014}) are trying to adapt
the powerful heuristics from state-based planning to PSP's approach. An
interesting approach of these last efforts is found in (Shekhar and
Khemani \protect\hyperlink{ref-shekhar_learning_2016}{2016}) with
meta-heuristics based on offline training on the domain. Yet, we clearly
note that only a few papers lay the emphasis upon plan quality using PSP
(Ambite and Knoblock
\protect\hyperlink{ref-ambite_planning_1997}{1997}).

\hypertarget{definitions}{%
\subsubsection{Definitions}\label{definitions}}

In \cref{sec:general_psp} we have formalized how PSP works in the
general planning formalism. However, since this formalism is very new,
it isn't used by the rest of the comunity. This means that we still need
to define the classical PSP algotrithm. In order to define this
algorithm we need to explain the notions of flaws and resolvers.

\begin{definition}[Flaws]\label[definition]{def:flaws}

Flaws are constraints violations within a plan. The set of flaws in a
plan \(\bb{\pi}\) is noted \(\otimes_{\bb{\pi}}\). There are different
kind of flaws in classical PSP and additional ones can be defined
depending on the application.

Classical flaws often have a few common features. They are often
\textbf{positive} since they \emph{add} causal links and step in a plan
to refine it. They have a \emph{proper fluent} \(f\) that is the cause
of the violation in the plan the flaw is representing and a
\emph{needer} \(a_n\) that is the action requiring the proper fluent be
fulfilled. In classical PSP flaws are either:

\begin{itemize}
\tightlist
\item
  \textbf{Subgoals}, also called \emph{open condition} that are yet to
  be supported by a \emph{provider} \(a_p\). We note subgoals
  \(f \otimes^\downarrowbarred_{\bb{\pi}} a_n\).
\item
  \textbf{Threats} are caused by steps that can break a causal link with
  their effects. They are called \emph{breakers} of the threatened link.
  A step \(a_b\) threatens a causal link
  \(l_t = a_p \xrightarrow{f} a_n\) if and only if
  \((f : a_b = \emptyset) \land a_b \not\succ a_p \land a_n \not\succ a_b\).
  Said otherwise, the breaker can cancel an effect of a providing step
  \(a_p\), before it gets used by its needer \(a_n\). We note threats
  \(f \otimes^\dagger_{\bb{\pi}} a_n\).
\end{itemize}

\end{definition}

\begin{figure}
\hypertarget{fig:flaws}{%
\centering
\includegraphics{graphics/todo.svg}
\caption{Example of partial plan having flaws}\label{fig:flaws}
}
\end{figure}

\emph{Example}: In the block world example we used previously, we can
get the first flaws being the open conditions of the goal step. Once
fixed, along with open, conditions of the added steps, we have a threat
\textbf{TODO} as shown in \cref{fig:flaws}.

These flaws need to be fixed in order for the plan to be valid. In POCL
it is done by finding their resolvers.

\begin{definition}[Resolvers]\label[definition]{def:resolvers}

A resolver is a plan refinement that attempts to solve a flaw
\(\otimes_{\bb{\pi}}\). Since classical flaws are positive, so are the
classical resolvers. They are defined as follow:

\begin{itemize}
\tightlist
\item
  \emph{For subgoals}, the resolvers are a set of potential causal links
  containing the proper fluent \(f\) of a given subgoal in their causes
  while taking the needer step \(a_n\) as their target and a
  \textbf{provider} step \(a_p\) as their source. They are noted
  \(\odot^+(f \otimes^\downarrowbarred_{\bb{\pi}} a_n) = a_p \xrightarrow{f} a_n\).
\item
  \emph{For threats}, we usually consider only two resolvers:
  \textbf{demotion} (\(a_b \succ a_p\)) and \textbf{promotion}
  (\(a_n \succ a_b\)) of the breaker relative to the threatened link. We
  call the added causeless causal link a \textbf{guarding} link.
\end{itemize}

It is possible to introduce extra resolvers to fix custom flaws. In such
a case we call positive resolvers, those which adds causal links and
steps to the plan and negative those that removes causal links and
steps. It is preferable to engineer flaws and resolver to not mix
positive and negative aspect at once because of the complicated side
effects that might result from it.

\end{definition}

\begin{figure}
\hypertarget{fig:resolvers}{%
\centering
\includegraphics{graphics/todo.svg}
\caption{Example of resolvers that fixes the previously illustrated
flaws}\label{fig:resolvers}
}
\end{figure}

\emph{Example}: From our previous example, \textbf{TODO}
\cref{fig:resolvers}.

The application of a resolver does not necessarily mean progress. It can
have consequences that may require reverting its application in order to
respect the backtracking of the POCL algorithm.

\begin{definition}[Side effects]\label[definition]{def:side-effects}

Flaws that are caused by the application of a resolver are called
\emph{related flaws}. They are inserted into the \emph{agenda}\footnote{An
  agenda is a flaw container used for the flaw selection of POCL.} with
each application of a resolver:

\begin{itemize}
\tightlist
\item
  \emph{Related subgoals} are all the new open conditions inserted by
  new steps.
\item
  \emph{Related threats} are the causal links threatened by the
  insertion of a new step or the deletion of a guarding link.
\end{itemize}

Flaws can also become irrelevant when a resolver is applied. It is
always the case for the targeted flaw, but this can also affect other
flaws. Those \emph{invalidated flaws} are removed from the agenda upon
detection:

\begin{itemize}
\tightlist
\item
  \emph{Invalidated subgoals} are subgoals satisfied by the new causal
  links or the removal of their needer.
\item
  \emph{Invalidated threats} happen when the breaker no longer threatens
  the causal link because the order guards the threatened causal link or
  either of them has been removed.
\end{itemize}

\end{definition}

\begin{figure}
\hypertarget{fig:sideeffects}{%
\centering
\includegraphics{graphics/todo.svg}
\caption{Example of the side effects of the application of a
resolver}\label{fig:sideeffects}
}
\end{figure}

\emph{Example}: \textbf{TODO}

In \cref{alg:pocl} we present a generic version of POCL inspired by
Ghallab \emph{et al.}
(\protect\hyperlink{ref-ghallab_automated_2004}{2004}, sec. 5.4.2).

\textbf{FIXME} Symbol update

\begin{algorithm}\caption{Partial Order Planner}\label[algorithm]{alg:pocl}\begin{algorithmic}[1]\Function{POCL}{Agenda $a$, Problem $\mathcal{P}$}
    \If{$a = \emptyset$} \Comment{Populated agenda needs to be provided}
        \State \Return Success \Comment{Stops all recursion}
    \EndIf
    \State Flaw $f \gets$ \Call{choose}{$a$} \label{line:flawselection}
    \Comment{Heuristically chosen flaw}
    \State Resolvers $R \gets$ \Call{solve}{$f$, $\mathcal{P}$} \label{line:resolverselection}
    \ForAll{$r \in R$} \Comment{Non-deterministic choice operator}
        \State \Call{apply}{$r$, $\pi$} \label{line:resolverapplication} 
        \Comment{Apply resolver to partial plan}
        \State Agenda $a' \gets$ \Call{update}{$a$} \label{line:updateagenda}
        \If{\protect\Call{POCL}{$a'$, $\mathcal{P}$} = Success} \Comment{Refining recursively}
            \State \Return Success
        \EndIf
        \State \Call{revert}{$r$, $\pi$} \Comment{Failure, undo resolver application} \label{line:revert}
    \EndFor
    \State $a \gets a \cup \{f\}$ \Comment{Flaw was not resolved}
    \State \Return Failure \Comment{Revert to last non-deterministic choice}
\EndFunction\end{algorithmic}\end{algorithm}

For our version of POCL we follow a refinement procedure that works in
several generic steps. In \cref{fig:refinement} we detail the resolution
of a subgoal as done in the \cref{alg:pocl}.

The first is the search for resolvers. It is often done in two separate
steps: first, select the candidates and then check each of them for
validity. This is done using the polymorphic function \texttt{solve} at
\cref{line:resolverselection}.

In the case of subgoals, variable unification is performed to ensure the
compatibility of the resolvers. Since this step is time-consuming, the
operator is instantiated accordingly at this step to factories the
computational effort. Composite operators have also all their methods
instantiated at this step if they are selected as a candidate.

Then a resolver is picked non-deterministically for applications (this
can be heuristically driven). At \cref{line:resolverapplication} the
resolver is effectively applied to the current plan. All side effects
and invalidations are handled during the update of the agenda at
\cref{line:updateagenda}. If a problem occurs, \cref{line:revert}
backtracks and tries other resolvers. If no resolver fits the flaw, the
algorithm backtracks to previous resolver choices to explore all the
possible plans and ensure completeness.

\begin{figure}
\hypertarget{fig:refinement}{%
\centering
\includegraphics{graphics/todo.svg}
\caption{Example of the refinement process for subgoal
resolution}\label{fig:refinement}
}
\end{figure}

\hypertarget{plan-repair}{%
\subsection{Plan repair}\label{plan-repair}}

In order to make an efficient online planner, we must be able to
capitalize on previously computed plans. This leads to the idea of plan
repair or case based planning for online planning.

Classical PSP algorithms don't take as an input an existing plan but can
be enhanced to fit plan repairing, as for instance in (Van Der Krogt and
De Weerdt \protect\hyperlink{ref-vanderkrogt_plan_2005}{2005}). Usually,
PSP algorithms take a problem as an input and use a loop or a recursive
function to refine the plan into a solution. We can't solely use the
refining recursive function to be able to use our existing partial plan.
This causes multiples side effects if the input plan is suboptimal. This
problem was already explored as of LGP-adapt ({\textbf{???}}) that
explains how re-using a partial plan often implies replanning parts of
the plan.

\hypertarget{htn}{%
\subsection{HTN}\label{htn}}

HTN is often combined with classical approaches since it allows for a
more natural expression of domains making expert knowledge easier to
encode. These kinds of planners are named \textbf{decompositional
planners} when no initial plan is provided (Fox
\protect\hyperlink{ref-fox_natural_1997}{1997}). Most of the time the
integration of HTN simply consists in calling another algorithm when
introducing a composite operator during the planning process. The DUET
planner by Gerevini \emph{et al.}
(\protect\hyperlink{ref-gerevini_combining_2008}{2008}) does so by
calling an instance of a HTN planner based on task insertion called
SHOP2 (Nau \emph{et al.} \protect\hyperlink{ref-nau_shop2_2003}{2003})
to decompose composite actions. Some planners take the integration
further by making the decomposition of composite actions into a special
step in their refinement process. Such works include the discourse
generation oriented DPOCL (Young and Moore
\protect\hyperlink{ref-young_dpocl_1994}{1994}) and the work of
Kambhampati \emph{et al.}
(\protect\hyperlink{ref-kambhampati_hybrid_1998}{1998}) generalizing the
practice for decompositional planners.

In our case, we chose a class of hierarchical planners based on
Plan-Space Planning (PSP) algorithms (Bechon \emph{et al.}
\protect\hyperlink{ref-bechon_hipop_2014}{2014}; Dvorak \emph{et al.}
\protect\hyperlink{ref-dvorak_flexible_2014}{2014}; Bercher \emph{et
al.} \protect\hyperlink{ref-bercher_hybrid_2014}{2014}) as a reference
approach. The main difference here is that the decomposition is
integrated into the classical POCL algorithm by only adding new types of
flaws. This allows keeping all the flexibility and properties of POCL
while adding the expressivity and abstraction capabilities of HTN.

\hypertarget{lollipop}{%
\section{Lollipop}\label{lollipop}}

\hypertarget{operator-graph}{%
\subsection{Operator Graph}\label{operator-graph}}

One of the main contributions of the present paper is our use of the
concept of \emph{operator graph}. First of all, we define this notion.

\begin{definition}[Operator Graph]

An operator graph \(O^\Pi\) of a set of operators \(O\) is a labeled
directed graph that binds two operators with the causal link
\(o_1 \xrightarrow{f} o_2\) iff there exists at least one unifying
fluent \(f \in eff(o_1) \cap pre(o_2)\).

\end{definition}

This definition was inspired by the notion of domain causal graph as
explained in (Göbelbecker \emph{et al.}
\protect\hyperlink{ref-gobelbecker_coming_2010}{2010}) and originally
used as a heuristic in (Helmert \emph{et al.}
\protect\hyperlink{ref-helmert_fast_2011}{2011}). Causal graphs have
fluents as their nodes and operators as their edges. Operator graphs are
the opposite: an \emph{operator dependency graph} for a set of actions.
A similar structure was used in ({\textbf{???}}) that builds the
operator dependency graph of goals and uses precondition nodes instead
of labels. Cycles in this graph denote the dependencies of operators. We
call \emph{co-dependent} operators that form a cycle. If the cycle is
made of only one operator (self-loop), then it is called
\emph{auto-dependent}.

While building this operator graph, we need a \textbf{providing map}
that indicates, for each fluent, the list of operators that can provide
it. This is a simpler version of the causal graphs that is reduced to an
associative table easier to update. The list of providers can be sorted
to drive resolver selection (as detailed in \cref{sec:selection}). A
\textbf{needing map} is also built but is only used for operator graph
generation. We note \(\mathcal{D}^\Pi\) the operator graph built with
the set of operators in the domain \(\mathcal{D}\). In the
\cref{fig:operatorgraph}, we illustrate the application of this
mechanism on our example from \cref{fig:example}. Continuous lines
correspond to the \emph{domain operator graph} computed during domain
compilation time.

\begin{figure}
\hypertarget{fig:operatorgraph}{%
\centering
\includegraphics{graphics/todo.svg}
\caption{Diagram of the operator graph of example domain. Full arrows
represent the domain operator graph and dotted arrows the dependencies
added to inject the initial and goal steps.}\label{fig:operatorgraph}
}
\end{figure}

The generation of the operator graph is detailed in
\cref{alg:operatorgraph}. It explores the operators space and builds a
providing and a needing map that gives the provided and needed fluents
for each operator. Once done it iterates on every precondition and
searches for a satisfying cause to add the causal links to the operator
graph.

\begin{algorithm}\caption{Operator graph generation and update algorithm}\label[algorithm]{alg:operatorgraph}\begin{algorithmic}\footnotesize
\Function{addVertex}{Operator $o$}
    \State \Call{cache}{$o$} \Comment{Update of the providing and needing map}
    \If {binding} \Comment{boolean that indicates if the binding was requested}
        \State \Call{bind}{$o$}
    \EndIf
\EndFunction
\Function{cache}{Operator $o$}
    \ForAll{$eff \in eff(o)$} \Comment{Adds $o$ to the list of providers of $eff$}
        \State \Call{add}{$providing, eff, o$}
    \EndFor
    \State ... \Comment{Same operation with needing and preconditions}
\EndFunction
\Function{bind}{Operator $o$}
    \ForAll{$pre \in pre(o)$}
        \If{$pre \in providing$}
            \ForAll{$\pi \in$ \Call{get}{$providing$, $pre$}}
                \State Link $l \gets$ \Call{getEdge}{$\pi$, $o$} \Comment{Create the link if needed}
                \State $l \gets l \cup \{pre\}$ \Comment{Add the fluent as a cause}
            \EndFor
        \EndIf
    \EndFor
    \State ... \Comment{Same operation with needing and effects}
\EndFunction\end{algorithmic}\end{algorithm}

To apply the notion of operator graphs to planning problems, we just
need to add the initial and goal steps to the operator graph. In
\cref{fig:operatorgraph}, we depict this insertion with our previous
example using dotted lines. However, since operator graphs may have
cycles, they can't be used directly as input to POP algorithms to ease
the initial backchaining. Moreover, the process of refining an operator
graph into a usable one could be more computationally expensive than POP
itself.

In order to give a head start to the LOLLIPOP algorithm, we propose to
build operator graphs differently with the algorithm detailed in
\cref{alg:safeoperatorgraph}. A similar notion was already presented as
``basic plans'' in ({\textbf{???}}). These ``basic'' partial plans use a
more complete but slower solution for the generation that ensures that
each selected steps are \emph{necessary} for the solution. In our case,
we built a simpler solution that can solve some basic planning problems
but that also make early assumptions (since our algorithm can handle
them). It does a simple and fast backward construction of a partial plan
driven by the providing map. Therefore, it can be tweaked with the
powerful heuristics of state search planning.

\begin{algorithm}\caption{Safe operator graph generation algorithm}\label[algorithm]{alg:safeoperatorgraph}\begin{algorithmic}\footnotesize
\Function{safe}{Problem $\mathcal{P}$}
    \State Stack<Operator> $open \gets [G]$
    \State Stack<Operator> $closed \gets \emptyset$
    \While{$open \neq \emptyset$}
        \State Operator $o \gets$ \Call{pop}{$open$} \Comment{Remove $o$ from $open$}
        \State \Call{push}{$closed$, $o$}
        \ForAll {$pre \in pre(o)$}
            \State Operators $p \gets$ \Call{getProviding}{$\pi$, $pre$} \Comment{Sorted by usefulness}
            \If{$p = \emptyset$} \Comment{(see section~\ref{sec:selection})}
                \State $S \gets S \setminus \{\pi\}$
                \Continue
            \EndIf
            \State Operator $o' \gets$ \Call{getFirst}{$\pi$} \label{line:safefirst}
            \If{$o' \in closed$}
                \Continue
            \EndIf
            \If{$o' \not \in S$}
                \State \Call{push}{$open$, $o'$}
            \EndIf
            \State $S \gets S \cup \{o'\}$
            \State Link $l \gets$ \Call{getEdge}{$o'$, $o$} \Comment{Create the link if needed}
            \State $l \gets l \cup \{pre\}$ \Comment{Add the fluent as a cause}
        \EndFor
    \EndWhile
\EndFunction\end{algorithmic}\end{algorithm}

This algorithm is useful since it is specifically used on goals. The
result is a valid partial plan that can be used as input to POP
algorithms.

\hypertarget{negative-refinements}{%
\subsection{Negative Refinements}\label{negative-refinements}}

The classical POP algorithm works upon a principle of positive plan
refinements. The two standard flaws (subgoals and threats) are fixed by
\emph{adding} steps, causal links, or variable binding constraints to
the partial plan. Online planning needs to be able to \emph{remove}
parts of the plan that are not necessary for the solution. Since we
assume that the input partial plan is quite complete, we need to define
new flaws to optimize and fix this plan. These flaws are called
\emph{negative} as their resolvers apply subtractive refinements on
partial plans.

\begin{definition}[Alternative]\label[definition]{def:alternative}

An alternative is a negative flaw that occurs when there is a better
provider choice for a given link. An alternative to a causal link
\(a_p \xrightarrow{f} a_n\) is a provider \(a_b\) that has a better
\emph{utility value} than \(a_p\).

\end{definition}

The \textbf{utility value} of an operator is a measure of usefulness at
the heart of our ranking mechanism detailed in \cref{sec:selection}. It
uses the incoming and outgoing degrees of the operator in the domain
operator graph to measure its usefulness.

Finding an alternative to an operator is computationally expensive. It
requires searching a better provider for every fluent needed by a step.
To simplify that search, we select only the best provider for a given
fluent and check if the one used is the same. If not, we add the
alternative as a flaw. This search is done only on updated steps for
online planning. Indeed, the safe operator graph mechanism is guaranteed
to only choose the best provider (\cref{alg:safeoperatorgraph} at
\cref{line:safefirst}). Furthermore, subgoals won't introduce new
fixable alternative as they are guaranteed to select the best possible
provider.

\begin{definition}[Orphan]

An orphan is a negative flaw that occurs when a step in the partial plan
(other than the initial or goal step) is not participating in the plan.
Formally, \(a_o\) is an orphan iff
\(a_o \neq I \land a_o \neq G \land \left( d_\pi^+(a_o) = 0 \right) \lor \forall l \in L_\pi^+(a_o), l=\emptyset\).

\end{definition}

With \(d_\pi^+(a_o)\) being the \emph{outgoing degree} of \(a_o\) in the
directed graph formed by \(\pi\) and \(L_\pi^+(a_o)\) being the set of
\emph{outgoing causal links} of \(a_o\) in \(\pi\). This last condition
checks for \emph{hanging orphans} that are linked to the goal with only
bare causal links (introduced by threat resolution).

The introduction of negative flaws requires modifying the resolver
definition (cf.~\cref{def:resolver}). ::: \{.definition name=``Signed
Resolvers''\} A signed resolver is a resolver with a notion of sign. We
add to the resolver tuple the sign of the resolver noted
\(s \in \{+, -\}\). :::

The solution to an alternative is a negative refinement that simply
removes the targeted causal link. This causes a new subgoal as a side
effect, that will focus on its resolver by its rank (explained in
\cref{sec:selection}) and then pick the first provider (the most useful
one). The resolver for orphans is the negative refinement that is meant
to remove a step and its incoming causal link while tagging its
providers as potential orphans.

\begin{figure}
\hypertarget{fig:sideeffects}{%
\centering
\includegraphics{graphics/todo.svg}
\caption{Schema representing flaws with their signs, resolvers and side
effects relative to each other}\label{fig:sideeffects}
}
\end{figure}

The side effect mechanism also needs an upgrade since the new kind of
flaws can interfere with one another. This is why we extend the side
effect definition (cf.~\cref{def:sideeffect}) with a notion of sign.

\begin{definition}[Signed Side Effects]

A signed side effect is either a regular \emph{causal side effect} or an
\emph{invalidating side effect}. The sign of a side effect indicates if
the related flaw needs to be added or removed from the agenda.

\end{definition}

The \cref{fig:sideeffects} exposes the extended notion of signed
resolvers and side effects. When treating positive resolvers, nothing
needs to change from the classical method. When dealing with negative
resolvers, we need to search for extra subgoals and threats. Deletion of
causal links and steps can cause orphan flaws that need to be identified
for removal.

In the method described in (Peot and Smith
\protect\hyperlink{ref-peot_threatremoval_1993}{1993}), a
\textbf{invalidating side effect} is explained under the name of
\emph{DEnd} strategy. In classical POP, it has been noticed that threats
can disappear in some cases if subgoals or other threats were applied
before them. For our mechanisms, we decide to gather under this notion
every side effect that removes the need to consider a flaw. For example,
orphans can be invalidated if a subgoal selects the considered step.
Alternatives can remove the need to compute further subgoal of an orphan
step as orphans simply remove the need to fix any flaws that concern the
selected step.

These interactions between flaws are decisive for the validity and
efficiency of the whole model, that is why we aim to drive flaw
selection in a rigorous manner.

\hypertarget{usefullness-heuristic}{%
\subsection{Usefullness Heuristic}\label{usefullness-heuristic}}

Resolvers and flaws selection are the keys to improving performances.
Choosing a good resolver helps to reduce the branching factor that
accounts for most of the time spent on running POP algorithms
(Kambhampati \protect\hyperlink{ref-kambhampati_design_1994}{1994} ).
Flaw selection is also important for efficiency, especially when
considering negative flaws which can conflict with other flaws.

Conflicts between flaws occur when two flaws of opposite sign target the
same element of the partial plan. This can happen, for example, if an
orphan flaw needs to remove a step needed by a subgoal or when a threat
resolver tries to add a promoting link against an alternative. The use
of side effects will prevent most of these occurrences in the agenda but
a base ordering will increase the general efficiency of the algorithm.

Based on the \cref{fig:sideeffects}, we define a base ordering of flaws
by type. This order takes into account the number of flaw types affected
by causal side effects.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Alternatives} will cut causal links that have a better
  provider. It is necessary to identify them early since they will add
  at least another subgoal to be fixed as a related flaw.
\item
  \textbf{Subgoals} are the flaws that cause most of the branching
  factor in POP algorithms. This is why we need to make sure that all
  open conditions are fixed before proceeding on finer refinements.
\item
  \textbf{Orphans} remove unneeded branches of the plan. Yet, these
  branches can be found out to be necessary for the plan to meet a
  subgoal. Since a branch can contain many actions, it is preferable to
  leave the orphan in the plan until they are no longer needed. Also,
  threats involving orphans are invalidated if the orphan is resolved
  first.
\item
  \textbf{Threats} occur quite often in the computation. Searching and
  solving them is computationally expensive since they need to check if
  there are no paths that fix the flaw already. Many threats are
  generated without the need of resolver application (Peot and Smith
  \protect\hyperlink{ref-peot_threatremoval_1993}{1993}). That is why we
  rank all related subgoals and orphans before threats because they can
  add causal links or remove threatening actions that will fix the
  threat.
\end{enumerate}

Resolvers need to be ordered as well, especially for the subgoal flaws.
Ordering resolvers for a subgoal is the same operation as choosing a
provider. Therefore, the problem becomes ``how to rank operators?''. The
most relevant information on an operator is its usefulness and
hurtfulness. These show how much an operator will help and how much it
may cause branching after selection.

\begin{definition}[Degree of an operator]

Degrees are a measurement of the usefulness of an operator. Such a
notion is derived from the incoming and outgoing degrees of a node in a
graph.

We note \(d_\pi^+(o)\) and \(d_\pi^-(o)\) respectively the outgoing and
incoming degrees of an operator in a plan \(\pi\). These represent the
number of causal links that goes out or toward the operator. We call
proper degree of an operator \(d^+(o) = |eff(o)|\) and
\(d^-(o) = |pre(o)|\) the number of preconditions and effects that
reflect its intrinsic usefulness.

\end{definition}

There are several ways to use the degrees as indicators. \emph{Utility
value} increases with every \(d^+\), since this reflects a positive
participation in the plan. It decreases with every \(d^-\) since actions
with higher incoming degrees are harder to satisfy. The utility value
bounds are useful when selecting special operators. For example, a
user-specified constraint could be laid upon an operator to ensure it is
only selected as a last resort. This operator will be affected with the
smallest utility value possible. More commonly, the highest value is
used for initial and goal steps to ensure their selection.

Our ranking mechanism is based on scores noted \(s^\pm(o)\). A score
contains two components: a positive subscore array that acts as a
participation measurement and a negative subscore array that represents
the dependencies of the operator. Each component of the score is an
array of \emph{subscores}.

The first step is the computation of the \textbf{base scores}. They are
computed according to the following:

\begin{itemize}
\tightlist
\item
  \(s^+(o)\) contains only \(d_{\mathcal{D}^\Pi}^+(o)\), the positive
  degree of \(o\) in the domain operator graph. This will give a
  measurement of the predicted usefulness of the operator.
\item
  \(s^-(o)\) containing the following subscores:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \(d^-(o)\) the proper negative degree of \(o\). Having more
    preconditions can lead to a potentially higher need for subgoals.
  \item
    \(\sum_{c \in C_o(\mathcal{D}^\Pi)}|c|\) with
    \(C_o(\mathcal{D}^\Pi)\) being the set of cycles where \(o\)
    participates in the domain operator graph. If an action is
    co‑dependent (cf.~\cref{sec:operatorgraph}) it may lead to a
    dead-end as its addition will cause the formation of a cycle.
  \item
    \(|C_o^s(\mathcal{D}^\Pi)|\) is the number of self-cycle (\(0\) or
    \(1\)) \(o\) participates in. This is usually symptomatic of a
    \emph{toxic operator} (cf.~\cref{def:toxic}). Having an operator
    behaving this way can lead to backtracking because of operator
    instantiation.
  \item
    \(\left|pre(o) \setminus L_{\mathcal{D}^\Pi}^-(o)\right|\) with
    \(L_{\mathcal{D}^\Pi}^-(o)\) being the set of incoming causal links
    of \(o\) in the domain operator graph. This represents the number of
    open conditions. This is symptomatic of action that can't be
    satisfied without a compliant initial step.
  \end{enumerate}
\end{itemize}

Once these subscores are computed, the ranking mechanism starts the
second phase, which computes the \textbf{realization scores}. These
scores are potential bonuses given once the problem is known. It first
searches the \emph{inapplicable operators} that are all operators in the
domain operator graph that have a precondition that isn't satisfied with
a causal link. Then it searches the \emph{eager operators} that provide
fluents with no corresponding causal link (as they are not needed).
These operators are stored in relation with their inapplicable or eager
fluents.

The third phase starts with the beginning of the solving algorithm, once
the problem has been provided. It computes the \textbf{effective
realization scores} based on the initial and goal steps. It will
increment \(s_1^+(o)\) for each realized eager links (if the goal
contains the related fluent) and decrement \(s_4^-(o)\) for each
inapplicable preconditions realized by the initial step.

Last, the \textbf{final score} of each operator \(o\), noted \(h(o)\),
is computed from positive and negative scores using the following
formula:

\[h(o) = \sum_{n=1}^{|s^\pm(o)|}{\pm p_n^\pm s^\pm_n(o)}\]

A parameterized coefficient is associated to each subscore. It is noted
\(p_n^\pm\) with \(n\) being the index of the subscore in the array
\(s^\pm\). This respects the criteria of having a bound for the
\emph{utility value} as it ensures that it remains positive with \(0\)
as a minimum bound and \(+\infty\) for a maximum. The initial and goal
steps have their utility values set to the upper bound to ensure their
selections over other steps.

Choosing to compute the resolver selection at operator level has some
positive consequences on the performances. Indeed, this computation is
much lighter than approaches with heuristics on plan space (Shekhar and
Khemani \protect\hyperlink{ref-shekhar_learning_2016}{2016}) as it
reduces the overhead caused by real time computation of heuristics on
complex data. In order to reduce this overhead more, the algorithm sorts
the providing associative array to easily retrieve the best operator for
each fluent. This means that the evaluation of the heuristic is done
only once for each operator. This reduces the overhead and allows for
faster results on smaller plans.

\hypertarget{algorithm}{%
\subsection{Algorithm}\label{algorithm}}

The LOLLIPOP algorithm uses the same refinement algorithm as described
in \cref{alg:pop}. The differences reside in the changes made on the
behavior of resolvers and side effects. In
\cref{line:resolverapplication} of \cref{alg:pop}, LOLLIPOP algorithm
applies negative resolvers if the selected flaw is negative. In
\cref{line:sideeffectapplication}, it searches for both signs of side
effects. Another change resides in the initialization of the solving
mechanism and the domain as detailed in \cref{alg:lollipopinit}. This
algorithm contains several parts. First, the {domainInit} function
corresponds to the code computed during the domain compilation time. It
will prepare the rankings, the operator graph, and its caching
mechanisms. It will also use strongly connected component detection
algorithm to detect cycles. These cycles are used during the base score
computation (\cref{line:basescore}). We add a detection of illegal
fluents and operators in our domain initialization
(\cref{line:isillegal}). Illegal operators are either inconsistent or
toxic.

\begin{algorithm}\caption{LOLLIPOP initialization mechanisms}\label[algorithm]{alg:lollipopinit}\begin{algorithmic}\footnotesize
\Function{domainInit}{Operators $O$}
    \State operatorgraph $\mathcal{D}^\Pi$
    \State Score $S$
    \ForAll{Operator $o \in O$}
        \If{\Call{isIllegal}{$o$}} \Comment{Remove toxic and useless fluents} \label{line:isillegal}
            \State $O \gets O \setminus \{o\}$  \Comment{If entirely toxic or useless}
            \Continue
        \EndIf
        \State \Call{addVertex}{$o, \mathcal{D}^\Pi$} \Comment{Add and bind all operators}
        \State \Call{cache}{$p, o$} \Comment{Cache operator in providing map}
    \EndFor
    \State Cycles $C \gets$ \Call{stronglyConnectedComponent}{$\mathcal{D}^\Pi$} \Comment{Using DFS}
    \State $S \gets$ \Call{baseScores}{$O$, $\mathcal{D}^\Pi$} \label{line:basescore}
    \State $i \gets$ \Call{inapplicables}{$\mathcal{D}^\Pi$}
    \State $e \gets$ \Call{eagers}{$\mathcal{D}^\Pi$}
\EndFunction
\Function{lollipopInit}{Problem $\mathcal{P}$}
    \State \Call{realize}{$S, \mathcal{P}$} \Comment{Realize the scores}
    \State \Call{cache}{$providing, I$} \Comment{Cache initial step in providing ...}
    \State \Call{cache}{$providing, G$} \Comment{... as well as goal step}
    \State \Call{sort}{$providing, S$} \Comment{Sort the providing map}
    \If{$L = \emptyset$}
        \State $\mathcal{P}^\Pi \gets$ \Call{safe}{$\mathcal{P}$} \Comment{Computing the safe operator graph if the plan is empty}
    \EndIf
    \State \Call{populate}{$a$, $\mathcal{P}$} \Comment{populate agenda with first flaws} \label{line:populate}
\EndFunction
\Function{populate}{Agenda $a$, Problem $\mathcal{P}$}
    \ForAll{Update $u \in U$} \Comment{Updates due to online planning}
        \State Fluents $F \gets eff(u.new) \setminus eff(u.old)$ \Comment{Added effects}
        \ForAll{Fluent $f \in F$}
            \ForAll{Operator $o \in$ \Call{better}{$providing$, $f$, $o$}}
                \ForAll{Link $l \in L^+(o)$}
                    \If{$f \in l$}
                        \State \Call{addAlternative}{$a$, $f$, $o$, $l_{\leftarrow}$, $\mathcal{P}$} \Comment{With $l_{\leftarrow}$ the target of $l$}
                    \EndIf
                \EndFor
            \EndFor
        \EndFor
        \State $F \gets eff(u.old) \setminus eff(u.new)$ \Comment{Removed effects}
        \ForAll{Fluent $f \in F$}
            \ForAll{Link $l \in L^+(u.new)$}
                \If{\Call{isLiar}{$l$}}
                    \State $L \gets L \setminus \{l\}$
                    \State \Call{addOrphans}{$a$, $u$, $\mathcal{P}$}
                \EndIf
            \EndFor
        \EndFor
        \State ... \Comment{Same with removed preconditions and incomming liar links}
    \EndFor
    \ForAll{Operator $o \in S$}
        \State \Call{addSubgoals}{$a$, $o$, $\mathcal{P}$}
        \State \Call{addThreats}{$a$, $o$, $\mathcal{P}$}
    \EndFor
\EndFunction\end{algorithmic}\end{algorithm}

\begin{definition}[Inconsistent operators]

An operator \(a\) is contradictory iff
\(\exists f \{f, \lnot f \} \in eff(o) \lor \{f, \lnot f \} \in pre(o)\)

\end{definition}

\begin{definition}[Toxic operators]\label[definition]{def:toxic}

Toxic operators have effects that are already in their preconditions or
empty effects. An operator \(o\) is toxic iff
\(pre(o) \cap eff(o) \neq \emptyset \lor eff(o) = \emptyset\)

\end{definition}

Toxic actions can damage a plan as well as make the execution of POP
algorithm longer than necessary. This is fixed by removing the toxic
fluents (\(pre(a) \nsubseteq eff(a)\)) and by updating the effects with
\(eff(a) = eff(a) \setminus pre(a)\). If the effects become empty, the
operator is removed from the domain.

The {lollipopInit} function is executed during the initialization of the
solving algorithm. We start by realizing the scores, then we add the
initial and goal steps in the providing map by caching them. Once the
ranking mechanism is ready, we sort the providing map. With the ordered
providing map, the algorithm runs the fast generation of the safe
operator graph for the problem's goal.

The last part of this initialization (\cref{line:populate}) is the
agenda population that is detailed in the {populate} function. During
this step, we perform a search of alternatives based on the list of
updated fluents. Online updates can make the plan outdated relative to
the domain. This forms liar links :

\begin{definition}[Liar links]

A liar link is a link that doesn't hold a fluent in the preconditions or
effect of its source and target. We note:
\[a_i \xrightarrow{f} a_j | f \notin eff(a_i) \cap pre(a_j)\]

\end{definition}

A liar link can be created by the removal of an effect or preconditions
during online updates (with the causal link still remaining).

We call lies the fluents that are held by links without being in the
connected operators. To resolve the problem, we remove all lies. We
delete the link altogether if it doesn't bear any fluent as a result of
this operation. This removal triggers the addition of orphan flaws as
side effects.

While the list of updated operators is crucial for solving online
planning problems, a complementary mechanism is used to ensure that
LOLLIPOP is complete. User provided plans have their steps tagged. If
the failure has backtracked to a user-provided step, then it is removed
and replaced by subgoals that represent each of its participation in the
plan. This mechanism loops until every user provided steps have been
removed.

\hypertarget{theoretical-and-empirical-results}{%
\subsection{Theoretical and Empirical
Results}\label{theoretical-and-empirical-results}}

As proven in (Penberthy \emph{et al.}
\protect\hyperlink{ref-penberthy_ucpop_1992}{1992}), the classical POP
algorithm is \emph{sound} and \emph{complete}.

First, we define some new properties of partial plans. The following
properties are taken from the original proof. We present them again for
convenience.

::: \{\#def:fullsupport .definition name=``Full Support'' \} A partial
plan \(\pi\) is fully supported if each of its steps \(o \in S\) is
fully supported. A step is fully supported if each of its preconditions
\(f \in pre(o)\) is supported. A precondition is fully supported if
there exists a causal link \(l\) that provides it. We note:
\[\Downarrow \pi \equiv
\begin{array}{l}
    \forall o \in S \thickspace \forall f \in pre(o) \thickspace \exists l \in L_\pi^-(o): \\
        \left(f \in l \land \not \exists t \in S (l_{\rightarrow} \succ t \succ o \land \lnot f \in eff(t))\right)
\end{array}\] with \(L_\pi^-(o)\) being the incoming causal links of
\(o\) in \(\pi\) and \(l_{\rightarrow}\) being the source of the link.

\begin{definition}[Partial Plan Validity]\label[definition]{def:partialplanvalidity}

A partial plan is a \textbf{valid solution} of a problem \(\mathcal{P}\)
iff it is \emph{fully supported} and \emph{contains no cycles}. The
validity of \(\pi\) regarding a problem \(\mathcal{P}\) is noted
\(\pi \models \left( \mathcal{P} \equiv \Downarrow \pi \land \left(C(\pi) = \emptyset \right) \right)\)
with \(C(\pi)\) being the set of cycles in \(\pi\).

\end{definition}

\hypertarget{proof-of-soundness}{%
\subsubsection{Proof of Soundness}\label{proof-of-soundness}}

In order to prove that this property applies to LOLLIPOP, we need to
introduce some hypothesis:

\begin{itemize}
\tightlist
\item
  operators updated by online planning are known.
\item
  user provided steps are known.
\item
  user provided plans don't contain illegal artifacts. This includes
  toxic or inconsistent actions, lying links and cycles.
\end{itemize}

Based on the \cref{def:partialplanvalidity} we state that:
\begin{equation}
\left(
\begin{array}{l}
    \forall pre \in pre(G): \\
    \Downarrow pre \land
    \begin{array}{l}
        \forall o \in L_\pi^-(G)_{\rightarrow} \thickspace \forall pre' \in pre(o): \\
        \left(\Downarrow pre'\land C_o(\pi) = \emptyset\right)
    \end{array}
\end{array}
\right) \implies \pi \models \mathcal{P}\label{eq:recursivevalidity}\end{equation}
where \(L_\pi^-(G)_{\rightarrow}\) is the set of direct antecedents of
\(G\) and \(C_o(\pi)\) is the set of fluents containing \(o\) in
\(\pi\).

This means that \(\pi\) is a solution if all preconditions of \(G\) are
satisfied. We can satisfy these preconditions using operators iff their
preconditions are all satisfied and if there is no other operator that
threatens their supporting links.

First, we need to prove that \cref{eq:recursivevalidity} holds on
LOLLIPOP initialization. We use our hypothesis to rule out the case when
the input plan is invalid. The \cref{alg:safeoperatorgraph} will only
solve open conditions in the same way subgoals do it. Thus, safe
operator graphs are valid input plans.

Since the soundness is proven for regular refinements and flaw
selection, we need to consider the effects of the added mechanisms of
LOLLIPOP. The newly introduced refinements are negative, they don't add
new links:

\begin{equation}\forall f \in \mathcal{F}(\pi) \thickspace \forall r \in r(f): C_\pi(f.n) = C_{f(\pi)}(f.n)\label{eq:nocycle}\end{equation}
with \(\mathcal{F}(\pi)\) being the set of flaws in \(\pi\), \(r(f)\)
being the set of resolvers of \(f\), \(f.n\) being the needer of the
flaw and \(f(\pi)\) being the resulting partial plan after application
of the flaw. Said otherwise, an iteration of LOLLIPOP won't add cycles
inside a partial plan.

The orphan flaw targets steps that have no path to the goal and so can't
add new open conditions or threats. The alternative targets existing
causal links. Removing a causal link in a plan breaks the full support
of the target step. This is why an alternative will always insert a
subgoal in the agenda corresponding to the target of the removed causal
link. Invalidating side effects also don't affect the soundness of the
algorithm since the removed flaws are already solved. This makes:
\begin{equation}
\forall f \in \mathcal{F}^-(\pi): \Downarrow \pi \implies \Downarrow f(\pi)
\label{eq:conssupport}\end{equation} with \(\mathcal{F}^-(\pi)\) being
the set of negative flaws in the plan \(\pi\). This means that negative
flaws don't compromise the full support of the plan.

\Cref{eq:nocycle,eq:conssupport} lead to \cref{eq:recursivevalidity}
being valid after the execution of LOLLIPOP. The algorithm is sound.

\hypertarget{proof-of-completeness}{%
\subsubsection{Proof of Completeness}\label{proof-of-completeness}}

The soundness proof shows that LOLLIPOP's refinements don't affect the
support of plans in term of validity. It was proven that POP is
complete. There are several cases to explore to transpose the property
to LOLLIPOP:

\begin{lemma}[Conservation of Validity]

If the input plan is a valid solution, LOLLIPOP returns a valid
solution.

\end{lemma}

\begin{proof}

With \cref{eq:nocycle,eq:conssupport} and the proof of soundness, the
conservation of validity is already proven. \qedhere

\end{proof}

\begin{lemma}[Reaching Validity with incomplete partial plans]\label[lemma]{lem:incompletevalidity}

If the input plan is incomplete, LOLLIPOP returns a valid solution if it
exists.

\end{lemma}

\begin{proof}

Since POP is complete and the \cref{eq:conssupport} proves the
conservation of support by LOLLIPOP, then the algorithm will return a
valid solution if the provided plan is an incomplete plan and the
problem is solvable. \qedhere

\end{proof}

\begin{lemma}[Reaching Validity with empty partial plans]

If the input plan is empty and the problem is solvable, LOLLIPOP returns
a valid solution.

\end{lemma}

\begin{proof}

This is proven using ({\textbf{???}}) and POP's completeness. However,
we want to add a trivial case to the proof: \(pre(G) = \emptyset\). In
this case the \cref{line:emptygoal} of the \cref{alg:pop} will return a
valid plan.

\end{proof}

\begin{lemma}[Reaching Validity with a dead-end partial plan]\label[lemma]{lem:deadend-validity}

If the input plan is in a dead-end, LOLLIPOP returns a valid solution.

\end{lemma}

\begin{proof}

Using input plans that can be in an undetermined state is not covered by
the original proof. The problem lies in the existing steps in the input
plan. Still, using our hypothesis we add a failure mechanism that makes
LOLLIPOP complete. On failure, the needer of the last flaw is deleted if
it wasn't added by LOLLIPOP. User defined steps are deleted until the
input plan acts like an empty plan. Each deletion will cause
corresponding subgoals to be added to the agenda. In this case, the
backtracking is preserved and all possibilities are explored as in POP.
\qedhere

\end{proof}

Since all cases are covered, this proves the property of completeness.

\hypertarget{sec:results}{%
\subsubsection{Experimental Results}\label{sec:results}}

The experimental results focused on the properties of LOLLIPOP for
online planning. Since classical POP is unable to perform online
planning, we tested our algorithm considering the time taken for solving
the problem for the first time. We profiled the algorithm on a benchmark
problem containing each of the possible issues described earlier.

\begin{figure}
\hypertarget{fig:experiment}{%
\centering
\includegraphics{graphics/todo.svg}
\caption{Domain used to compute the results. First line is the initial
and goal step along with the useful actions. Second line contains a
threatening action \(t\), two co-dependent actions \(n\) and \(l\), a
useless action \(u\), a toxic action \(v\), a deadend action \(w\) and
an inconsistent action \(x\)}\label{fig:experiment}
}
\end{figure}

In \cref{fig:experiment}, we expose the planning domain used for the
experiments. During the domain initialization, the actions \(u\) and
\(v\) are eliminated from the domain since they serve no purpose in the
solving process. The action \(x\) is stripped of its negative effect
because it is inconsistent with the effect \(2\).

As the solving starts, LOLLIPOP computes a safe operator graph (full
lines in \cref{fig:experimentplan}). As we can see this partial plan is
nearly complete already. When the main refining function starts it
receives an agenda with only a few flaws remaining.

\begin{figure}
\hypertarget{fig:experimentplan}{%
\centering
\includegraphics{graphics/todo.svg}
\caption{In full lines the initial safe operator graph. In thin, sparse
and irregularly dotted lines a subgoal, alternative and threat caused
causal link.}\label{fig:experimentplan}
}
\end{figure}

Then the main refinement function starts (time markers \textbf{1}).
LOLLIPOP selects as resolver a causal link from \(a\) to satisfy the
open condition of the goal step. Once the first threat between \(a\) and
\(t\) is resolved the second threat is invalidated. On a second
execution, the domain changes for online planning with \(6\) added to
the initial step. This solving (time markers \textbf{2}) add as flaw an
alternative on the link from \(c\) to the goal step. A subgoal is added
that links the initial and goal step for this fluent. An orphan flaw is
also added that removes \(c\) from the plan. Another solving takes place
as the goal step doesn't need \(3\) as a precondition (time markers
\textbf{3}). This causes the link from \(a\) to be cut since it became a
liar link. This adds \(a\) as an orphan that gets removed from the plan
even if it was hanging by the bare link to \(t\).

The measurements exposed in \cref{tbl:results} were made with an Intel®
Core™ i7-4720HQ with a 2.60GHz clock. Only one core was used for the
solving. The same experiment done only with the chronometer code gave a
result of \(70 ns\) of error. We can see an increase of performance in
the online runs because of the way they are conducted by LOLLIPOP.

\hypertarget{tbl:results}{}
\begin{table}\footnotesize
\centering

\caption{\label{tbl:results}Average times of \(1.000\) executions on the
problem. The first column is for a simple run on the problem. Second and
third columns are times to replan with one and two changes done to the
domain for online planning.}

\begin{tabular}{@{}llll@{}}
\toprule

\textbf{Experiment} & \emph{Single} & \emph{Online 1} & \emph{Online
2} \\\midrule

\textbf{Time (\(ms\))} & \(0.86937\) & \(0.38754\) & \(0.48123\) \\

\bottomrule
\end{tabular}

\end{table}

\hypertarget{heart}{%
\section{HEART}\label{heart}}

\hypertarget{domain-compilation}{%
\subsection{Domain Compilation}\label{domain-compilation}}

In order to simplify the input of the domain, the causes of the causal
links in the methods are optional. If omitted, the causes are inferred
by unifying the preconditions and effects with the same mechanism as in
the subgoal resolution in our POCL algorithm. Since we want to guarantee
the validity of abstract plans, we need to ensure that user provided
plans are solvable. We use the following formula to compute the final
preconditions and effects of any composite action \(a\):
\(\mathit{pre}(a) = \bigcup_{l \in L^+(I_m)}^{m \in methods(a)} \mathit{causes}(l)\)
and
\(\mathit{eff}(a) = \bigcup_{l \in L^-(G_m))}^{m \in methods(a)} \mathit{causes}(l)\).
An instance of the classical POCL algorithm is then run on the problem
\(\mathcal{P}_a = \langle \mathcal{D}, C_{\mathcal{P}} , a\rangle\) to
ensure its coherence. The domain compilation fails if POCL cannot be
completed. Since our decomposition hierarchy is acyclic
(\(a \notin A_a\), see \cref{def:proper}) nested methods cannot contain
their parent action as a step.

\hypertarget{abstraction-in-pop}{%
\subsection{Abstraction in POP}\label{abstraction-in-pop}}

In order to properly introduce the changes made for using HTN domains in
POCL, we need to define a few notions.

Transposition is needed to define decomposition.

\begin{definition}[Transposition]

In order to transpose the causal links of an action \(a'\) with the ones
of an existing step \(a\) in a plan \(\pi\), we use the following
operation:

\[a \rhd^-_\pi a' = \left \lbrace \phi^-(l) \xrightarrow{\mathit{causes}(l)} a': l \in L^-_\pi(a) \right \rbrace\]

It is the same with \(a' \xrightarrow{\mathit{causes}(l)} \phi^+(l)\)
and \(L^+\) for \(a \rhd^+ a'\). This supposes that the respective
preconditions and effects of \(a\) and \(a'\) are equivalent. When not
signed, the transposition is generalized:
\(a \rhd a' = a\rhd^-a' \cup a\rhd^+a'\).

\end{definition}

\emph{Example:} \(a\rhd^-a'\) gives all incoming links of \(a\) with the
\(a\) replaced by \(a'\).

\begin{definition}[Proper Actions]\label[definition]{def:proper}

Proper actions are actions that are ``contained'' within an entity
(either a domain, plan or action). We note this notion
\(A_a = A_a^{lv(a)}\) for an action \(a\). It can be applied to various
concepts:

\begin{itemize}
\tightlist
\item
  For a \emph{domain} or a \emph{problem},
  \(A_{\mathcal{P}} = A_{\mathcal{D}}\).
\item
  For a \emph{plan}, it is \(A^0_\pi = S_\pi\).
\item
  For an \emph{action}, it is
  \(A^0_a = \bigcup_{m \in \mathit{methods}(a)} S_m\). Recursively:
  \(A_a^n = \bigcup_{b\in A_a^0} A_{b}^{n-1}\). For atomic actions,
  \(A_a = \emptyset\).
\end{itemize}

\end{definition}

\emph{Example:} The proper actions of \(make(drink)\) are the actions
contained within its methods. The set of extended proper actions adds
all proper actions of its single proper composite action
\(infuse(drink, water, cup)\).

\begin{definition}[Abstraction Level]\label[definition]{def:level}

This is a measure of the maximum amount of abstraction an entity can
express, defined recursively by:\footnote{We use Iverson brackets here,
  see notations in \cref{tbl:symbols}.}
\[lv(x) = \left ( \max_{a \in A_x}(lv(a)) + 1 \right ) [A_x \neq \emptyset]\]

\end{definition}

\emph{Example:} The abstraction level of any atomic action is \(0\)
while it is \(2\) for the composite action \(make(drink)\). The example
domain (in \cref{lst:domain}) has an abstraction level of \(3\).

The most straightforward way to handle abstraction in regular planners
is illustrated by Duet (Gerevini \emph{et al.}
\protect\hyperlink{ref-gerevini_combining_2008}{2008}) by managing
hierarchical actions separately from a task insertion planner. We chose
to add abstraction in POCL in a manner inspired by the work of Bechon
\emph{et al.} (\protect\hyperlink{ref-bechon_hipop_2014}{2014}) on a
planner called HiPOP. The difference between the original HiPOP and our
implementation of it is that we focus on the expressivity and the ways
flaw selection can be exploited for partial resolution. Our version is
lifted at runtime while the original is grounded for optimizations. All
mechanisms we have implemented use POCL but with different management of
flaws and resolvers. The original \cref{alg:pocl} is left untouched.

One of those changes is that resolver selection needs to be altered for
subgoals. Indeed, as stated by the authors of HiPOP: the planner must
ensure the selection of high-level operators in order to benefit from
the hierarchical aspect of the domain. Otherwise, adding operators only
increases the branching factor. Composite actions are not usually meant
to stay in a finished plan and must be decomposed into atomic steps from
one of their methods.

\begin{definition}[Decomposition Flaws]\label[definition]{def:decomposition}

They occur when a partial plan contains a non-atomic step. This step is
the needer \(a_n\) of the flaw. We note its decomposition
\(a_n \oplus\).

\begin{itemize}
\tightlist
\item
  \emph{Resolvers:} A decomposition flaw is solved with a
  \textbf{decomposition resolver}. The resolver will replace the needer
  with one of its instantiated methods \(m \in \mathit{methods}(a_n)\)
  in the plan \(\pi\). This is done by using transposition such that:
  \(a_n \oplus^m_\pi = \langle S_m \cup (S_\pi \setminus \lbrace a \rbrace) , a_n \rhd^- I_m \cup a_n \rhd^+ G_m \cup (L_\pi \setminus L_\pi(a_n))\).
\item
  \emph{Side effects:} A decomposition flaw can be created by the
  insertion of a composite action in the plan by any resolver and
  invalidated by its removal:
  \[\bigcup^{f \in \mathit{pre}(a_m)}_{a_m \in S_m} \pi' \downarrowbarred_f a_m\bigcup^{l \in L_{\pi'}}_{a_b \in S_{\pi'}} a_b \olcross l \bigcup_{a_c \in S_m}^{lv(a_c) \neq 0} a_c \oplus\]
\end{itemize}

\end{definition}

\emph{Example:} When adding the step \(make(tea)\) in the plan to solve
the subgoal that needs tea being made, we also introduce a decomposition
flaw that will need this composite step replaced by its method using a
decomposition resolver. In order to decompose a composite action into a
plan, all existing links are transposed to the initial and goal step of
the selected method, while the composite action and its links are
removed from the plan. The main differences between HiPOP and HEART in
our implementations are the functions of flaw selection and the handling
of the results (one plan for HiPOP and a plan per cycle for HEART). In
HiPOP, the flaw selection is made by prioritizing the decomposition
flaws. Bechon \emph{et al.}
(\protect\hyperlink{ref-bechon_hipop_2014}{2014}) state that it makes
the full resolution faster. However, it also loses opportunities to
obtain abstract plans in the process.

\hypertarget{planning-in-cycle}{%
\subsection{Planning in cycle}\label{planning-in-cycle}}

\begin{figure}
\hypertarget{fig:cycles}{%
\centering
\includegraphics{graphics/todo.svg}
\caption{Illustration of how the cyclical approach is applied on the
example domain. Atomic actions that are copied from a cycle to the next
are omitted.}\label{fig:cycles}
}
\end{figure}

The main focus of our work is toward obtaining \textbf{abstract plans}
which are plans that are completed while still containing composite
actions. In order to do that the flaw selection function will enforce
cycles in the planning process.

\begin{definition}[Cycle]

A cycle is a planning phase defined as a triplet
\(c = \langle lv(c), agenda, \pi_{lv(c)}\rangle\) where: \(lv(c)\) is
the maximum abstraction level allowed for flaw selection in the
\(agenda\) of remaining flaws in partial plan \(\pi_{lv(c)}\). The
resolvers of subgoals are therefore constrained by the following:
\(a_p \downarrow_f a_n: lv(a_p) \leq lv(c)\).

\end{definition}

During a cycle all decomposition flaws are delayed. Once no more flaws
other than decomposition flaws are present in the agenda, the current
plan is saved and all remaining decomposition flaws are solved at once
before the abstraction level is lowered for the next cycle:
\(lv(c') = lv(c)-1\). Each cycle produces a more detailed abstract plan
than the one before.

Abstract plans allow the planner to do an approximate form of anytime
execution. At any given time the planner is able to return a fully
supported plan. Before the first cycle, the plan returned is
\(\pi_{lv(a_0)}\).

\emph{Example:} In our case using the method of intent recognition of
Sohrabi \emph{et al.} Sohrabi \emph{et al.}
(\protect\hyperlink{ref-sohrabi_plan_2016}{2016}), we can already use
\(\pi_{lv(a_0)}\) to find a likely goal explaining an observation (a set
of temporally ordered fluents). That can make an early assessment of the
probability of each goal of the recognition problem.

For each cycle \(c\), a new plan \(\pi_{lv(c)}\) is created as a new
method of the root operator \(a_0\). These intermediary plans are not
solutions of the problem, nor do they mean that the problem is solvable.
In order to find a solution, the HEART planner needs to reach the final
cycle \(c_0\) with an abstraction level \(lv(c_0) = 0\). However, these
plans can be used to derive meaning from the potential solution of the
current problem and give a good approximation of the final result before
its completion.

\emph{Example:} In the \cref{fig:cycles}, we illustrate the way our
problem instance is progressively solved. Before the first cycle
\(c_2\), all we have is the root operator and its plan \(\pi_3\). Then
within the first cycle, we select the composite action \(make(tea)\)
instantiated from the operator \(make(drink)\) along with its methods.
All related flaws are fixed until all that is left in the agenda is the
abstract flaws. We save the partial plan \(\pi_2\) for this cycle and
expand \(make(tea)\) into a copy of the current plan \(\pi_1\) for the
next cycle. The solution of the problem will be stored in \(\pi_0\) once
found.

\hypertarget{properties-of-abstract-planning}{%
\subsection{Properties of Abstract
Planning}\label{properties-of-abstract-planning}}

In this section, we prove several properties of our method and resulting
plans: HEART is complete, sound and its abstract plans can always be
decomposed into a valid solution.

The completeness and soundness of POCL has been proven in (Penberthy
\emph{et al.} \protect\hyperlink{ref-penberthy_ucpop_1992}{1992}). An
interesting property of POCL algorithms is that flaw selection
strategies do not impact these properties. Since the only modification
of the algorithm is the extension of the classical flaws with a
decomposition flaw, all we need to explore, to update the proofs, is the
impact of the new resolver. By definition, the resolvers of
decomposition flaws will take into account all flaws introduced by its
resolution into the refined plan. It can also revert its application
properly.

\begin{lemma}[Decomposing preserves acyclicity]

The decomposition of a composite action with a valid method in an
acyclic plan will result in an acyclic plan. Formely,
\(\forall a_s \in S_\pi: a_s \nsucc_\pi a_s \implies \forall a'_s \in S_{a \oplus^m_\pi}: a'_s \nsucc_{a \oplus^m_\pi} a'_s\).

\end{lemma}

\begin{proof}

When decomposing a composite action \(a\) with a method \(m\) in an
existing plan \(\pi\), we add all steps \(S_m\) in the refined plan.
Both \(\pi\) and \(m\) are guaranteed to be cycle free by definition. We
can note that
\(\forall a_s \in S_m: \left ( \nexists a_t \in S_m: a_s \succ a_t \land \lnot f \in \mathit{eff}(a_t)\right ) \implies f \in \mathit{eff}(a)\).
Said otherwise, if an action \(a_s\) can participate a fluent \(f\) to
the goal step of the method \(m\) then it is necessarily present in the
effects of \(a\). Since higher level actions are preferred during the
resolver selection, no actions in the methods are already used in the
plan when the decomposition happens. This can be noted
\(\exists a \in \pi \implies S_m \cupdot S_\pi\) meaning that in the
graph formed both partial plans \(m\) and \(\pi\) cannot contain the
same edges therefore their acyclicity is preserved when inserting one
into the other.

\end{proof}

\begin{lemma}[Solved decomposition flaws cannot reoccur]

The application of a decomposition resolver on a plan \(\pi\),
guarantees that \(a \notin S_{\pi'}\) for any partial plan refined from
\(\pi\) without reverting the application of the resolver.

\end{lemma}

\begin{proof}

As stated in the definition of the methods
(\cref{def:action}):~\(a \notin A_a\). This means that \(a\) cannot be
introduced in the plan by its decomposition or the decomposition of its
proper actions. Indeed, once \(a\) is expanded, the level of the
following cycle \(c_{lv(a)-1}\) prevents \(a\) to be selected by subgoal
resolvers. It cannot either be contained in the methods of another
action that are selected afterward because otherwise following
\cref{def:level} its level would be at least \(lv(a)+1\).

\end{proof}

\begin{lemma}[Decomposing to abstraction level 0 guarantees solvability]

Finding a partial plan that contains only decomposition flaws with
actions of abstraction level 1, guarantees a solution to the problem.

\end{lemma}

\begin{proof}

Any method \(m\) of a composite action \(a: lv(a) = 1\) is by definition
a solution of the problem
\(\mathcal{P}_a = \langle \mathcal{D}, C_{\mathcal{P}} , a\rangle\). By
definition, \(a \notin A_a\), and \(a \notin A_{a \oplus^m_\pi}\)
(meaning that \(a\) cannot reoccur after being decomposed). It is also
given by definition that the instantiation of the action and its methods
are coherent regarding variable constraints (everything is instantiated
before selection by the resolvers). Since the plan \(\pi\) only has
decomposition flaws and all flaws within \(m\) are guaranteed to be
solvable, and both are guaranteed to be acyclical by the application of
any decomposition \(a \oplus^m_\pi\), the plan is solvable.

\end{proof}

\begin{lemma}[Abstract plans guarantee solvability]

Finding a partial plan \(\pi\) that contains only decomposition flaws,
guarantees a solution to the problem.

\end{lemma}

\begin{proof}

Recursively, if we apply the previous proof on higher level plans we
note that decomposing at level 2 guarantees a solution since the method
of the composite actions are guaranteed to be solvable.

\end{proof}

From these proofs, we can derive the property of soundness (from the
guarantee that the composite action provides its effects from any
methods) and completeness (since if a composite action cannot be used,
the planner defaults to using any action of the domain).

\hypertarget{computational-profile}{%
\subsection{Computational Profile}\label{computational-profile}}

In order to assess its capabilities, HEART was evaluated on two
criteria: quality and complexity. All tests were executed on an Intel®
Core™ i7-7700HQ CPU clocked at 2.80GHz. The Java process used only one
core and was not limited by time or memory (32 GB that wasn't entirely
used up) . Each experiment was repeated between \(700\) and \(10 000\)
times to ensure that variations in speed were not impacting the results.

\begin{figure}
\hypertarget{fig:quality}{%
\centering
\includegraphics{graphics/todo.svg}
\caption{Evolution of the quality with computation
time.}\label{fig:quality}
}
\end{figure}

\Cref{fig:quality} shows how the quality is affected by the abstraction
in partial plans. The tests are made using our example domain (see
\cref{lst:domain}). The quality is measured by counting the number of
providing fluents in the plan
\(\left| \bigcup_{a \in S_\pi} \mathit{eff}(a) \right|\). This metric is
actually used to approximate the probability of a goal given
observations in intent recognition (\(P(G|O)\) with noisy observations,
see (Sohrabi \emph{et al.}
\protect\hyperlink{ref-sohrabi_plan_2016}{2016})). The percentages are
relative to the total number of unique fluents of the complete solution.
These results show that in some cases it may be more interesting to plan
in a leveled fashion to solve HTN problems. For the first cycle of level
\(3\), the quality of the abstract plan is already of \(60\%\). This is
the quality of the exploitation of the plan \emph{before any planning}.
With almost three-quarters of the final quality and less than half of
the complete computation time, the result of the first cycle is a good
quality/time compromise.

\begin{figure}
\hypertarget{fig:width}{%
\centering
\includegraphics{graphics/todo.svg}
\caption{Impact of domain shape on the computation time by levels. The
scale of the vertical axis is logarithmic. Equations are the definition
of the trend curves.}\label{fig:width}
}
\end{figure}

In the second test, we used generated domains. These domains consist of
an action of abstraction level \(5\). This action has a single method
containing a number of actions of level \(4\). We call this number the
width of the domain. All needed actions are built recursively to form a
tree shape. Atomic actions only have single fluent effects. The goal is
the effect of the higher level action and the initial state is empty.
These domains do not contain negative effects. \Cref{fig:width} shows
the computational profile of HEART for various levels and widths. We
note that the behavior of HEART seems to follow an exponential law with
the negative exponent of the trend curves seemingly being correlated to
the actual width. This means that computing the first cycles has a
complexity that is close to being \emph{linear} while computing the last
cycles is of the same complexity as classical planning which is at least
\emph{P-SPACE} (depending on the expressivity of the domain) (Erol
\emph{et al.} \protect\hyperlink{ref-erol_complexity_1995}{1995}).

\hypertarget{perspectives}{%
\chapter{Perspectives}\label{perspectives}}

\hypertarget{fondation}{%
\section{Fondation}\label{fondation}}

\hypertarget{issues-with-current-fondations}{%
\subsection{Issues with current
fondations}\label{issues-with-current-fondations}}

Like stated, these three aspect of knowledge representation makes
defining a fondation of mathematics way more difficult than it may
appear. In this section, we analyse some of the existing fondation of
mathematics and their dependancies.

\hypertarget{set-theory-1}{%
\subsubsection{Set theory}\label{set-theory-1}}

From the book releasing the complete formulation of the ZFC theory, we
can find that:

\begin{quote}
\emph{``the discipline underlying ZF will be the \textbf{first-order
predicate calculus}. The primitive symbols of this set theory, taken
from logic, are the connectives, quantifiers and variables mentioned
above and, possibly, also the symbol of equality (discussed below), as
well as such auxiliary symbols as commas, parentheses and
brackets.''}\footnote{Fraenkel \emph{et al.}
  (\protect\hyperlink{ref-fraenkel_foundations_1973}{1973}, 67,22)}
\end{quote}

This means that ZFC is using FOL as a host language. In turn FOL is
using formal grammar as its host language. And at last, formal grammar
are defined using set theory. This means that there is a dependancy
cycle in the definition of ZFC.

This is significative as this theory is the ground work for most of the
classical mathematics still used to this day. Morever, several
assumptions are made in the theory that are not explicitely stated. For
example, when naming an element or a set we supose the principle of
identity as we can distinguish them from one another. Also, the notion
of set isn't properly defined as well as the classical FOL quantifiers
that are used in even the very first formula of the theory.

\hypertarget{type-theory}{%
\subsubsection{Type theory}\label{type-theory}}

Russell and Whitehead
(\protect\hyperlink{ref-russell_principia_1978}{1978})

\textbf{TODO?} Read the book since typed Lambda calculus is posterior to
type theory

Type theory -\textgreater{} Typed Lambda calculus (no definition of
type) -\textgreater{} Lambda claculus -\textgreater{} set theory or
formal grammar

\hypertarget{category-theory}{%
\subsubsection{Category theory}\label{category-theory}}

Awodey (\protect\hyperlink{ref-awodey_category_2010}{2010})

\textbf{TODO?} Is that what I found ?

Category theory -\textgreater{} Typed Lamda calculus

\hypertarget{proof-theory}{%
\subsubsection{Proof theory}\label{proof-theory}}

\textbf{TODO?}

Proof Theory -\textgreater{} Hilbert calculus -\textgreater{} Mathematic
logic -\textgreater{} (FOL \textbar{} Set \textbar{} etc)

\hypertarget{reverse-mathematics}{%
\subsection{Reverse mathematics}\label{reverse-mathematics}}

Another radical way to approach the fondations of mathematics is called
reverse mathematics. In this field, the goal is to find the minimal set
of axioms needed to prove any theorem. This is not directly meant to be
a fondation of mathematics but helps approaching it by finding groups of
axioms that prove all theorems of a given field of mathematics.

This is the approach that should be used to find the most adequate set
of axioms while keeping in mind the constraints any fondation of
mathematics has.

\hypertarget{possible-paradox-in-proposed-theory}{%
\subsection{Possible paradox in proposed
theory}\label{possible-paradox-in-proposed-theory}}

\textbf{TODO}: Speak about Russel paradox in fonctional form: A fonction
that associates any function that doesn't associate itself. This arrise
from the complement operation.

\hypertarget{knowledge-representation-1}{%
\section{Knowledge representation}\label{knowledge-representation-1}}

Listing the contributions there are a couple that didn't make the cut.
It is mainly ideas or projects that were too long to check or implement
and needed more time to complete. SELF is still a prototype, and even if
the implementation seemed to perform well on a simple example, no
benchmarks have been done on it. It might be good to make a theoretical
analysis of OWL compared to SELF along with some benchmark results.

On the theoretical parts there are some works that seems worthy of
exposure even if unfinished.

\hypertarget{sec:peano}{%
\subsection{Literal definition using Peano's axioms}\label{sec:peano}}

The only real exceptions to the axioms and criteria are the first
statement, the comments and the liberals.

For the first statement, there is yet to find a way to express both
inclusion, the equality relation and solution quantifier. If such a
convenient expression exists, then the language can become almost
entirely self described.

Comments can be seen as a special kind of container. The difficult part
is to find a clever way to differentiate them from regular containers
and to ignore their content in the regular grammar. It might be possible
to at first describe their structure but then they become parseable
entities and fail at their purpose.

Lastly, and perhaps the most complicated violation to fix: laterals. It
is quite possible to define literals by structure. First we can define
boolean logic quite easily in SELF as demonstrated by \cref{lst:bool}.

\begin{codelisting}

\caption{Possible definition of boolean logic in SELF.}

\hypertarget{lst:bool}{%
\label{lst:bool}}%
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{~(}\KeywordTok{false}\NormalTok{) = }\KeywordTok{true}\NormalTok{;$\textbackslash{}label\{line:negation\}$}
\NormalTok{( }\KeywordTok{false}\NormalTok{, }\KeywordTok{true}\NormalTok{ ) :: }\BuiltInTok{Boolean}\NormalTok{;$\textbackslash{}label\{line:}\DataTypeTok{boolean}\NormalTok{\}$}
\KeywordTok{true}\NormalTok{ =?; }\CommentTok{//conflicts with the first statement!$\textbackslash{}label\{line:true\}$}
\NormalTok{*a : ((a | }\KeywordTok{true}\NormalTok{) = }\KeywordTok{true}\NormalTok{);$\textbackslash{}label\{line:logic\}$}
\NormalTok{*a : ((}\KeywordTok{false}\NormalTok{ | a) = a);}
\NormalTok{*a : ((a & }\KeywordTok{false}\NormalTok{) = }\KeywordTok{false}\NormalTok{); }
\NormalTok{*a : ((}\KeywordTok{true}\NormalTok{ & a) = a);}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

Starting with \cref{line:negation}, we simply define the negation using
the exclusive quantifier. From there we define the boolean type as just
the two truth values. And now it gets complicated. We could either
arbitrarily say that the false literal is always parameters of the
exclusion quantifier or that it comes first on either first two
statements but that would just violate minimalism even more. We could
use the solution quantifier to define truth but that collides with the
first statement definition. There doesn't seem to be a good answer for
now.

From \cref{line:logic} going on, we state the definition of the logical
operators \(\land\) and \(\lor\). The problem with this is that we
either need to make a native property for those operators or the
inference to compute boolean logic will be terribly inefficient.

We can use Peano's axioms
(\protect\hyperlink{ref-peano_arithmetices_1889}{1889}) to define
integers in SELF. The attempt at this definition is presented in
\cref{lst:peano}.

\begin{codelisting}

\caption{Possible integration of the Peano axioms in SELF.}

\hypertarget{lst:peano}{%
\label{lst:peano}}%
\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{0}\NormalTok{ :: }\BuiltInTok{Integer}\NormalTok{;}
\NormalTok{*n : (++(n) :: }\BuiltInTok{Integer}\NormalTok{);}
\NormalTok{(*m, *n) : ((m=n) : (++m = ++n));}
\NormalTok{*n : (++n ~= }\DecValTok{0}\NormalTok{);}
\NormalTok{*n : ((n + }\DecValTok{0}\NormalTok{) = n);}
\NormalTok{(*n, *m) : ((n + ++m)= ++(n + m));}
\NormalTok{*n : ((n × }\DecValTok{0}\NormalTok{) = }\DecValTok{0}\NormalTok{);}
\NormalTok{(*n, *m) : ((n × ++m) = (n + (n × m)));}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

We got several problems doing so. The symbols \texttt{*} and \texttt{/}
are already taken in the default file and so would need replacement or
we should use the non-ASCII \texttt{×} and \texttt{÷} symbols for
multiplication and division. Another more fundamental issue is as
previously discussed for booleans: the inference would be excruciatingly
slow or we should revert to a kind of parsing similar to what we have
already under the hood. The last problem is the definition of digits and
bases that would quickly become exceedingly complicated and verbose.

For floating numbers this turns out even worse and complicated and such
a description wasn't even attempted for now.

The last part concerns textual laterals. The issue is the same as the
one with comments but even worse. We get to interpret the content as
literal value and that would necessitate a similar system as we already
have and wouldn't improve the minimalist aspect of things much. Also we
should define ways to escape characters and also to input escape
sequences that are often needed in such case. And since SELF isn't meant
for programming that can become very verbose and complex.

\hypertarget{advanced-inference}{%
\subsection{Advanced Inference}\label{advanced-inference}}

The inference in SELF is very basic. It could be improved a lot more by
simply checking the consistency of the database on most aspects.
However, such a task seems to be very difficult or very slow. Since that
kind of inference is undecidable in SELF, it would be all a research
problem just to find a performant inference algorithm.

Another kind of inference is more about convenience. For example, one
can erase singlets (containers with a single value) to make the database
lighter and easier to maintain and query.

\hypertarget{queries}{%
\subsection{Queries}\label{queries}}

We haven't really discussed quarries in SELF. They can be made using the
main syntax and the solution quantifiers but efficiency of such queries
is unknown. Making an efficient query engine is a big research project
on its own.

For now a very simplified query API exists in the prototype and seems to
perform well but further tests are needed to assess its scalability
capacities.

\hypertarget{general-automated-planning}{%
\section{General Automated Planning}\label{general-automated-planning}}

\hypertarget{planning-improvements}{%
\section{Planning Improvements}\label{planning-improvements}}

\hypertarget{heuristics-using-semantics}{%
\subsection{Heuristics using
Semantics}\label{heuristics-using-semantics}}

\hypertarget{macro-action-learning}{%
\subsection{Macro-Action learning}\label{macro-action-learning}}

\hypertarget{recognition}{%
\section{Recognition}\label{recognition}}

Since the original goal of this thesis was on intent recognition of
dependant persons, we need to explain some more about that specific
domain. The problem is to infer the goals of an external agent through
only observations without intervention. In the end, the idea is to infer
that goal confidently enough to start assisting the other agent without
explicit instructions.

\hypertarget{domain-problems}{%
\subsection{Domain problems}\label{domain-problems}}

This field was widely studied. Indeed, at the end of the last century,
several works started using \emph{abduction} to infer intents from
observational data. Of course this comes as a chalenge since there is a
lot of uncertainty involved. We have no reliable information on the
possible goals of the other agent and we don't have the same knowledge
of the world. Also, observations can sometimes be incomplete or
misleading, the agent can abandon goals or pursue several goals at once
while also doing them suboptimally or even fail at them. To finish,
someimes agents can actively try to hide their intents to the viewer.

\hypertarget{observations-and-inferences}{%
\subsubsection{Observations and
inferences}\label{observations-and-inferences}}

As explained above, we can only get close to reality and any progress in
relevance and detail is exponentially expensive in terms of computing
and memory resources. That is why any system will maintain a high degree
of abstraction that will cause errors inherent in this approximation.

This noise phenomenon can impact the activity and situation recognition
system and therefore seriously impact the intention recognition and
decision making system with an amplification of the error as it is
processed. It is also important to remember that this phenomenon of data
noise is also present in inhibition and that the lack of perception of
an event is as disabling as the perception of the wrong event.

It is possible to protect recognition systems in an appropriate way, but
this often implies a restriction on the levels of possibilities offered
by the system such as specialized recognition or recognition at a lower
level of relevance.

\hypertarget{cognitive-inconsistencies}{%
\subsubsection{Cognitive
inconsistencies}\label{cognitive-inconsistencies}}

In the field of personal assistance, activity recognition is a crucial
element. However, it happens that these events are very difficult to
recognize. The data noise mentioned above can easily be confused with an
omission on the part of the observed agent. This dilemma is also present
during periods of inactivity, the system can start creating events from
scratch to fill what it may perceive as inhibition noise.

These problems are accompanied by others related to the behaviour of the
observed agent. For example, they may perform unnecessary steps, restart
ongoing activities or suddenly abandon them. It is added that other
aspects of observations can make automated inferences such as ambiguous
actions or the agent performing an action that resembles another
complicated.

However, some noise problems can be easily detected by simple cognitive
processes such as impossible sequences (e. g. closing a closed door).
Contextual analyses provide a partial solution to some of these
problems.

\hypertarget{sequentiality}{%
\subsubsection{Sequentiality}\label{sequentiality}}

Since our recognition is based on a highly temporal planning aspect, we
must take into account the classic problems of sequentiality.

A first problem is to determine the end of one plan and the beginning of
another. Indeed, it is possible that some transitions between two planes
may appear to be a plane in itself and therefore may cause false
positives. Another problem is that of intertwined planes. A person can
do two things at once, such as answering the phone while cooking. An
action in an intertwined plan can then be identified as a
discontinuation of activity or a logical inconsistency. A final problem
is that of overloaded actions. Not only can an agent perform two tasks
simultaneously, but also perform an action that contributes to two
activities. These overloaded actions make the process of intention
recognition complex because they are close to data noise.

\hypertarget{existing-approaches}{%
\subsection{Existing approaches}\label{existing-approaches}}

The problem of intention recognition has been strongly addressed from
many angles. It is therefore not surprising that there are many
paradigms in the field. The first studies on the subject highlight the
fact that intention recognition problems are problems of abductive logic
or graph coverage. Since then, many models have competed in imagination
and innovation to improve the field. These include constraint
system-based models that provide a solution based on pre-established
rules and compiled plan libraries, those that use state or action
networks that then launch algorithms on these data, and reverse planning
systems.

\hypertarget{constraint}{%
\subsubsection{Constraint}\label{constraint}}

One of the approaches to intention recognition is the one that builds a
system around a strong logical constraint. There is often a time
constraint system that is complemented by various extensions to cover as
many sequential problems as possible.

In order to solve a problem of intention recognition, abductive logic
can be used. Contrary to deductive logic, the goal is to determine the
objective from the observed actions. Among the first models introduced
is Goldman \emph{et al.}
(\protect\hyperlink{ref-goldman_new_1999}{1999})'s model, which uses the
principle of action to construct a logical representation of the
problem. This paradigm consists in creating logical rules as if the
action in question were actually carried out, but in hypothesizing the
predicates that concretize the action and thus being able to browse the
research space thus created in order to find all the possible plans
containing the observed actions and concretizing defined intentions.
This model is strongly based on first-order logic and SWI Prolog logic
programming languages. Although revolutionary for the time, this system
pale in comparison to recent systems, particularly in terms of
prediction accuracy.

Some paradigms use algebra to determine possible plans from observed
actions. In particular, we find the model of Bouchard \emph{et al.}
(\protect\hyperlink{ref-bouchard_smart_2006}{2006}) which extends the
subsumption relationship from domain theory to the description of action
and sequence of action in order to introduce it as an order relationship
in the context of the construction of a lattice composed of possible
plans considering the observed actions. This model simply takes into
account the observed actions and selects any plan from the library of
plans that contains at least one observed action. Then this paradigm
will construct all the possible plans that correspond to the Cartesian
product of the observed actions with the actions contained in the
selected plans (while respecting their order). This system makes it
possible to obtain a subsumption relationship that corresponds to the
fact that the plans are more or less general. Unfortunately, this
relationship alone does not provide any information on which plan is
most likely.

That is why Roy \emph{et al.}
(\protect\hyperlink{ref-roy_possibilistic_2011}{2011}) created a
probabilistic extension of this model. This uses frequency data from a
system learning period to calculate the influence probabilities of each
plane in the recognition space. This makes it possible to calculate
probabilistic intervals for each plan, action as well as for a plan
knowing a given action. In order to determine the probability of each
plane knowing the upper bound of the lattice (plane containing all
observed actions) the sum of the conditional probabilities of the plane
for each observed action divided by the number of observed actions is
made. This gives a probability interval for each plane allowing the
ordinates. This model has the advantage of considering many possible
plans but has the disadvantage of seeing a computational explosion as
soon as the number of observed actions increases and the context is not
taken into account.

Another approach is that of grammar. Indeed, we can consider actions as
words and sequences as sentences and thus define a system that allows us
to recognize shots from incomplete sequences. ({\textbf{???}}) has
therefore created a system of intention recognition based on grammar. It
uses the evaluated grammar system to specify measurements from
observations. These measures will make it possible to select specific
plans and thus return a hierarchical hypothesis tree with the actions
already carried out, the future and the plans from which they are
derived. This model is very similar to first order logic-based systems,
and uses a SWI Prolog type logic language programming system. Given the
scope of maritime surveillance, this model, although taking very well
into account the context and the evolution of the measures, is only
poorly adapted to an application in assistance, particularly in the
absence of a system for discriminating against results plans.

Another class of approaches is that of diverting standard
problem-solving tools to solve the problem of intention recognition. It
is therefore possible, by modifying traditional algorithms or by
transforming a problem, to ensure that the solution of the tool
corresponds to the one sought.

Inoue and Inui (\protect\hyperlink{ref-inoue_ilpbased_2011}{2011})
develops the idea of a model that uses linear programming to solve the
recognition problem. Indeed, observations are introduced in the form of
causes in relation to hypotheses, in a first-order logic predicate
system. Each atom is then weighted and introduced into a process of
problem transformation by feedback and the introduction of order and
causality constraints in order to force the linear program towards
optimal solutions by taking into account observations. Although
ingenious, this solution does not discriminate between possible plans
and is very difficult to apply to real-time recognition situations,
mainly because of the problem transformation procedure required each
time the problem is updated.

Another constraint paradigm is the one presented by Raghavana \emph{et
al.} using a Markovian extension of first-order logic. The model
consists of a library of plans represented in the form of Horn clauses
indicating which actions imply which intentions. The aim is therefore to
reverse the implications in order to transform the deduction mechanism
into an abduction mechanism. Exclusionary constraints and a system of
weights acquired through learning must then be introduced to determine
the most likely intention. Once again, despite the presence of a system
of result discrimination, there is no consideration of context and
abductive transformation remains too cumbersome a process for real-time
recognition.

\hypertarget{networks}{%
\subsubsection{Networks}\label{networks}}

As in its early days, intention recognition can still be modeled in the
form of graphs. Very often in intention recognition, trees are used to
exploit the advantages of acyclicity in resolution and path algorithms.
In the prolific literature of Geib et al.~we find the model at the basis
of PHATT (Geib \protect\hyperlink{ref-geib_problems_2002}{2002}) which
consists of an AND/OR tree representing a HTN that contains the
intentions as well as their plans or methods. A prior relationship is
added to this model and it is through this model that constraints are
placed on the execution of actions. Once an action is observed, all the
successors of the action are unlocked as potential next observed action.
We can therefore infer by hierarchical path the candidate intentions for
the observed sequence.

Since this model does not allow discrimination of results, Geib and
Goldman (\protect\hyperlink{ref-geib_partial_2005}{2005}) then adds
probabilities to the explanations of the observations. The degree of
coverage of each possible goal is used to calculate the probability of
each goal. That is, the goal with the plan containing the most observed
action and the least unobserved action will be the most likely. This is
very ingenious, as the coverage rate is one of the most reliable
indicators. However, the model only takes into account temporality and
therefore has no contextual support. The representation in the form of a
tree also makes it very difficult to be flexible in terms of the plans,
which are then fixed a priori.

The RTH model is often used in the field, such as the hierarchical tree
form used by ({\textbf{???}}). The tree consists of nodes that represent
various levels of action and intent. A hierarchical relationship links
these elements together to define each intention and its methods. To
this tree is added an anteriority relationship that constrains the
execution order. This paradigm uses time markers that guarantee order
using an actualisation algorithm that also updates a hypothesis tree
containing possible intentions for each observation.

A probabilistic extension of the Avrahami-Zilberbrand and Kaminka
(\protect\hyperlink{ref-avrahami-zilberbrand_hybrid_2006}{2006}) applies
a hierarchical hidden Markov model to the action tree. Using three types
of probability, that of plan tracking, execution interleaving and
interruption, we can calculate the probability of execution of each plan
according to the observed sequence. The logic and contextual model
filtered on the possible plans upstream leaving us with few calculations
to order these plans.

This contextual model uses a decision tree based on a system of world
properties. Each property has a finite (and if possible very limited)
number of possible values. This allows you to create a tree containing
for each node a property and an arc for each value. This is combined
with other nodes or leaves that are actions. While running through the
tree during execution, the branches that do not correspond to the
current value of each property are pruned. Once a leaf is reached, it is
stored as a possible action. This considerably reduces the research
space but requires a balanced tree that is not too large or restrictive.

When we approach stochastic models, we very often find Markovian or
Bayesian models. These models use different probabilistic tools ranging
from simple probabilistic inference to the fusion of stochastic
networks. It can be noted that probabilities are often defined by
standard distributions or are isomorphic to weighted systems.

A stochastic model based on THRs is the one presented by Blaylock and
Allen (\protect\hyperlink{ref-blaylock_fast_2006}{2006}). This creates
hierarchical stacks to categorize abstraction levels from basic actions
to high level intentions. By chaining a hidden Markov model to these
stacks, the model is able to affect a probability of intention according
to the observed action.

Another stochastic paradigm is the one of Han and Pereira
(\protect\hyperlink{ref-han_contextdependent_2013}{2013}). It uses
Bayesian networks to define relationships between causes, intentions and
actions in a given field. Each category is treated separately in order
to reduce the search space. The observed actions are then selected from
the action network and extracted. The system then uses the intention
network to build a temporary Bayesian network using the NoisyOR method.
The network created is combined in the same way with the network of
causes and makes it possible to give the intention as well as the most
probable cause according to the observations.

The model of Kelley \emph{et al.}
(\protect\hyperlink{ref-kelley_contextbased_2012}{2012}) (based on
(Hovland \emph{et al.}
\protect\hyperlink{ref-hovland_skill_1996}{1996})) is a model using
hidden Markov networks. This stochastic network is built here by
learning data from robotic perception systems. The goal is to determine
intent using past observations. This model uses the theory of mind by
invoking that humans infer the intentions of their peers by using a
projection of their own.

Another contextual approach is the one developed for robotics by
({\textbf{???}}). The stochastic system is completed by a weighting
based on an analysis of vernacular corpuses. We can therefore use the
context of an observation to determine the most credible actions using
the relational system built with corpus analysis. This is based on the
observation of the objects in the scene and their condition. This makes
common sense actions much more likely and almost impossible actions
leading to semantic contradictions.

This principle is also used as the basis of the paradigm of
({\textbf{???}}) which forms a Bayesian theory of the mind. Using a
limited representation of the human mind, this model defines formulas
for updating beliefs and probabilities a posteriori of world states and
actions. This is constructed with sigmoid distributions on the simplex
of inferred belief. Then the probabilities of desire are calculated in
order to recover the most probable intention. This has been validated as
being close to the assessment of human candidates on simple intention
recognition scenarios.

\hypertarget{inverted-planning}{%
\subsection{Inverted planning}\label{inverted-planning}}

Another way to do intent recognition is to use the \emph{theory of
mind}. This theory states that an agent recognizes other agent's
behavior by projecting its own beliefs, desires and intentions on the
other agent. This particular model is called BDI fo Belief, Desire and
Intention. In our case the belief part is the knowledge database and
planning domain. The desires are the set of available goals and the
intent is the plan to pursue a set of goals.

This way of doing intent recognition is called ``inverse planning''
because it is the inversed problem of planning. In planning we get a
goal and we need to find a plan to achieve it, and in intent
recognition, we got an observed plan and we search for the goals.

This approach first emerged with the work of Ramırez and Geffner
(\protect\hyperlink{ref-ramirez_plan_2009}{2009}). This article uses a
classical planner to compute the most probable goal given a sequence of
actions. This uses constraints to make the cost of a plan correlate with
its likelyhood.

\hypertarget{probabilities-and-approximations}{%
\subsubsection{Probabilities and
approximations}\label{probabilities-and-approximations}}

In that section we explain how this operation is done.

For any set of observations \(\cal{O}\) the probability of the set is
the product of the probability of any observation \(o \in \cal{O}\). We
can then note \(\bb{P}(\cal{O})=\prod_{o\in \cal{O}} \bb{P}(o)\).

We assume that the observed agent is pursuing one of the knowns goals.
The event of an agent pursuing a goal is noted \(g\). This means that
\(\bb{P}(\cal{G}) = \sum_{g\in \cal{G}}\bb{P}(\cal{G}) = 1\) because the
event is certain.

Using conditional probabilities we can also note
\(\bb{P}(\cal{G}|\pi) = 1\) for a valid plan \(\pi\) that achieves
\(G\).

From dirrect application of Bayes theorem and the previous assomptions,
we have :

\begin{equation} P(\pi|O) = \frac{P(O|\pi) P(\pi)}{P(O)} = \frac{P(O|\pi) P(\pi|G) P(G)}{P(O)}\label{eq:plan-obs}\end{equation}

\begin{equation} P(G|O) = \frac{P(O|G)P(G)}{P(O)}\label{eq:goal-obs}\end{equation}

And from the total probability formula :

\begin{equation}P(O|G) = \sum_{\pi \in \Pi_G} P(O|\pi) P(\pi|G)\label{eq:obs-goal}\end{equation}

\hypertarget{rico}{%
\subsection{Rico}\label{rico}}

\hypertarget{conclusion}{%
\chapter{Conclusion}\label{conclusion}}

\hypertarget{references}{%
\chapter{References}\label{references}}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-alessandro_ometa_2007}{}%
Alessandro, W., and I. Piumarta\\
OMeta: An object-oriented language for pattern matching,
\emph{Proceedings of the 2007 symposium on Dynamic languages}, 2007.

\leavevmode\hypertarget{ref-aljazzar_heuristic_2011}{}%
Aljazzar, H., and S. Leue\\
K⁎: A heuristic search algorithm for finding the k shortest paths,
\emph{Artificial Intelligence}, 175 (18), 2129--2154, 2011.

\leavevmode\hypertarget{ref-ambite_planning_1997}{}%
Ambite, J. L., and C. A. Knoblock\\
\emph{Planning by Rewriting: Efficiently Generating High-Quality
Plans.}, DTIC Document, 1997.

\leavevmode\hypertarget{ref-americanheritagedictionary_formal_2011}{}%
American Heritage Dictionary\\
Formal (adj.), \emph{American Heritage Dictionary of the English
Language}, 2011a.

\leavevmode\hypertarget{ref-americanheritagedictionary_circularity_2011}{}%
American Heritage Dictionary\\
Circularity (n.d.), \emph{American Heritage Dictionary of the English
Language}, 2011b.

\leavevmode\hypertarget{ref-asimov_gods_1973}{}%
Asimov, I.\\
\emph{The gods themselves}, Greenwich, Connecticut: Fawcett Crest, 1973.

\leavevmode\hypertarget{ref-avrahami-zilberbrand_hybrid_2006}{}%
Avrahami-Zilberbrand, D., and G. A. Kaminka\\
Hybrid symbolic-probabilistic plan recognizer: Initial steps,
\emph{Proceedings of AAAI workshop on modeling others from observations
(MOO-06)}, 2006.

\leavevmode\hypertarget{ref-awodey_category_2010}{}%
Awodey, S.\\
\emph{Category theory}, 2nd ed. Oxford logic guides 52Oxford ; New York:
Oxford University Press, 2010.

\leavevmode\hypertarget{ref-baader_description_2003}{}%
Baader, F.\\
\emph{The description logic handbook: Theory, implementation and
applications}, Cambridge university press, 2003.

\leavevmode\hypertarget{ref-babli_use_2015}{}%
Babli, M., E. Marzal, and E. Onaindia\\
On the use of ontologies to extend knowledge in online planning,
\emph{KEPS 2018}, 54, 2015.

\leavevmode\hypertarget{ref-backus_syntax_1959}{}%
Backus, J. W.\\
The syntax and semantics of the proposed international algebraic
language of the Zurich ACM-GAMM conference, \emph{Proceedings of the
International Comference on Information Processing, 1959}, 1959.

\leavevmode\hypertarget{ref-bechon_hipop_2014}{}%
Bechon, P., M. Barbier, G. Infantes, C. Lesire, and V. Vidal\\
HiPOP: Hierarchical Partial-Order Planning, \emph{European Starting AI
Researcher Symposium}, IOS Press, 2014, 264,51--60.

\leavevmode\hypertarget{ref-becket_dcgs_2008}{}%
Becket, R., and Z. Somogyi\\
DCGs+ memoing= packrat parsing but is it worth it?, \emph{International
Symposium on Practical Aspects of Declarative Languages}, Springer,
2008, 182--196.

\leavevmode\hypertarget{ref-beckett_turtle_2011}{}%
Beckett, D., and T. Berners-Lee\\
\emph{Turtle - Terse RDF Triple Language}, W3C Team Submission W3C,
March 2011.

\leavevmode\hypertarget{ref-bercher_hybrid_2014}{}%
Bercher, P., S. Keen, and S. Biundo\\
Hybrid planning heuristics based on task decomposition graphs,
\emph{Seventh Annual Symposium on Combinatorial Search}, 2014.

\leavevmode\hypertarget{ref-blaylock_fast_2006}{}%
Blaylock, N., and J. Allen\\
Fast hierarchical goal schema recognition, \emph{Proceedings of the
National Conference on Artificial Intelligence}, Menlo Park, CA;
Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006, 21,796.

\leavevmode\hypertarget{ref-bouchard_smart_2006}{}%
Bouchard, B., S. Giroux, and A. Bouzouane\\
A Smart Home Agent for Plan Recognition of Cognitively-impaired
Patients., \emph{Journal of Computers}, 1 (5), 53--62, 2006.

\leavevmode\hypertarget{ref-brenner_multiagent_2003}{}%
Brenner, M.\\
A multiagent planning language, \emph{Proc. Of the Workshop on PDDL,
ICAPS}, 2003, 3,33--38.

\leavevmode\hypertarget{ref-burckert_terminologies_1994}{}%
Bürckert, H.-J.\\
Terminologies and rules, \emph{Workshop on Information Systems and
Artificial Intelligence}, Springer, 1994, 44--63.

\leavevmode\hypertarget{ref-cantor_property_1874}{}%
Cantor, G.\\
On a Property of the Class of all Real Algebraic Numbers.,
\emph{Crelle's Journal for Mathematics}, 77, 258--262, 1874.

\leavevmode\hypertarget{ref-cantor_beitrage_1895}{}%
Cantor, G.\\
Beiträge zur Begründung der transfiniten Mengenlehre,
\emph{Mathematische Annalen}, 46 (4), 481--512, 1895.

\leavevmode\hypertarget{ref-chomsky_three_1956}{}%
Chomsky, N.\\
Three models for the description of language, \emph{IRE Transactions on
information theory}, 2 (3), 113--124, 1956.

\leavevmode\hypertarget{ref-ciesielski_set_1997}{}%
Ciesielski, K.\\
\emph{Set Theory for the Working Mathematician}, Cambridge University
Press, August 1997.

\leavevmode\hypertarget{ref-coles_popf2_2011}{}%
Coles, A., A. Coles, M. Fox, and D. Long\\
Popf2 : A forward-chaining partial order planner, \emph{IPC}, 65, 2011.

\leavevmode\hypertarget{ref-collinsenglishdictionary_abstraction_2014}{}%
Collins English Dictionary\\
Abstraction (n.d.), \emph{Collins English Dictionary Complete and
Unabridged}, 2014.

\leavevmode\hypertarget{ref-dalfonso_generalized_2011}{}%
D'Alfonso, D.\\
Generalized Quantifiers: Logic and Language, 2011.

\leavevmode\hypertarget{ref-dvorak_flexible_2014}{}%
Dvorak, F., A. Bit-Monnot, F. Ingrand, and M. Ghallab\\
A flexible ANML actor and planner in robotics, \emph{Planning and
Robotics (PlanRob) Workshop (ICAPS)}, 2014.

\leavevmode\hypertarget{ref-erol_umcp_1994}{}%
Erol, K., J. A. Hendler, and D. S. Nau\\
UMCP: A Sound and Complete Procedure for Hierarchical Task-network
Planning, \emph{Proceedings of the International Conference on
Artificial Intelligence Planning Systems}, University of Chicago,
Chicago, Illinois, USA: AAAI Press, June 1994, 2,249--254.

\leavevmode\hypertarget{ref-erol_complexity_1995}{}%
Erol, K., D. S. Nau, and V. S. Subrahmanian\\
Complexity, decidability and undecidability results for
domain-independent planning, \emph{Artificial intelligence}, 76 (1-2),
75--88, 1995.

\leavevmode\hypertarget{ref-fikes_strips_1971}{}%
Fikes, R. E., and N. J. Nilsson\\
STRIPS: A new approach to the application of theorem proving to problem
solving, \emph{Artificial intelligence}, 2 (3-4), 189--208, 1971.

\leavevmode\hypertarget{ref-ford_packrat_2002}{}%
Ford, B.\\
Packrat parsing:: Simple, powerful, lazy, linear time, functional pearl,
\emph{ACM SIGPLAN Notices}, ACM, 2002, 37,36--47.

\leavevmode\hypertarget{ref-ford_parsing_2004}{}%
Ford, B.\\
Parsing expression grammars: A recognition-based syntactic foundation,
\emph{ACM SIGPLAN Notices}, ACM, 2004, 39,111--122.

\leavevmode\hypertarget{ref-fox_natural_1997}{}%
Fox, M.\\
Natural hierarchical planning using operator decomposition,
\emph{European Conference on Planning}, Springer, 1997, 195--207.

\leavevmode\hypertarget{ref-fox_pddl_2002}{}%
Fox, M., and D. Long\\
PDDL+: Modeling continuous time dependent effects, \emph{Proceedings of
the 3rd International NASA Workshop on Planning and Scheduling for
Space}, 2002, 4,34.

\leavevmode\hypertarget{ref-fraenkel_foundations_1973}{}%
Fraenkel, A. A., Y. Bar-Hillel, and A. Levy\\
\emph{Foundations of set theory}, vol. 67Elsevier, 1973.

\leavevmode\hypertarget{ref-geib_problems_2002}{}%
Geib, C. W.\\
Problems with intent recognition for elder care, \emph{Proceedings of
the AAAI-02 Workshop ``Automation as Caregiver}, 2002, 13--17.

\leavevmode\hypertarget{ref-geib_partial_2005}{}%
Geib, C. W., and R. P. Goldman\\
Partial observability and probabilistic plan/goal recognition,
\emph{Proceedings of the International workshop on modeling other agents
from observations (MOO-05)}, 2005.

\leavevmode\hypertarget{ref-gerevini_combining_2008}{}%
Gerevini, A., U. Kuter, D. S. Nau, A. Saetti, and N. Waisbrot\\
Combining Domain-Independent Planning and HTN Planning: The Duet
Planner, \emph{Proceedings of the European Conference on Artificial
Intelligence}, 2008, 18,573--577.

\leavevmode\hypertarget{ref-ghallab_automated_2004}{}%
Ghallab, M., D. Nau, and P. Traverso\\
\emph{Automated planning: Theory \& practice}, Elsevier, 2004.

\leavevmode\hypertarget{ref-ghallab_automated_2016}{}%
Ghallab, M., D. Nau, and P. Traverso\\
\emph{Automated Planning and Acting}, Cambridge University Press, 2016.

\leavevmode\hypertarget{ref-goldman_new_1999}{}%
Goldman, R. P., C. W. Geib, and C. A. Miller\\
A new model of plan recognition, \emph{Proceedings of the Fifteenth
conference on Uncertainty in artificial intelligence}, Morgan Kaufmann
Publishers Inc., 1999, 245--254.

\leavevmode\hypertarget{ref-gobelbecker_coming_2010}{}%
Göbelbecker, M., T. Keller, P. Eyerich, M. Brenner, and B. Nebel\\
Coming Up With Good Excuses: What to do When no Plan Can be Found,
\emph{Proceedings of the International Conference on Automated Planning
and Scheduling}, AAAI Press, May 2010, 20,81--88.

\leavevmode\hypertarget{ref-gradel_twovariable_1997}{}%
Gradel, E., M. Otto, and E. Rosen\\
Two-variable logic with counting is decidable, \emph{Logic in Computer
Science, 1997. LICS'97. Proceedings., 12th Annual IEEE Symposium on},
IEEE, 1997, 306--317.

\leavevmode\hypertarget{ref-grunwald_minimum_1996}{}%
Grünwald, P.\\
A minimum description length approach to grammar inference, in S.
Wermter, E. Riloff, and G. Scheler (eds.), \emph{Connectionist,
Statistical and Symbolic Approaches to Learning for Natural Language
Processing}, Lecture Notes in Computer Science; Springer Berlin
Heidelberg, 1996, 203--216.

\leavevmode\hypertarget{ref-han_contextdependent_2013}{}%
Han, T. A., and L. M. Pereira\\
Context-dependent incremental decision making scrutinizing the
intentions of others via Bayesian network model construction,
\emph{Intelligent Decision Technologies}, 7 (4), 293--317, 2013.

\leavevmode\hypertarget{ref-hart_opencog_2008}{}%
Hart, D., and B. Goertzel\\
Opencog: A software framework for integrative artificial general
intelligence, \emph{AGI}, 2008, 468--472.

\leavevmode\hypertarget{ref-hehner_practical_2012}{}%
Hehner, E. C.\\
\emph{A practical theory of programming}, Springer Science \& Business
Media, 2012.

\leavevmode\hypertarget{ref-helmert_fast_2011}{}%
Helmert, M., G. Röger, and E. Karpas\\
Fast downward stone soup: A baseline for building planner portfolios,
\emph{ICAPS 2011 Workshop on Planning and Learning}, Citeseer, 2011,
28--35.

\leavevmode\hypertarget{ref-henglein_peg_2017}{}%
Henglein, F., and U. T. Rasmussen\\
PEG parsing in less space using progressive tabling and dynamic
analysis, \emph{Proceedings of the 2017 ACM SIGPLAN Workshop on Partial
Evaluation and Program Manipulation}, ACM, 2017, 35--46.

\leavevmode\hypertarget{ref-hernandez_reifying_2015}{}%
Hernández, D., A. Hogan, and M. Krötzsch\\
Reifying RDF: What works well with wikidata?, \emph{SSWS@ ISWC}, 1457,
32--47, 2015.

\leavevmode\hypertarget{ref-horrocks_shiq_2003}{}%
Horrocks, I., P. F. Patel-Schneider, and F. V. Harmelen\\
From SHIQ and RDF to OWL: The Making of a Web Ontology Language,
\emph{Journal of Web Semantics}, 1, 2003, 2003.

\leavevmode\hypertarget{ref-hovland_skill_1996}{}%
Hovland, G. E., P. Sikka, and B. J. McCarragher\\
Skill acquisition from human demonstration using a hidden markov model,
\emph{Robotics and Automation, 1996. Proceedings., 1996 IEEE
International Conference on}, Ieee, 1996, 3,2706--2711.

\leavevmode\hypertarget{ref-hoschele_mining_2017}{}%
Höschele, M., and A. Zeller\\
Mining input grammars with AUTOGRAM, \emph{Proceedings of the 39th
International Conference on Software Engineering Companion}, IEEE Press,
2017, 31--34.

\leavevmode\hypertarget{ref-hutton_monadic_1996}{}%
Hutton, G., and E. Meijer\\
Monadic parser combinators, 1996.

\leavevmode\hypertarget{ref-inoue_ilpbased_2011}{}%
Inoue, N., and K. Inui\\
ILP-Based Reasoning for Weighted Abduction., \emph{Plan, Activity, and
Intent Recognition}, 2011.

\leavevmode\hypertarget{ref-kambhampati_design_1994}{}%
Kambhampati, S.\\
Design Tradeoffs in Partial Order (Plan space) Planning., \emph{AIPS},
1994, 92--97.

\leavevmode\hypertarget{ref-kambhampati_hybrid_1998}{}%
Kambhampati, S., A. Mali, and B. Srivastava\\
Hybrid planning for partially hierarchical domains, \emph{AAAI/IAAI},
1998, 882--888.

\leavevmode\hypertarget{ref-kelley_contextbased_2012}{}%
Kelley, R., A. Tavakkoli, C. King, A. Ambardekar, and M. Nicolescu\\
Context-based bayesian intent recognition, \emph{Autonomous Mental
Development, IEEE Transactions on}, 4 (3), 215--225, 2012.

\leavevmode\hypertarget{ref-klyne_resource_2004}{}%
Klyne, G., and J. J. Carroll\\
\emph{Resource Description Framework (RDF): Concepts and Abstract
Syntax}, Language Specification W3C RecommendationW3C, 2004.

\leavevmode\hypertarget{ref-korzybski_science_1958}{}%
Korzybski, A.\\
\emph{Science and sanity; an introduction to non-Aristotelian systems
and general semantics.}, Lakeville, Conn.: International
Non-Aristotelian Library Pub. Co.; distributed by Institute of General
Semantics, 1958.

\leavevmode\hypertarget{ref-kovacs_bnf_2011}{}%
Kovacs, D. L.\\
\emph{BNF Description of PDDL 3.1}, Unpublished manuscript from the
IPC-2011 websiteIPC, 2011.

\leavevmode\hypertarget{ref-kovacs_multiagent_2012}{}%
Kovács, D. L.\\
A multi-agent extension of PDDL3. 1, 2012.

\leavevmode\hypertarget{ref-kovacs_converting_2013}{}%
Kovács, D. L., and T. P. Dobrowiecki\\
Converting MA-PDDL to extensive-form games, \emph{Acta Polytechnica
Hungarica}, 10 (8), 27--47, 2013.

\leavevmode\hypertarget{ref-kovitz_terminology_2018}{}%
Kovitz, B.\\
Terminology - What do you call graphs that allow edges to edges?,
\emph{Mathematics Stack Exchange}, January 2018.

\leavevmode\hypertarget{ref-lindstrom_first_1966}{}%
Lindström, P.\\
First Order Predicate Logic with Generalized Quantifiers, 1966.

\leavevmode\hypertarget{ref-loff_computational_2018}{}%
Loff, B., N. Moreira, and R. Reis\\
The Computational Power of Parsing Expression Grammars,
\emph{International Conference on Developments in Language Theory},
Springer, 2018, 491--502.

\leavevmode\hypertarget{ref-mcdermott_opt_2005}{}%
McDermott, D.\\
\emph{OPT Manual Version 1.7. 3 (Reflects Opt Version 1.6. 11)* DRAFT},
2005.

\leavevmode\hypertarget{ref-mcdermott_representing_2002}{}%
McDermott, D., and D. Dou\\
Representing disjunction and quantifiers in RDF, \emph{International
Semantic Web Conference}, Springer, 2002, 250--263.

\leavevmode\hypertarget{ref-motik_properties_2007}{}%
Motik, B.\\
On the properties of metamodeling in OWL, \emph{Journal of Logic and
Computation}, 17 (4), 617--637, 2007.

\leavevmode\hypertarget{ref-nau_shop2_2003}{}%
Nau, D. S., T.-C. Au, O. Ilghami, U. Kuter, J. W. Murdock, et al.\\
SHOP2: An HTN planning system, \emph{J. Artif. Intell. Res.(JAIR)}, 20,
379--404, 2003.

\leavevmode\hypertarget{ref-nguyen_reviving_2001}{}%
Nguyen, X., and S. Kambhampati\\
Reviving partial order planning, \emph{IJCAI}, 2001, 1,459--464.

\leavevmode\hypertarget{ref-paulson_semanticsdirected_1982}{}%
Paulson, L.\\
A semantics-directed compiler generator, \emph{Proceedings of the 9th
ACM SIGPLAN-SIGACT symposium on Principles of programming languages -
POPL '82}, Albuquerque, Mexico: ACM Press, 1982, 224--233.
doi:\href{https://doi.org/10.1145/582153.582178}{10.1145/582153.582178}.

\leavevmode\hypertarget{ref-peano_arithmetices_1889}{}%
Peano, G.\\
\emph{Arithmetices principia: Nova methodo exposita}, Fratres Bocca,
1889.

\leavevmode\hypertarget{ref-pednault_adl_1989}{}%
Pednault, E. P.\\
ADL: Exploring the Middle Ground Between STRIPS and the Situation
Calculus., \emph{Kr}, 89, 324--332, 1989.

\leavevmode\hypertarget{ref-pellier_pddl4j_2017}{}%
Pellier, D., and H. Fiorino\\
PDDL4J: A planning domain description library for java, December 2017.
doi:\href{https://doi.org/10.1080/0952813X.2017.1409278}{10.1080/0952813X.2017.1409278}.

\leavevmode\hypertarget{ref-penberthy_ucpop_1992}{}%
Penberthy, J. S., D. S. Weld, and others\\
UCPOP: A Sound, Complete, Partial Order Planner for ADL, \emph{Kr}, 92,
103--114, 1992.

\leavevmode\hypertarget{ref-peot_threatremoval_1993}{}%
Peot, M. A., and D. E. Smith\\
Threat-removal strategies for partial-order planning, \emph{AAAI}, 1993,
93,492--499.

\leavevmode\hypertarget{ref-raghavana_plan}{}%
Raghavana, S., P. Singlab, and R. J. Mooneya\\
Plan Recognition using Statistical Relational Models.

\leavevmode\hypertarget{ref-ramirez_plan_2009}{}%
Ramırez, M., and H. Geffner\\
Plan recognition as planning, \emph{Proceedings of the International
Conference on International Conference on Automated Planning and
Scheduling}, AAAI Press, 2009, 19,1778--1783.

\leavevmode\hypertarget{ref-ramoul_mixedinitiative_2018}{}%
RAMOUL, A.\\
Mixed-initiative planning system to assist the management of complex IT
systems 2018.

\leavevmode\hypertarget{ref-renggli_practical_2010}{}%
Renggli, L., S. Ducasse, T. Gîrba, and O. Nierstrasz\\
Practical Dynamic Grammars for Dynamic Languages, \emph{Workshop on
Dynamic Languages and Applications}, Malaga, Spain, 2010, 4.

\leavevmode\hypertarget{ref-roy_possibilistic_2011}{}%
Roy, P. C., A. Bouzouane, S. Giroux, and B. Bouchard\\
Possibilistic activity recognition in smart homes for cognitively
impaired people, \emph{Applied Artificial Intelligence}, 25 (10),
883--926, 2011.

\leavevmode\hypertarget{ref-russell_mysticism_1917}{}%
Russell, B.\\
\emph{Mysticism and Logic and Other Essays}, London: George Allen and
Unwin, 1917.

\leavevmode\hypertarget{ref-russell_principia_1978}{}%
Russell, B., and A. N. Whitehead\\
\emph{Principia mathematica}, Cambridge University Press, 1978.

\leavevmode\hypertarget{ref-sapena_combining_2014}{}%
Sapena, O., E. Onaindıa, and A. Torreno\\
Combining heuristics to accelerate forward partial-order planning,
\emph{CSTPS}, 25, 2014.

\leavevmode\hypertarget{ref-shekhar_learning_2016}{}%
Shekhar, S., and D. Khemani\\
Learning and Tuning Meta-heuristics in Plan Space Planning, \emph{arXiv
preprint arXiv:1601.07483}, 2016.

\leavevmode\hypertarget{ref-silberschatz_port_1981}{}%
Silberschatz, A.\\
Port directed communication, \emph{The Computer Journal}, 24 (1),
78--82, January 1981.
doi:\href{https://doi.org/10.1093/comjnl/24.1.78}{10.1093/comjnl/24.1.78}.

\leavevmode\hypertarget{ref-sohrabi_plan_2016}{}%
Sohrabi, S., A. V. Riabov, and O. Udrea\\
Plan Recognition as Planning Revisited, \emph{Proceedings of the
International Joint Conference on Artificial Intelligence}, Vol. 25,
2016.

\leavevmode\hypertarget{ref-souto_dynamic_1998}{}%
Souto, D. C., M. V. Ferro, and M. A. Pardo\\
Dynamic Programming as Frame for Efficient Parsing, \emph{Proceedings
SCCC'98. 18th International Conference of the Chilean Society of
Computer Science (Cat. No.98EX212)(SCCC)}, November 1998, 68.
doi:\href{https://doi.org/10.1109/SCCC.1998.730784}{10.1109/SCCC.1998.730784}.

\leavevmode\hypertarget{ref-thiebaux_defense_2005}{}%
Thiébaux, S., J. Hoffmann, and B. Nebel\\
In defense of PDDL axioms, \emph{Artificial Intelligence}, 168 (1-2),
38--69, 2005.

\leavevmode\hypertarget{ref-tolksdorf_semantic_2004}{}%
Tolksdorf, R., L. Nixon, F. Liebsch, D. Minh Nguyen, and E. Paslaru
Bontas\\
Semantic web spaces, 2004.

\leavevmode\hypertarget{ref-unicodeconsortium_unicode_2018a}{}%
Unicode Consortium\\
\emph{The Unicode Standard, Version 11.0}, Core Specification
11.0Mountain View, CA, June 2018a.

\leavevmode\hypertarget{ref-unicodeconsortium_unicode_2018}{}%
Unicode Consortium\\
Unicode Character Database, \emph{About the Unicode Character Database},
June 2018b.

\leavevmode\hypertarget{ref-vanderkrogt_plan_2005}{}%
Van Der Krogt, R., and M. De Weerdt\\
Plan Repair as an Extension of Planning., \emph{ICAPS}, 2005,
5,161--170.

\leavevmode\hypertarget{ref-vanharmelen_handbook_2008}{}%
Van Harmelen, F., V. Lifschitz, and B. Porter\\
\emph{Handbook of knowledge representation}, vol. 1Elsevier, 2008.

\leavevmode\hypertarget{ref-vepstas_hypergraph_2008}{}%
Vepstas, L.\\
Hypergraph edge-to-edge, \emph{Wikipedia}, May 2008.

\leavevmode\hypertarget{ref-vepstas_sheaves_2008}{}%
Vepštas, L.\\
\emph{Sheaves: A Topological Approach to Big Data}, 2008.

\leavevmode\hypertarget{ref-w3c_rdf_2004a}{}%
W3C\\
\emph{RDF Semantics}, W3C, 2004a.

\leavevmode\hypertarget{ref-w3c_rdf_2004}{}%
W3C\\
RDF Vocabulary Description Language 1.0: RDF Schema February 2004b.

\leavevmode\hypertarget{ref-younes_ppddl_2004}{}%
Younes, H. akan L., and M. L. Littman\\
PPDDL 1.0: An extension to PDDL for expressing planning domains with
probabilistic effects, \emph{Techn. Rep. CMU-CS-04-162}, 2004.

\leavevmode\hypertarget{ref-younes_vhpop_2003}{}%
Younes, H. akan L., and R. G. Simmons\\
VHPOP : Versatile heuristic partial order planner, \emph{JAIR},
405--430, 2003.

\leavevmode\hypertarget{ref-young_dpocl_1994}{}%
Young, R. M., and J. D. Moore\\
DPOCL: A principled approach to discourse planning, \emph{Proceedings of
the Seventh International Workshop on Natural Language Generation},
Association for Computational Linguistics, 1994, 13--20.

\hypertarget{apendix}{%
\chapter*{Apendix}\label{apendix}}
\addcontentsline{toc}{chapter}{Apendix}

\hypertarget{chapter-1}{%
\section{Chapter 1}\label{chapter-1}}

\textbf{SAMIR}:

\begin{itemize}
\tightlist
\item
  Cite slow start 1.2
\item
  System : implentation -\textgreater{} Model : theory
\item
  Examples
\item
  theorems or not
\item
  s : D -\textgreater{} D notation in table
\item
  Double arrow too for ±
\item
  to port : porter une notion in smth
\item
  1.2.2 : criterion
\item
  Example of parsing
\item
  1.2.6.1 : light defeasible logic
\item
  lacks properties and proofs: what are the advantages
\end{itemize}

\begin{table}\footnotesize
\centering

\caption{}

\begin{tabular}{@{}llllll@{}}
\toprule

Symbol & Name & Arity & Arguments & Type & Definition \\\midrule

\(=\) & Equal & 2 & expr, expr & boolean & \(x = x : \top\) \\
\(\neq\) & Not Equal & 2 & expr, expr & boolean & \(x \neq x : \bot\) \\
\(:\) & Such that & 2 & element,
predicate & element & \namecref{axi:specification} of \nameref{axi:specification} \\
\(?\) & Predicate & n & expr & boolean & arbitrary \\
\(\top\) & True & 0 & NA & boolean &  \\
\(\bot\) & False & 0 & NA & boolean &  \\
\(\lnot\) & Negation & 1 & boolean & boolean & \(\lnot \top = \bot\) \\
\(\land\) & Conjunction & n & boolean & boolean & \(a \land b \vdash (a = b = \top)\) \\
\(\lor\) & Disjunction & n & boolean & boolean & \(\lnot(a \lor b) \vdash (a = b = \bot)\) \\
\(\veeonwedge\) & Logic operator & n & boolean & boolean & arbitrary \\
\(\vdash\) & Entailment & 2 & boolean & boolean &  \\
\(\forall\) & Universality & 1 & var & modifier &  \\
\(\exists\) & Existentiality & 1 & var & modifier &  \\
\(\exists!\) & Unicity & 1 & var & modifier &  \\
\(\nexists\) & Exclusivity & 1 & var & modifier & \(\lneq \exists\) \\
\(\textsection\) & Solution & 1 & var & modifier &  \\
\([]\) & Iverson's
brackets & 1 & boolean & int & \([\bot]=0 \land [\top]=1\) \\
\(\{\}\) & Set & n & elements & set &  \\
\(\emptyset\) & Empty set & 0 & NA & set & \(\emptyset = \{\}\) \\
\(\in\) & Member & 2 & element, set & boolean &  \\
\(\subset\) & Subset & 2 & set & boolean & \(\cal{S} \subset \cal{T} \vdash ((e \in \cal{S} \vdash e\in \cal{T}) \land \cal{S} \neq \cal{T})\) \\
\(\cup\) & Union & n & set & set & \(\cal{S} \cup \cal{T} = \{e : e \in \cal{S} \lor e \in \cal{T} \}\) \\
\(\cap\) & Intersection & n & set & set & \(\cal{S} \cap \cal{T} = \{e : e \in \cal{S} \land e \in \cal{T} \}\) \\
\(\setminus\) & Difference & 2 & set & set & \(\cal{S} \setminus \cal{T} = \{e : e \in \cal{S} \land e \notin \cal{T} \}\) \\
\(\times\) & Cartesian
product & n & set & set & \(\cal{S} \times \cal{T} = \{\langle e_{\cal{S}}, e_{\cal{T}} \rangle : e_{\cal{S}} \in \cal{S} \land e_{\cal{T}}\in \cal{T}\}\) \\
\(| |\) & Cardinal & 1 & set & int &  \\
\(\wp\) & Powerset & 1 & set & set & \namecref{axi:powerset} of \nameref{axi:powerset} \\
\(\circ\) & Function composition & n & function & function &  \\
\(\sigma\) & Selection & 2 & predicate,
set & set & \(\sigma_?(\cal{S}) = \{e : ?(e) \land e\in \cal{S}\}\) \\
\(\pi\) & Projection & 2 & function,
set & set & \(\pi_f(\cal{S}) = \{ f(e) : e \in \cal{S}\}\) \\
\(\mapsto\) & Substitution & 2 & var, function,
expr & expr & \((e \mapsto f(e))(\bb{e}(e)) = \bb{e}(f(e))\) \\
\(\langle \rangle\) & Tuple & n & elements & tuple &  \\
\(\to\) & Association & 2 & elements & tuple &  \\
\(\phi\) & Incidence/Adjacence & n & various & various &  \\
\(\chi\) & Transitivity & 1 & relation & relation &  \\
\(\div\) & Quotient & 2 & function, graph & graph &  \\
\(\mu\) & Meta & 1 & entity & entity &  \\
\(\nu\) & Name & 1 & entity & string &  \\
\(\rho\) & Parameter & 1 & entity & list &  \\
\(\otimes\) & Flaws & 1 & plan & set &  \\
\(\odot\) & Resolvers & 1 & flaw & set &  \\
\(\downdasharrow\) & Partial support & 2 & link, action & boolean &  \\
\(\downarrow\) & Full support & 2 & plan, action & boolean &  \\
\(\prec\) & Precedance & 2 & action & boolean &  \\
\(\succ\) & Succession & 2 & action & boolean &  \\
\(\Mapsto^*\) & Shortest path & ? &  &  &  \\
\(h\) & Heuristic & 1 & element & float &  \\
\(\gamma\) & Constraints & 1 & element & set &  \\
\(¢\) & Cost & 1 & element & float &  \\
\(d\) & Duration & 1 & element & time &  \\
 &  &  &  &  &  \\
 &  &  &  &  &  \\
 &  &  &  &  &  \\
 &  &  &  &  &  \\
 &  &  &  &  &  \\
 &  &  &  &  &  \\
 &  &  &  &  &  \\
 &  &  &  &  &  \\
 &  &  &  &  &  \\

\bottomrule
\end{tabular}

\end{table}

\end{document}
