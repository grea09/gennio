% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
  11pt,
  a4paper,
  twoside,
  openright,
  titlepage,
  numbers=noenddot,
  headinclude,
  cleardoublepage=empty,
  openany,
  parskip]{scrreprt}
\usepackage{lmodern}
\usepackage{multicol}
\usepackage{amssymb,amsmath}
 %%%%% PANCAKE
	\usepackage{amsthm}
	\usepackage{mfirstuc}
	\theoremstyle{plain}
		\newtheorem{theorem}{\capitalisewords{theorem}}[]
			\newtheorem*{lemma}{\capitalisewords{lemma}}
		\newtheorem*{proposition}{\capitalisewords{proposition}}
		\theoremstyle{definition}
		\newtheorem{definition}{\capitalisewords{definition}}[]
		\newtheorem{example}{\capitalisewords{example}}[]
		\newtheorem{postulate}{\capitalisewords{postulate}}[]
		\newtheorem{problem}{\capitalisewords{problem}}[]
			\newtheorem*{axiom}{\capitalisewords{axiom}}
		\theoremstyle{remark}
		
 %%%%% PANCAKE
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  \setmainfont[]{Aller}
  \setmathfont[]{STIX Two Math}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
 %%%%% PANCAKE

\IfFileExists{xcolor-solarized.sty}{\usepackage{xcolor-solarized}} {
% We now define the sixteen \solarized{} colors.
\definecolor{solarized-base03} {RGB}{000, 043, 054}
\definecolor{solarized-base02} {RGB}{007, 054, 066}
\definecolor{solarized-base01} {RGB}{088, 110, 117}
\definecolor{solarized-base00} {RGB}{101, 123, 131}
\definecolor{solarized-base0}  {RGB}{131, 148, 150}
\definecolor{solarized-base1}  {RGB}{147, 161, 161}
\definecolor{solarized-base2}  {RGB}{238, 232, 213}
\definecolor{solarized-base3}  {RGB}{253, 246, 227}
\definecolor{solarized-yellow} {RGB}{181, 137, 000}
\definecolor{solarized-orange} {RGB}{203, 075, 022}
\definecolor{solarized-red}    {RGB}{220, 050, 047}
\definecolor{solarized-magenta}{RGB}{211, 054, 130}
\definecolor{solarized-violet} {RGB}{108, 113, 196}
\definecolor{solarized-blue}   {RGB}{038, 139, 210}
\definecolor{solarized-cyan}   {RGB}{042, 161, 152}
\definecolor{solarized-green}  {RGB}{133, 153, 000}
}
 %%%%% PANCAKE
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Endomorphic metalanguage and abstract planning for real-time intent recognition},
  pdfauthor={Antoine Gréa},
  colorlinks=true,
  linkcolor=solarized-cyan, %%%%% PANCAKE
  filecolor=solarized-magenta, %%%%% PANCAKE
  citecolor=solarized-base1, %%%%% PANCAKE
  urlcolor=solarized-blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
 %%%%% PANCAKE
 %%%%% PANCAKE
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

%%%%%% for algorithms
\errorcontextlines\maxdimen
\makeatletter
\algrenewcommand\ALG@beginalgorithmic{\footnotesize}
%\algrenewcommand\alglinenumber[1]{\color{solarized-base0} #1}
\renewcommand{\algorithmiccomment}[1]{{\hfill\(\smalltriangleright\) \emph{#1}}}

\newcommand{\algorithmicbreak}{\textbf{break}}
\newcommand{\Break}{\State \algorithmicbreak}\newcommand{\algorithmiccontinue}{\textbf{continue}} 
\newcommand{\Continue}{\State \algorithmiccontinue}

% begin vertical rule patch for algorithmicx (http://tex.stackexchange.com/questions/144840/vertical-loop-block-lines-in-algorithmicx-with-noend-option)
% start with some helper code
% This is the vertical rule that is inserted
    \newcommand*{\algrule}[1][\algorithmicindent]{\makebox[#1][l]{\color{solarized-base0}\hspace*{.5em}\thealgruleextra\vrule height \thealgruleheight depth \thealgruledepth}}%
% its height and depth need to be adjustable
\newcommand*{\thealgruleextra}{}
\newcommand*{\thealgruleheight}{.95\baselineskip}
\newcommand*{\thealgruledepth}{.25\baselineskip}

\newcount\ALG@printindent@tempcnta
\def\ALG@printindent{%
    \ifnum \theALG@nested>0% is there anything to print
        \ifx\ALG@text\ALG@x@notext% is this an end group without any text?
            % do nothing
        \else
            \unskip
            \addvspace{-1pt}% FUDGE to make the rules line up
            % draw a rule for each indent level
            \ALG@printindent@tempcnta=1
            \loop
                \algrule[\csname ALG@ind@\the\ALG@printindent@tempcnta\endcsname]%
                \advance \ALG@printindent@tempcnta 1
            \ifnum \ALG@printindent@tempcnta<\numexpr\theALG@nested+1\relax% can't do <=, so add one to RHS and use < instead
            \repeat
        \fi
    \fi
    }%
\usepackage{etoolbox}
% the following line injects our new indent handling code in place of the default spacing
\patchcmd{\ALG@doentity}{\noindent\hskip\ALG@tlm}{\ALG@printindent}{}{\errmessage{failed to patch}}
\makeatother

%For nested Calls
\MakeRobust{\Call}

% the required height and depth are set by measuring the content to be shown
% this means that the content is processed twice
\newbox\statebox
\newcommand{\myState}[1]{%
    \setbox\statebox=\vbox{#1}%
    \edef\thealgruleheight{\dimexpr \the\ht\statebox+1pt\relax}%
    \edef\thealgruledepth{\dimexpr \the\dp\statebox+1pt\relax}%
    \ifdim\thealgruleheight<.75\baselineskip
        \def\thealgruleheight{\dimexpr .75\baselineskip+1pt\relax}%
    \fi
    \ifdim\thealgruledepth<.25\baselineskip
        \def\thealgruledepth{\dimexpr .25\baselineskip+1pt\relax}%
    \fi
    %\showboxdepth=100
    %\showboxbreadth=100
    %\showbox\statebox
    \State #1%
    %\State \usebox\statebox
    %\State \unvbox\statebox
    %reset in case the next command is not wrapped in \myState
    \def\thealgruleheight{\dimexpr .75\baselineskip+1pt\relax}%
    \def\thealgruledepth{\dimexpr .25\baselineskip+1pt\relax}%
}
% end vertical rule patch for algorithmicx
 %%%%% PANCAKE
\usepackage{listings}
\newcommand{\passthrough}[1]{#1}
\lstset{defaultdialect=[5.3]Lua}
\lstset{defaultdialect=[x86masm]Assembler}
%%%%% PANCAKE
\lstset{
    language=java,
    keywords={pre, eff, method, Action},
    numbers=left,
    stepnumber=1,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    keepspaces=true,
    showtabs=true,
    tabsize=1,
    extendedchars=true,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=true,
    breakautoindent=true,
%    framexrightmargin=-2mm,
    xleftmargin=5mm,
    framexleftmargin=5mm} %%%%% PANCAKE

 %%%%% PANCAKE

%%%% For listings (code blocks)
\lstset{
    basicstyle={\color{solarized-base02}\footnotesize\ttfamily},
    backgroundcolor=\color{solarized-base3},
    keywordstyle=\color{solarized-orange}\bfseries,
    identifierstyle=\color{solarized-base01},
    stringstyle=\color{solarized-green},
    commentstyle=\color{solarized-base1}\itshape,
    numberstyle=\footnotesize\color{solarized-base0}
}

 %%%%% PANCAKE
 %%%%% PANCAKE
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{tfrupee}
\usepackage{stmaryrd}
\DeclareFontFamily{U}{mathb}{\hyphenchar\font45}
\DeclareFontShape{U}{mathb}{m}{n}{<5> <6> <7> <8> <9> <10> gen * mathb <10.95> mathb10 <12> <14.4> <17.28> <20.74> <24.88> mathb12}{}
\DeclareSymbolFont{mathb}{U}{mathb}{m}{n}
\DeclareMathSymbol{\bigvarstar}{2}{mathb}{"0F}
\DeclareFontFamily{U}{FdSymbolC}{}
\DeclareFontShape{U}{FdSymbolC}{m}{n}{<-> s * FdSymbolC-Book}{}
\DeclareSymbolFont{fdarrows}{U}{FdSymbolC}{m}{n}
\DeclareMathSymbol{\leftblackspoon}{2}{fdarrows}{"6E}
\DeclareMathSymbol{\leftrightspoon}{2}{fdarrows}{"70}
\newfontfamily\arcfont[]{Arc}
\newfontfamily\lirisfont[]{LIRIS}
\newfontfamily\dinfont[]{DIN}
\newfontfamily\graphikregfont[]{Graphik LC Web Regular}
\newfontfamily\graphikmedfont[]{Graphik LC Web Medium}
\newfontfamily\lyondfont[]{Lyon2}
\newfontfamily\perihelionbbfont[]{PerihelionBB-Bold}
\newfontfamily\disp[]{Aller Display}
\newcommand{\arc}{\textcolor{solarized-violet}{\vspace{0pt}\arcfont arc 2}~\includegraphics[height=2.8mm]{logos/arc2_pastille.pdf} }
\newcommand{\liris}{\textcolor{solarized-base02}{\lirisfont LỊRIS} }
\newcommand{\sma}{\textcolor{solarized-base02}{\disp sma}~\vspace{0pt}\includegraphics[height=2.8mm]{logos/sma_pastille.pdf} }
\newcommand{\polytech}{\vspace{0pt}\includegraphics[height=2.8mm]{logos/polytech_pastille.pdf}~\textcolor{solarized-base03}{\perihelionbbfont POLYTECH} }
\newcommand{\ucbl}{\vspace{0pt}\includegraphics[height=4mm]{logos/lyon1_pastille.pdf}{\dinfont~Lyon 1} }
\newcommand{\ara}{{\graphikregfont Auvergne-Rhône-Alpes}~\vspace{0pt}\includegraphics[height=2.6mm]{logos/ara_pastille.pdf} }
\newcommand{\lumiere}{\textcolor{solarized-red}{\lyondfont lumière lyon 2} }
\newcommand\colonvdash{\mathrel{\ooalign{\hss$\vdash$\hss\cr\kern0.6ex\raise0.15ex\hbox{\scalebox{1}{$:$}}}}}
\newcommand{\ingoing}{\mathrel{\raisebox{.01em}{\rotatebox[origin=c]{90}{$\scriptstyle\pmb\curlyveedownarrow$}}}}
\newcommand{\outgoing}{\mathrel{\raisebox{.01em}{\rotatebox[origin=c]{270}{$\scriptstyle\pmb\curlyveeuparrow$}}}}
\usepackage{marginnote}
\newcounter{mynote}
\newcommand{\mynote}[1]{ \refstepcounter{mynote} \mbox{\textsuperscript{\themynote}} \marginnote{\mbox{\textsuperscript{\footnotesize{\themynote}}}#1}}
\renewcommand*{\footnote}{\marginnote}
\usepackage[ bottom = 3cm,]{geometry}
\newcommand{\bb}{\mathbb}
\renewcommand{\cal}{\mathcal}
\newcommand{\rrp}{\rrparenthesis}
\newcommand{\rrbp}{\rblkbrbrak}
\newcommand{\llp}{\llparenthesis}
\newcommand{\llbp}{\lblkbrbrak}
\newcommand{\none}{\gtrdot}
\newcommand{\all}{\lessdot}
\newcommand{\comb}{\bowtie}
\newcommand{\super}{\vartriangle}
\newcommand{\sub}{\triangledown}
\newcommand{\comp}{\circ}
\newcommand{\inv}{\bullet}
\newcommand{\inci}{\mathrel{-\!\!\!\!\bullet\!\!\!\!-}}
\newcommand{\adja}{\leftrightspoon}
\newcommand{\ingo}{\mathrel{\raisebox{.01em}{\rotatebox[origin=c]{90}{$\scriptstyle\pmb\curlyveedownarrow$}}}\!\bullet}
\newcommand{\outgo}{\bullet\!\mathrel{\raisebox{.01em}{\rotatebox[origin=c]{270}{$\scriptstyle\pmb\curlyveeuparrow$}}}}
\newcommand{\univ}{\mathbb{U}}
\newcommand{\proba}{\mathbb{P}}
\newcommand{\obs}{\mathcal{O}}
\newcommand{\dom}{\mathcal{D}}
\newcommand{\pb}{\mathbb{p}}
\newcommand{\imply}{\vdash}
\newcommand{\powerset}{\wp}
\newcommand{\landor}{\veeonwedge}
\newcommand{\seed}{\varstar}
\newcommand{\sheaf}{\mathcal{F}}
\newcommand{\state}{\smwhtsquare}
\newcommand{\states}{\square}
\newcommand{\cstate}{\boxonbox}
\newcommand{\plan}{\mathbb{\pi}}
\newcommand{\plans}{\mathbb{\Pi}}
\newcommand{\search}{\mathbb{s}}
\newcommand{\searches}{\mathbb{S}}
\newcommand{\pre}{\mathit{pre}}
\newcommand{\eff}{\mathit{eff}}
\newcommand{\reward}{\text{\rupee}}
\newcommand{\cost}{¢}

\title{Endomorphic metalanguage and abstract planning for real-time intent
recognition}
\author{Antoine Gréa}
\date{}

\ifdefined\and %%%%% PANCAKE
\else
  \newcommand{\and}{\quad}
\fi %%%%% PANCAKE
\begin{document}
\maketitle
 %%%%% PANCAKE
\begin{abstract}
\chapter*{Abstract}

Human-machine interaction is among the most complex problems in the
field of artificial intelligence. Indeed, software that cooperate with
dependent people must have contradictory qualities such as speed and
expressiveness, or even precision and generality. This requires a
radical paradigm shift from standard approaches, which, while effective,
are often rigid. The objective is then to design models and mecanisms
capable of generating intermediate solutions that sometimes get useful
for certain problems. These solutions make it possible to expand the
possibilities of adaptation in a fluid and continuous way. With this
idea, the role of explainability in the resolution process is a decisive
criterion for obtaining flexible systems. Thus, the search for a
complete and optimal response has overshadowed the usefulness of these
models.

\par

Mathematics offers the best tools for formalization. However, it appears
that the foundation of mathematics is defined by an implicit
circularity. This defect reveals a lack of rigor in describing the basic
concepts used in mathematics. It is therefore sufficient to recognize
the need for a formalism based on a precursor language. In the absence
of alternatives, natural language is a precursor to a new formalism.
This is inspired by lambda calculus an is derived from category theory.
From basic concepts, defined by explicit circularity, it is then
possible to reconstruct classical mathematical concepts as well as other
tools and structures useful what is following.

\par

By using this formalism, it becomes possible to axiomatize an
endomorphic metalanguage. This one manipulates a dynamic grammar capable
of adjusting its semantics to exploitation. The recognition of basic
structures allows this language to avoid using keywords. This, combined
with a new model of knowledge representation, supports the construction
of an expressive knowledge description model.

\par

With this language and this formalism, it becomes possible to create
frameworks in fields that were previously heterogeneous. For example, in
automatic planning, the classic state-based model makes it impossible to
unify the representation of planning domains. This results in a general
planning framework that allows all types of planning domains to be
expressed.

\par

Concrete algorithms are then created that show the principle of
intermediate solutions. Two new approaches to real-time planning are
developed and evaluated. The first is based on a usefulness heuristic of
planning operators to repair existing plans. The second uses
hierarchical task networks to generate valid plans that are abstract and
intermediate solutions. These plans allow for a shorter execution time
for any use that does not require a detailed plan.

\par

We then illustrate the use of these plans on intent recognition by
reverse planning. Indeed, in this field, the fact that no plan libraries
are required makes it easier to design recognition models. By exploiting
abstract plans, it becomes possible to create a system theoretically
more efficient than those using complete planning.

\chapter*{Résumé}

L'interaction personne-machine fait partie des problèmes les plus
complexes dans le domaine de l'intelligence artificielle. En effet, les
logiciels qui coopèrent avec des personnes dépendantes doivent avoir des
qualités contradictoires telles que la rapidité et l'expressivité, voire
la précision et la généralité. Cela exige un changement radical de
paradigme par rapport aux approches standards, qui s'avèrent souvent
efficaces, mais rigides. On a alors, pour objectif, la conception
d'outils capables de générer des solutions intermédiaires qui ont
parfois une utilité pour certaines problématiques. Ces solutions
permettent d'élargir les possibilités d'adaptation de manière fluide et
continue. Dans l'idée, le rôle de l'explicabilité dans le processus de
résolution constitue un critère déterminant pour obtenir des systèmes
flexibles. Aussi, la recherche d'une réponse complète et optimale n'a
que trop occulté l'utilité de ces modèles.

\par

Les mathématiques offrent les meilleurs outils de formalisation.
Cependant, il s'avère que la fondation des mathématiques est définie par
une circularité implicite. Ce défaut agit comme un révélateur d'un
manque de rigueur pour décrire des notions de base utilisées en
mathématique. Il suffit dès lors de reconnaitre la nécessité d'un
formalisme établie sur un langage précurseur. À défaut d'alternatives,
le langage naturel fait office de précurseur pour un nouveau formalisme.
Celui-ci est inspiré par le lambda-calcul et dérivé de la théorie des
catégories. À partir de concepts de base, définis par circularité
explicite, il est alors réalisable de reconstruire les notions
mathématiques classiques ainsi que d'autres outils et structures très
utiles pour la suite.

\par

En se servant de ce formalisme, il devient possible d'axiomatiser un
métalangage endomorphique. Celui-ci manipule une grammaire dynamique
capable d'ajuster sa sémantique à l'usage. La reconnaissance des
structures de base permet à ce langage de ne pas utiliser de mot-clés.
Ceci, combiné à un nouveau modèle de représentation des connaissances,
supporte la construction d'un modèle de représentation des connaissances
expressive.

\par

Avec ce langage et ce formalisme, il devient envisageable de créer des
cadriciels dans des champs jusqu'alors hétéroclites. Par exemple, en
planification automatique, le modèle classique à état rend l'unification
de la représentation des domaines de planification impossible. Il en
résulte un cadriciel général de la planification permettant d'exprimer
tout type de domaines en vigueur.

\par

On crée alors des algorithmes concrets qui montrent le principe des
solutions intermédiaires. Deux nouvelles approches à la planification en
temps réel sont présentées et évaluées. La première se base sur une
euristique d'utilité des opérateurs de planification afin de réparer des
plans existants. La seconde utilise la planification hiérarchique pour
générer des plans valides qui sont des solutions abstraites et
intermédiaires. Ces plans rendent possible un temps d'exécution plus
court pour tout usage ne nécessitant pas le plan détaillé.

\par

On illustre alors l'usage de ces plans sur la reconnaissance d'intention
par planification inversée. En effet, dans ce domaine, le fait de ne pas
nécessiter de bibliothèques de plans rend la création de modèles de
récognition plus aisée. En exploitant les plans abstraits, il devient
possible de réaliser un système théoriquement plus performant que ceux
utilisant de la planification complète.
\end{abstract}

 %%%%% PANCAKE

\renewcommand*\contentsname{Table of Content}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{4}
\tableofcontents
}
\listoffigures
\hypertarget{acknowledgements}{%
\chapter*{\texorpdfstring{Acknowledgements
\protect\includegraphics[width=\textwidth,height=0.16667in]{logos/uk.svg}}{Acknowledgements }}\label{acknowledgements}}
\addcontentsline{toc}{chapter}{Acknowledgements }

Ehlo, this is the author of this thesis. I would like to thank a few
people without whom none of this would have been possible.

It was a beautiful morning of June\footnote{\includegraphics{portraits/nature.jpg}
  Actual picture of the time} that I first walked the Doua wooded
driveways to meet my supervisor team: Samir Aknine and Laetitia
Matignon. They have given me their trust in the success of this thesis
project. Without their advice and support none of this would have been
possible. I would like to thank Samir in particular whom, as a mentor
and friend, has listened to me and supported me even in the most
difficult moments with compassion and understanding.

I would also like to thank Christine Gertosio, who offered me unfailing
support in my teaching department and granted me the opportunity to
pursue an ATER offer at \polytech Lyon. It was a real pleasure to teach
classes of passionate and enthusiastic students.

I would also like to offer my gratitude to the researchers who helped me
in this adventure, in no particular order: Fabrice Jumel, Jacques
Saraydaryan, Shirin Sohrabi, Damien Pelier. I would also like to thank
Linas Vepstas, who greatly helped me in the mathematical part of this
thesis.

Without funding, this work wouldn't have been possible. I would like to
thank the \arc (Academic Research Community) research allocation
courtesy of the \ara region in France. This organism is acting toward an
improvement of the quality of life and of aging.

This work was done with the \ucbl university in the \liris (Laboratory
of computeR science in Image and Information Systems). I was part of
vibrant and wonderful \sma team (Multi-Agent System). Part of the work
was funded by teaching at the university \lumiere and the
\polytech engineering school.

Finally, I would like to thank my family for supporting me despite my
periods of avolition. I would like to thank in particular my mother
Charlotte and my sister Sarah\footnote{\includegraphics{portraits/sarah_grea.jpg}
  Sarah Gréa helping me do the
  \href{https://en.wikipedia.org/wiki/Rubber_duck_debugging}{rubber duck
  method}.} for having done their best to keep me company and pretend to
listen to me talk about the technicalities of my thesis in order to make
me progress.

Thank you again.

\newpage

\hypertarget{remerciements}{%
\chapter*{\texorpdfstring{Remerciements
\protect\includegraphics[width=\textwidth,height=0.16667in]{logos/fr.svg}}{Remerciements }}\label{remerciements}}
\addcontentsline{toc}{chapter}{Remerciements }

Je voudrais remercier quelques personnes sans qui rien de cela n'aurait
été possible.

C'était une belle matinée de juin\footnote{\includegraphics{portraits/nature.jpg}
  Photo de l'époque} que j'ai parcouru pour la première fois les allées
boisées de la Doua afin de rencontrer mon équipe encadrante: Samir
Aknine et Laetitia Matignon. Ces derniers m'ont accordé leur confiance
dans la réussite de ce projet de thèse. Sans leur conseil et soutien
rien de cela n'aurait été possible. Je remercie particulièrement Samir,
qui en tant qu'encadrant, et ami, a su m'écouter et me soutenir même
dans les moments les plus pénibles avec compassion et compréhension.

Je souhaite également remercier Christine Gertosio, qui m'a offert une
aide sans faille dans mon service d'enseignement et qui m'a donné
l'opportunité d'une offre d'ATER à \polytech Lyon. C'était un véritable
plaisir de donner des cours à des promotions d'élèves passionnés et
enthousiastes.

Je souhaite également offrir ma gratitude aux chercheurs qui m'ont aidé
dans cette aventure, sans ordre particulier: Fabrice Jumel, Jacques
Saraydaryan, Shirin Sohrabi, Damien Pelier. J'aimerais également
remercier Linas Vepstas, qui m'a grandement aidé dans la partie
mathématique de cette thèse.

Sans financement, ce travail n'aurait pas été possible. Je tiens à
remercier l'allocation de recherche du \arc (Academic Research
Community) gracieuseté de la région de \ara en France. Cet organisme
agit pour l'amélioration de la qualité de vie et du vieillissement.

Ce travail a été réalisé avec l'université \ucbl dans le
\liris (Laboratoire de calcul scientifique en Image et Systèmes
d'Information). Je faisais partie d'une équipe dynamique et conviviale
\sma (Multi-Agent System). Une partie du travail a été financée par
l'enseignement à l'université \lumiere et à l'école d'ingénieurs
polytech.

Enfin, je tiens à remercier ma famille de m'avoir supporté malgré mes
périodes d'avolition. Je voudrais remercier tout particulièrement ma
mère Charlotte et ma sœur Sarah\footnote{\includegraphics{portraits/sarah_grea.jpg}
  Sarah Gréa m'aidant en utilisant la
  \href{https://fr.wikipedia.org/wiki/M\%C3\%A9thode_du_canard_en_plastique}{méthode
  du canard en plastique}.} pour avoir fait de leurs mieux pour me tenir
compagnie et faire semblant de m'écouter déblatérer des technicités de
ma thèse afin de me faire avancer.

Encore merci.

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

In this section we present a quick guide to the presentation of the
information in this document. This will give its global structure and
each of the type of formatting and its meaning.

\hypertarget{text-format}{%
\section*{Text format}\label{text-format}}
\addcontentsline{toc}{section}{Text format}

The text can be emphasized \textbf{more} or \emph{less} to make a key
word more noticeable.

\hypertarget{citations}{%
\section*{Citations}\label{citations}}
\addcontentsline{toc}{section}{Citations}

In text citations will be in this format: Author \emph{et al.}
(\href{https://citationstyles.org/}{year}) to make the author part of
the text and (Author \emph{et al.}
\href{https://citationstyles.org/}{year}) when simply referencing the
work.

\hypertarget{quotes}{%
\section*{Quotes}\label{quotes}}
\addcontentsline{toc}{section}{Quotes}

Sometimes, important quotes needs emphasis. They are presented as:

\begin{quote}
\emph{``Don't quote me on that !''}\footnote{Gréa
  (\href{antoine.grea.me}{2019})}
\end{quote}

\hypertarget{ch:introuction}{%
\chapter{Introduction}\label{ch:introuction}}

In antiquity, philosophy, mathematics and logic were considered as a
single discipline. Since Aristotle we have realized that the world is
not just black and white but full of nuances and colours. The
inspiration for this thesis comes from one of the most influential
philosophers and scientists of his time: Alfred Korzibsky. He founded a
discipline he called \emph{general semantics} to deal with problems of
knowledge representation in humans. Korzibsky then found that complete
knowledge of reality being inaccessible, we had to abstract. This
abstraction is then only similar to reality in its structure. In these
pioneering works, we find notions similar to that of modern descriptive
languages.

It is from this inspiration that this document is built. We then start,
off the beaten track and away from computer science by a brief excursion
into the world of mathematical and logical formalisms. This makes it
possible to formalize a language that allows to describe itself
partially by structure and that evolves with its use. The rest of the
work illustrates the possible applications through specific fields such
as automatic planning and intention recognition.

\hypertarget{motivations}{%
\section{Motivations}\label{motivations}}

The social skills of modern robots are rather poor. Often, it is that
lack that inhibits human-robot communication and cooperation. Humans
being a social species, they require the use of implicit social cues in
order to interact comfortably with an interlocutor.

In order to enhance assistance to dependent people, we need to account
for any deficiency they might have. The main issue is that the patient
is unable or unwilling to express their needs. That is a problem even
with human caregivers as the information about the patient's intents
needs to be inferred from their past actions.

This aspect of social communications often eludes the understanding of
Artificial Intelligence (AI) systems. This is the reason why intent
recogniton is such a complicated problem. The primary goal of this
thesis is to address this issue and create the formal foundations of a
system able to help dependent people.

\hypertarget{problem}{%
\section{Problem}\label{problem}}

First, what exactly is intent recogntion ? The problem is simple to
express: finding out what other agents want to do before they do. It is
important to distinguish between several notions. \emph{Plans} are the
sequence of actions that the agent is doing to achieve a \emph{goal}.
This goal is a concrete explanation of the wanted result. However, the
\emph{intent} is more of a set of abstract goals, some of which may be
vague or impossible (e.g.~drink something, survive forever, etc.).

Some approaches use trivial machine learning methods, along with a hand
made plan library to match observations to their most likely plan using
statistics. The issue with these common approaches is that they require
an extensive amount of training data and need to be trained on each
agent. This makes the practicality of such system quite limited. To
address this issue, some works proposed hybrid approaches using logical
constraints on probabilistic methods. These constraints are made to
guide the resolution toward a more coherent solution. However, all
probabilistic methods require an existing plan library that can be quite
expensive to create. Also, plan libraries cannot take into account
unforseen or unlikely plans.

A work from Ramırez and Geffner
(\protect\hyperlink{ref-ramirez_plan_2009}{2009}), added an interesting
method to solve this issue. Indeed, they noticed an interesting parallel
between that problem with the field of automated planning. This analogy
was made by using the Theory of mind Baker \emph{et al.}
(\protect\hyperlink{ref-baker_bayesian_2011}{2011}), which states that
any agent will infer the intent of other agents using a projection of
their own expectations on the observed behaviors of the other
agents.\footnote{\includegraphics{graphics/planning_vs_ir.svg}}

This made the use of planning techniques possible to infer intents
without the need for extensive and well-crafted plan libraries. Now only
the domain of the possible actions, their effects and prerequisites are
needed to infer the logical intent of an agent.

The main issue of planning for that particular use is computation time
and search space size. This prevents most planners to make any decision
before the intent is already realized and therefore being useless for
assistance. This time constraint leads to the search of a real-time
planner algorithm that is also expressive and flexible.

\hypertarget{contributions}{%
\section{Contributions}\label{contributions}}

In order to achieve such a planner, the first step was to formalize what
is exactly needed to express a domain. Hierarchical and partially
ordered plans gave the most expressivity and flexibility but at the cost
of time and performance. This is why, a new formalism of knowledge
representation was needed in order to increase the speed of the search
space exploration while restricting it using semantic inference rules.

While searching for a knowledge representation model, we developped some
prototypes using standard ontology tools but all proved to be too slow
and inexpressive for that application. This made the design of a lighter
but more flexible knowledge representation model, a requirement for
planning domain representation.

Then the planning formalism has to be encoded using our general
knowledge representation tool. Since automated planning has a very
diverse ecosystem of approaches and paradigms, its standard, the
Planning Domain Description Language (PDDL) needs use of various
extensions. However, no general formalism has been given for PDDL and
some approaches often lack proper extensions (hierarchical planning,
plan representation, etc). This is why a new formalism is proposed and
compared to the one used as standard of the planning community.

Then finally, a couple of planners were designed to attempt answering
the speed and flexibility requirements of human intent recognition. The
first one is a prototype that aims to evaluate the advantages of
repairing plans using several heuristics. The second is a more complete
prototype derived from the first (without plan repair), which also
implements a Breadth-First Search (BFS) approach to hierarchical
decomposition of composite actions. This allows the planner to provide
intermediary plans that, while incomplete, are an abstraction of the
result plans. This allows for anytime intent recognition probability
computation using existing techniques of inverted planning.

\hypertarget{plan}{%
\section{Plan}\label{plan}}

In this document we will describe a few contributions from the new
mathematical formalism to intent recognition. Each chapter builds on the
previous one.

First we will present a new fondation of mathematics, highlighting the
flaws of the current ones. This fondation is used to create a formalism
capable of describing all the classical mathematics.

In the third chapter, a new knowledge description system is presented as
well as the associated grammar and inference framework.

The fourth chapter is an application of that knowledge description
system to automated planning. This allows us to design a general
planning framework that can express any existing type of domain.
Existing formalism and languages are compared to our proposed approach.

Using this framework, two online planning algorithms are presented in
chapter five: one that uses repairs on existing plans and one that uses
hierarchical domains to create intermediary abstract plans.

The final chapter is about intent recognition and its link to planning.
Existing works are presented as well as a technique called
\emph{inverted planning}.

\hypertarget{ch:fondation}{%
\chapter{Foundation and Tools}\label{ch:fondation}}

\footnote{\includegraphics{portraits/alfred_korzybski.jpg} (Korzybski
  \protect\hyperlink{ref-korzybski_science_1933}{1933}, ch.~4 pp.~58)}

\begin{quote}
\emph{``A map \emph{is not} the territory it represents, but, if
correct, it has a \emph{similar structure} to the territory, which
accounts for its usefulness.''}
\end{quote}

Mathematics and logic are at the heart of all formal sciences, including
computer science. The boundary between mathematics and computer science
is quite blurry. Indeed, computer science is applied mathematics and
mathematics are abstract computer science. Both cannot be separated when
needing a formal description of a new model.

In mathematics, a fondation is a axiomatic theory that is consistent and
well defined. For the fondation to be generative of all mathematics it
only need to either define all basic notions of mathematics or simply to
define another existing fondation.

Since we need to use existing mathematical structures that are beyond
the expressivity of the classical mathematics, we need a non-classical
fondation. There are several existing ones but since we only use a
subset of their possibilities, it is more efficient to create a simpler
fondation.

In this chapter, we define a new formalism as well as a proposed
fondation that lays on the bases of type theory and lambda calculus.
From this formalism we define the classical set theory (which is the
most commonly used fondation).\footnote{\includegraphics{portraits/georg_cantor.jpg}
  Cantor (\protect\hyperlink{ref-cantor_property_1874}{1874})} The
contribution is mainly in the axiomatic system and functional formalism.
The rest is simply an explanation, using our formalism, of existing
mathematical domains and structures commonly use in computer science.
\textbf{This formalism is used for all the contributions later on this
document.}

\hypertarget{issues-with-some-existing-fondation}{%
\section{Issues with some existing
fondation}\label{issues-with-some-existing-fondation}}

Some of the most important problems in mathematics are the consistency
and formalization issues. Research on these issues starts at the end of
the 19\textsuperscript{th} century, with Cantor inventing set theory.
Then after a crisis in the beginning of the 20\textsuperscript{th}
century with Russel's paradox and Gödel's incompletude theorem, revised
versions of the set theory become one of the foundations of mathematics.
The most accepted version is the Zermelo-Fraenkel axiomatic set theory
with the axiom of Choice (ZFC).\footnote{Fraenkel \emph{et al.}
  (\protect\hyperlink{ref-fraenkel_foundations_1973}{1973}, vol. 67);
  Ciesielski (\protect\hyperlink{ref-ciesielski_set_1997}{1997})} This
effort leads to a formalization of mathematics itself, at least to a
certain degree.

Any knowledge must be expressed using an encoding support (medium) like
a language. Natural languages are quite expressive and allow for complex
abstract ideas to be communicated between individuals. However, in
science we encounter the first issues with such a language. It is
culturally biased and improperly convey formal notions and proof
constructs. Indeed, natural languages are not meant to be used for
rigorous mathematical proofs. This is one of the main conclusions of the
works of Korzybski
(\protect\hyperlink{ref-korzybski_science_1958}{1958}) on ``general
semantics''. The original goal of Korzybski was to pinpoint the errors
that led humans to fight each other in World War 1. He affirmed that the
language is unadapted to convey information reliably about objective
facts or scientific notions. There is a discrepancy between the natural
language and the underlying structure of the reality. This issue is
exacerbated in mathematics as the ambiguity in the definition of a term
can be the cause for a contradiction and make the entire theory
inconsistent.

\begin{quote}
\emph{``Mathematics may be defined as the subject in which we never know
what we are talking about, nor whether what we are saying is
true.''}\footnote{\includegraphics{portraits/bertrand_russel.jpg}
  Russell (\protect\hyperlink{ref-russell_mysticism_1917}{1917})}
\end{quote}

Mathematics are meant to be above those definition issues. Indeed, most
of mathematics are defined with a well formed fondation and their
validity has been tested over decades if not centuries.

In this part we expose some issues about the way mathematics are
formulated in the original theories and how it may be possible to
improve the way notions are defined to lift more ambiguity. This is done
by explaining the properties of laguages and their ability to formalize
other languages or even themselves. We expose the issues to be able to
correctly identify and isolate them once we do a formalism of our own.
This formalism will allow us to define precisely a new endomorphic
meta-language for knowledge description that is used for encoding
planning domains (see chapter~\ref{ch:self} and chapter~\ref{ch:color}).

\hypertarget{abstraction}{%
\subsection{Abstraction}\label{abstraction}}

\begin{quote}
\textbf{abstraction} (n.d.): \emph{The process of formulating
generalized ideas or concepts by extracting common qualities from
specific examples}\footnote{Collins English Dictionary
  (\protect\hyperlink{ref-collinsenglishdictionary_abstraction_2014}{2014})}
\end{quote}

The idea behind abstraction is to simplify the representation of complex
instances. This mechanism is at the base of any knowledge representation
system. Indeed, it is unnecessarily expensive to try to represent all
properties of an object. An efficient way to reduce that knowledge
representation is to prune away all irrelevant properties while also
only keeping the ones that will be used in the context. \emph{This means
that abstraction is a loosy process} since information is lost when
abstracting from an object.

Since this is done using a language as a medium, this language is a
\emph{host language}. Abstraction will refer to an instance using a
\emph{term} (also called symbol) of the host language. If the host
language is expressive enough, it is possible to do abstraction on an
object that is already abstract. The number of layers abstraction needed
for a term is called its \emph{abstraction level}. Very general notions
have a higher abstraction level and we represent reality using the null
abstraction level. In practice abstraction uses terms of the host
language to bind to a referenced instance in a lower abstraction level.
This forms a structure that is strongly hierarchical with higher
abstraction level terms on top.

\begin{example}

We can describe an individual organism with a name that is associated to
this specific individual. If we name a dog ``Rex'' we abstract a lot of
information about a complex, dynamic living being. We can also abstract
from a set of qualities of the specimen to build higher abstraction. For
example, its species would be \emph{Canis lupus familiaris} from the
\emph{Canidae} family. Sometimes several terms can be used at the same
abstraction level like the commonly used denomination ``dog'' in this
case. \marginpar{%
\includegraphics{/tmp/tmprrfa3vwo}

}

\end{example}

Terms are only a part of that structure. It is possible to combine
several terms into a \emph{formula} (also called proposition, expression
or statement).

\hypertarget{formalization}{%
\subsection{Formalization}\label{formalization}}

\begin{quote}
\textbf{formal} (adj.): \emph{Relating to or involving outward form or
structure, often in contrast to content or meaning.}\footnote{American
  Heritage Dictionary
  (\protect\hyperlink{ref-americanheritagedictionary_formal_2011}{2011}\protect\hyperlink{ref-americanheritagedictionary_formal_2011}{a})}
\end{quote}

A formalization is the act to make formal. The word ``formal'' comes
from Latin \emph{fōrmālis}, from \emph{fōrma}, meaning form, shape or
structure. This is the same base as for the word ``formula''. In
mathematics and \emph{formal sciences} the act of formalization is to
reduce knowledge down to formulas. Like stated previously, a formula
combines several terms. But a formula must follow rules at different
levels:

\begin{itemize}
\tightlist
\item
  \emph{Lexical} by using terms belonging in the host language.
\item
  \emph{Syntactic} as it must follow the grammar of the host language.
\item
  \emph{Semantic} as it must be internally consistent and meaningful.
\end{itemize}

The information conveyed from a formula can be reduced to one element:
its semantic structure. Like its etymology suggests, a formula is simply
a structured statement about terms. This structure holds its meaning.
Along with using abstraction, it becomes possible to abstract a formula
and therefore, to make a formula about other formulae should the host
language allowing it.

\begin{example}

The formula using English ``dog is man's best friend'' combines terms to
hold a structure between words. It is lexically correct since it uses
English words and grammatically correct since it can be grammatically
decomposed as (n.~v. n.~p.~adj. n.). In that the (n.) stands for nouns
(v.) for verbs (adj.) for adjectives and (p.) for possessives. Since the
verb ``is'' is the third person singular present indicative of ``be'',
and the adjective is the superlative of ``good'', this form is correct
in the English language. From there the semantic aspect is correct too
but that is too subjective and extensive to formalize here. We can also
build a formula about a formula like ``this is a common phrase'' using
the referential pronoun ``this'' to refer to the previous formula.

\end{example}

Any language is comprised of formulas. Each formula holds knowledge
about their subject and states facts or belief. A formula can describe
other formulas and even \emph{define} them. However, there is a strong
limitation of a formalization. Indeed, a complete formalization cannot
occur about the host language. It is possible to express formulae about
the host language but \emph{it is impossible to completely describe the
host language using itself} (Klein
\protect\hyperlink{ref-klein_metacompiling_1975}{1975}). This comes from
two principal reasons. As abstraction is a loosy process one cannot
completely describe a language while abstracting its definition. If the
language is complex enough, its description requires an even more
complex \emph{meta-language} to describe it. And even for simpler
language, the issue stands still while making it harder to express
knowledge about the language itself. For this we need knowledge of the
language \emph{a priori} and this is contradictory for a definition and
therefore impossible to achieve.

When abstracting a term, it may be useful to add information about the
term to define it properly. That is why most formal system requires a
\emph{definition} of each term using a formula. This definition is the
main piece of semantic information on a term and is used when needing to
evaluate a term in a different abstraction level. However, this is
causing yet another problem.

\hypertarget{circularity}{%
\subsection{Circularity}\label{circularity}}

\begin{quote}
\textbf{circularity} (n.d.): \emph{Defining one word in terms of another
that is itself defined in terms of the first word.}\footnote{American
  Heritage Dictionary
  (\protect\hyperlink{ref-americanheritagedictionary_circularity_2011}{2011}\protect\hyperlink{ref-americanheritagedictionary_circularity_2011}{b})}
\end{quote}

Circularity is one of the issues we explore in this section about the
limits of formalization languages. Indeed, defining a term requires
using a formula in the host language to express the abstracted
properties of the generalization (Korzybski
\protect\hyperlink{ref-korzybski_science_1933}{1933}). The problem is
that most terms will have \emph{circular} definitions.

\begin{example}

Using definitions from the American Heritage Dictionary
(\protect\hyperlink{ref-americanheritagedictionary_circularity_2011}{2011}\protect\hyperlink{ref-americanheritagedictionary_circularity_2011}{b}),
we can find that the term ``word'' is defined using the word ``meaning''
that is in turn defined using the term ``word''. Such circularity can
happen to use an arbitrarily long chain of definition that will form a
cycle in the dependencies.

\end{example}

This problem is very important as it is overlooked in most foundations
of mathematics. Since a formalization cannot fully be self defined,
another host language is generally used, sometimes without being
acknowledged. This causes cycles in the dependencies of languages and
theories in mathematics.

The only practical way to make some of this circularity disappear is to
base a foundation of mathematics using natural language as host language
for defining the most basic terms. This allows to acknowledge the
problem in an instinctive way while being aware of it while building the
theory.

\hypertarget{functional-theory-of-mathematics}{%
\section{Functional theory of
mathematics}\label{functional-theory-of-mathematics}}

We aim to reduce the set of axioms allowing to describe a foundation of
mathematics. The following theory is a proposition for a possible
foundation that takes into account the previously described constraints.
It is inspired by category theory (Awodey
\protect\hyperlink{ref-awodey_category_2010}{2010}), and typed lambda
calculus (Barendregt
\protect\hyperlink{ref-barendregt_lambda_1984}{1984}).

\hypertarget{category-theory}{%
\subsection{Category theory}\label{category-theory}}

This theory is based, as its name implies, on \emph{categories}. A
category is a mathematical structure that consist in two components:

\begin{itemize}
\tightlist
\item
  A set of \textbf{ojects} that can bee any arbitrary mathematical
  entities.
\item
  A set of \textbf{morphism} that are functional monomes. They are often
  represented as \emph{arrows}.
\end{itemize}

Many definitions of categories exists (Barr and Wells
\protect\hyperlink{ref-barr_category_1990}{1990}, vol. 49) but they are
all in essence similar to this explanation. The best way to see the
category theory is as a general theory of functions. Even if we can use
any mathematical entity for the types of the components, the structure
heavily implies a functional connotation.

\hypertarget{axioms}{%
\subsection{Axioms}\label{axioms}}

In this part, as an introduction to the fundamentals of maths and logic,
we propose another view of that foundation based on functions. The
unique advantage of it lays in its explicit structure that have emergent
reflexive properties. It also holds a coherent algebra that has a strong
expressive power. This approach can b described as a special case of
category theory. However, it differs in its priorities and formulation.
For example, since our goal is to build a fondation of mathematics, it
is impossible to fully specify the domain or co-domain of the functions
and they are therefore weakly specified (Godel and Brown
\protect\hyperlink{ref-godel_consistency_1940}{1940}).

Our theory is axiomatic, meaning it is based on fundamental logical
proposition called axioms. Those forms the base of the logical system
and therefore are accepted without any need for proof. In a nutshell,
axiomes are true prior hypotheses.

The following axioms are the explicit base of the formalism. It is
mandatory to properly state those axioms as all the theory is built on
top of it.

\begin{axiom}[Identity]\label{axi:identity}

Let's the identity function be \(=\) that associates every functions to
itself.\footnote{\((=) = x \to x\)} This function is therefore
transparent as by definition \(=(x)\) is the same as \(x\). It can be
described by using it as \textbf{affectation} or aliases to make some
expressions shorter or to define any function.

\end{axiom}

In the rest of the document, classical equality and the identity
function will refer to the same notion.

That axiom implies that the formalism is based on \emph{functions}. Any
function can be used in any way as long as it has a single argument and
returns only one value at a time (named image, which is also a
function).

It is important to know that \textbf{everything} in our formalism
\emph{is} a function. Even notions such as literals, variables or set
from classical mathematics are functions.

\begin{axiom}[Association]\label{axi:association}

\footnote{Using definition~\ref{def:currying} and
  definition~\ref{def:application}, we can note
  \((\to) = x \to (f(x) \to f)\)} Let's the term \(\to\) be the function
that associates two expression to another function that associates those
expressions. This special function is derived from the notation of
\emph{morphisms} of the category theory.

\end{axiom}

The formal definition uses currying to decompose the function into two.
It associates a parameter to a function that takes an expression and
returns a function.

Next we need to lay off the base definitions of the formalism.

\hypertarget{formalism-definition}{%
\subsection{Formalism definition}\label{formalism-definition}}

This functional algebra at the base of our foundation is inspired by
\emph{operator algebra} (Takesaki
\protect\hyperlink{ref-takesaki_theory_2013}{2013}, vol. 125) and
\emph{relational algebra} (Jónsson
\protect\hyperlink{ref-jonsson_maximal_1984}{1984}). The problem with
the operator algebra is that it supposes vectors and real numbers to
work properly. Also, relational algebra, like category theory supposes
of set theory.

Here we define the basic notions of our functional algebra that dictates
the rules of the formalism we are defining.

\begin{definition}[Currying]\label{def:currying}

\footnote{\(\llp f\rrp = (x \to \llp f(x)\rrp)\)} Currying is the
operation named after the mathematician Haskell Brooks Curry
(\protect\hyperlink{ref-curry_studies_1958}{1958}) that allows multiple
argument functions in a simpler monoidal formalism. A monome is a
function that has only one argument and has only one value, as in our
main axiom.

The operation of \emph{currying} is a function \(\llp\rrp\) that
associates to each function \(f\) another function that recursively
partially applies \(f\) with one argument at a time.

\footnote{\(\llbp f\rrbp = \llbp x, y \to f(x)(y) \rrbp^+\)} If we take
a function \(h\) such that when applied to \(x\) gives the function
\(g\) that takes an argument \(y\), \emph{unCurrying} is the function
\(\llbp \rrbp\) so that \(f(x,y)\) behaves the same way as \(h(x)(y)\).
We note \(h = \llbp f \rrbp\).

\end{definition}

\begin{definition}[Application]\label{def:application}

\footnote{\(y = f(x)\)} We note the application of \(f\) with an
argument \(x\) as \(f(x)\). The application allows to recover the image
\(y\) of \(x\) which is the value that \(f\) associates with \(x\).

\end{definition}

Along with Currying, function application can be used \emph{partially}
to make constant some arguments.

\begin{definition}[Partial Application]\label{def:partialapplication}

We call \emph{partial application} the application using an insufficient
number of arguments to any function \(f\). This results in a function
that has fewer arguments with the first ones being locked by the partial
application. It is interesting to note that currying is simply a
recursion of partial applications.

\end{definition}

From now on we will note \(f(x, y, z, …)\) any function that has
multiple arguments but will suppose that they are implicitly Curryied.
If a function only takes two arguments, we can also use the infix
notation e.g.~\(x f y\) for its application.

\begin{example}

Applying this to basic arithmetic for illustration, it is possible to
create a function that will triple its argument by making a partial
application of the multiplication function \(×(3)\)\footnote{\(×(3)=x\to3×x\)}
so we can write the operation to triple the number \(2\) as \(×(3)(2)\)
or \(×(2,3)\) or with the infix notation \(2×3\).

\end{example}

\begin{definition}[Null]\label{def:null}

The \emph{null function} is the function between nothing and nothing. We
note it \(\none\).\footnote{\(\none = \none \to \none\)}

\end{definition}

The notation \(\none\) was chosen to represent the association arrow
(\(\to\)) but with a dot instead of the tail of the arrow. This is meant
to represent the fact that it inhibits association.

\hypertarget{literals-and-variables}{%
\subsection{Literals and Variables}\label{literals-and-variables}}

As everything is a function in our formalism, we use the null function
to define notions of variables and literals.

\begin{definition}[Literal]\label{def:literal}

A literal is a function that associates null to its value. This consists
of any function \(l\) written as \(\none \to l\).\footnote{\(l = \none \to l\)}
This means that the function has only itself as an immutable value. We
call \emph{constants} functions that have no arguments and have as value
either another constant or a literal.

\end{definition}

\begin{example}

A good example of that would be the yet to be defined natural numbers.
We can define the literal \(3\) as \(3 = \none \to 3\). This is a
fonction that takes no argument and is always valued to \(3\).

\end{example}

\begin{definition}[Variable]\label{def:variable}

A variable is a function that associates itself to null. This consists
of any function \(x\) written as \(x \to \none\).\footnote{\(x = x \to \none\)}
This means that the function requires an argument and has undefined
value. Variables can be seen as a demand of value or expression and mean
nothing without being defined properly.

\end{definition}

\begin{figure}
\hypertarget{fig:function}{%
\centering
\includegraphics{graphics/function.svg}
\caption{Illustration of basic functional operators and their
properties.}\label{fig:function}
}
\end{figure}

\begin{example}

The function \(f\) defined in figure~\ref{fig:function} associates its
argument to an expression. Since the argument \(x\) is also a variable,
the value is therefore dependant on the value required by \(x\). In that
example, the number \(3\) is a literal and \(3 \div x\) is therefore an
expression using the function \(\div\).

\end{example}

An interesting property of this notation is that \(\gtrdot\) is both a
variable and a constant. Indeed, by definition, \(\none\) is the
function that associates \(\none \to \none\) and fullfil both
definitions.

When defining currying, we annotated with the formalism
\(\llp f \rrp = f \to (x \to \llp f(x) \rrp)\). The obvious issue is the
absence of stopping condition in that recursive expression. While the
end of the recursion doesn't technically happen, in practice from the
way variables and literals are defined, the recursion chain either ends
up becoming a variable or a constant because it is undefined when
currying a nullary function.

\hypertarget{functional-algebra}{%
\subsection{Functional algebra}\label{functional-algebra}}

Inspired by relational algebra and by category theory, we present a new
functional algebra. The first operator of this algebra allows to combine
several functions into one. This is very useful to add to the definition
of a function by specifying special cases using a reduced notation.

\begin{definition}[Combination]\label{def:combination}

\footnote{\(\comb = (f, \none \to f) \comb (\none, f \to f)\)} The
\emph{combination function} \(\bowtie\) associates any two functions to
a new function that is the union of the definition of \textbf{either}
functions. If both functions are defined for any given argument, then
the combination is undefined (\(\none\))

\end{definition}

It is intresting to note that the formal definition of the combination
is recursive. This means that it will bee evaluated if any of the
expression matches, decomposing the functions until one of them isn't
defined or until nothing matches and therefore the result is \(\none\).

\begin{example}

For two functions \(f_1\) and \(f_2\) that are defined respectively by:

\begin{itemize}
\tightlist
\item
  \(f_1= (1\to 2) \comb (3 \to 4)\)
\item
  \(f_2= (2\to 3) \comb (3 \to 5)\)
\end{itemize}

the combination \(f_3 = f_1 \bowtie f_2\) will behave as follows:

\begin{itemize}
\tightlist
\item
  \(f_3(1)=2\)
\item
  \(f_3(2)=3\)
\end{itemize}

\end{example}

\begin{definition}[Superposition]\label{def:superposition}

\footnote{\(\super = (\comb) \comb (f,f \to f)\)} The
\emph{superposition function} \(\super\) is the function that associates
any two functions to a new function that associates any function
associated by \textbf{both} functions.

\end{definition}

We can say that the superposition is akin to an intersection where the
resulting function is defined when both functions have the same
behavior.

\begin{example}

Reusing the functions of the previous example, we can note that
\(f_3 \vartriangle f_1 = f_1\) because \(1 \to 2\) is the only
combination that both functions associates.

\end{example}

\begin{definition}[Subposition]\label{def:subposition}

\footnote{\(\sub = f_1, f_2 \to f_1 \comb (f_1 \super f_2)\)} The
\emph{subposition function} is the function \(\triangledown\) that
associates any two functions \(f_1\) and \(f_2\) to a new function
\(f_1 \triangledown f_2\) that associates anything associated by the
first function \(f_1\) but \textbf{not} by the second function \(f_2\).

\end{definition}

The subposition is akin to a substraction of function where we remove
everything defined by the second function to the definition of the
first.

We can also note a few properties of these functions:

\begin{longtable}[]{@{}ll@{}}
\caption{Example properties of superposition an
subposition}\tabularnewline
\toprule
Formula & Description\tabularnewline
\midrule
\endfirsthead
\toprule
Formula & Description\tabularnewline
\midrule
\endhead
\(f \vartriangle f = f\) & A function superposed to itself is the
same.\tabularnewline
\(f \vartriangle \none = \none\) & Any function superposed by null is
null.\tabularnewline
\(f_1 \vartriangle f_2 = f_2 \vartriangle f_1\) & The superposition
order doesn't affect the result.\tabularnewline
\(f \triangledown f = \none\) & A function subposed by itself is always
null.\tabularnewline
\(f \triangledown \none = f\) & Subposing null to any function doesn't
change it.\tabularnewline
\bottomrule
\end{longtable}

These functions are intuitively the functional equivalent of the union,
intersection and difference from set theory. In our formalism we will
define the set operations from these.

The following operators are the classical operations on functions.

\begin{definition}[Composition]\label{def:composition}

The \emph{composition function} is the function that associates any two
functions \(f_1\) and \(f_2\) to a new function such that:
\(f_1 \circ f_2 = x \to f_1(f_2(x))\).

\end{definition}

\begin{definition}[Inverse]\label{def:inverse}

The \emph{inverse function} is the function that associates any function
to its inverse such that if \(y = f(x)\) then \(x = \bullet(f)(y)\).

We can also use an infix version of it with the composition of
functions: \(f_1 \bullet f_2 = f_1 \circ \bullet(f_2)\).

\end{definition}

These properties are akin to multiplication and division in arithmetic.

\begin{longtable}[]{@{}ll@{}}
\caption{Example of function composition and inverse with their
properties.}\tabularnewline
\toprule
Formula & Description\tabularnewline
\midrule
\endfirsthead
\toprule
Formula & Description\tabularnewline
\midrule
\endhead
\(f \circ \gtrdot = \gtrdot\) & This means that \(\gtrdot\) is the
absorbing element of the composition.\tabularnewline
\(f \circ (=) = f\) & Also, \(=\) is the neutral element of the
composition.\tabularnewline
\(\bullet(\gtrdot) = \gtrdot \land \bullet(=) = (=)\) & This means that
\(\none\) and \(=\) are commutative functions.\tabularnewline
\(f_1 \circ f_2 \neq f_2 \circ f_1\) & However, \(\circ\) is not
commutative.\tabularnewline
\bottomrule
\end{longtable}

From now on, we will use numbers and classical arithmetic as we had
defined them. However, we consider defining them from a foundation point
of view, later using set theory and Peano's axioms.

\begin{figure}
\hypertarget{fig:power}{%
\centering
\includegraphics{graphics/function_power.svg}
\caption{Illustration of how the functional equivalent of the power
function is behaving with notable values (in filled
circles)}\label{fig:power}
}
\end{figure}

In classical mathematics, the inverse of a function \(f\) is often
written as \(f^{-1}\). Therefore we can define the transitivity of the
\(n\)\textsuperscript{th} degree as the power of a function such that
\(f^n = f^{n-1} \circ f\). Figure~\ref{fig:power} shows how the power of
a function is behaving at key values.

By generalizing the formula, we can define the \emph{transitive cover}
of a function \(f\) and its inverse respectively as
\(f^+ = f^{+\infty}\) and \(f^- = f^{-\infty}\). This cover is the
application of the function to its result infinitely. This is useful
especially for graphs as the transitive cover of the adjacency function
of a graph gives the connectivity function (see
section~\ref{sec:graph}).

We also call \emph{arity} the number of arguments (or the currying
order) of a function noted \(|f|\).

\hypertarget{properties}{%
\subsection{Properties}\label{properties}}

A modern approach of mathematics is called \emph{reverse mathematics} as
instead of building theorems from axioms, we search the minimal set of
axioms required by a theorem. Inspired by this, we aim to minimize the
formal basis of our system as well as identifying the circularity
issues, we provide a dependency graph in figure~\ref{fig:dependancies}.
We start with the axiom of \nameref{axi:association} at the bottom and
the axiom of \nameref{axi:identity} at the top. Everything depends on
those two axioms but drawing all the arrows makees the figuree way less
leggible.

Then we define the basic application function \(()\) that has as
complement the Currying \(\llp\rrp\) and unCurrying \(\llbp\rrbp\)
functions. Similarly, the combination \(\comb\) has the superposition
\(\super\) and the subpostion \(\sub\) functions as complements. The
bottom bound of the algebra is the null function \(\none\) and the top
is the identity function \(=\). Composition \(\comp\) is the the main
operator of the algebra and allows it to have an inverse element as the
inverse function \(\inv\). The composition function needs the
application function in order to be constructed.

\begin{figure}
\hypertarget{fig:dependancies}{%
\centering
\includegraphics{graphics/dependancies.svg}
\caption{Dependency graph of notions in the functional
theory}\label{fig:dependancies}
}
\end{figure}

The algebra formed by the previously defined operations on functions is
a semiring \((\bb{F}, \bowtie, \circ)\) with \(\bb{F}\) being the set of
all functions.

Indeed, \(\bowtie\) is a commutative monoid having \(\none\) as its
identity element and \(\circ\) is a monoid with \((=)\) as its identity
element.

Also the composition of the combination is the same as the combination
of the composition. Therefore, \(\circ\) (composition) distributes over
\(\bowtie\) (combination).

At last, using partial application composing with null gives null:
\(\circ(\none) = ((\none) \to \none) = \none\).

This foundation is now ready to define other fields of mathematics. We
start with logic as it is a very basic formalism in mathematics. Then we
redefine the ZFC set theory using our formalism as a base. And finally
we will present derived mathematical tools to represent data structures
and their properties.

\hypertarget{sec:fol}{%
\section{First Order Logic}\label{sec:fol}}

In this section, we present First Order Logic (FOL). FOL is based on
boolean logic with the two literals \(\top\) \emph{true} and \(\bot\)
\emph{false}.

A function noted \(( ? )\) that have as only values either \(\top\) or
\(\bot\) is called a \textbf{predicate}.\footnote{\(\cal{D}(\bullet ?) = \{\bot, \top\}\)}

We define the classical logic \emph{entailment}, the predicate that
holds true when a predicate (the conclusion) is true if and only if the
first predicate (the premise) is true.

\[\vdash = (\bot, x \to \top) \bowtie (\top, x \to x)\]

Then we define the classical boolean operators \(\lnot\) \emph{not},
\(\land\) \emph{and} and \(\lor\) \emph{or} as:

\begin{itemize}
\tightlist
\item
  \(\lnot = (\bot \to \top) \bowtie (\top \to \bot)\), the negation
  associates true to false and false to true.
\item
  \(\land = x \to ((\top \to x) \bowtie (\bot \to \bot))\), the
  conjunction is true when all its arguments are simultaneously true.
\item
  \(\lor = x \to ((\top \to \top) \bowtie (\bot \to x))\), the
  disjunction is true if all its arguments are not false.
\end{itemize}

The last two operators are curried function and can take any number of
arguments as necessary and recursively apply their definition.

Another basic preicate is the \textbf{equation}. It is th identity
function \(=\) but as a binary predicate that is true whenever the two
arguments are the same.

Functions that takes an expression as parameters are called
\emph{modifiers}. FOL introduces a useful kind of modifier used to
modalize expressions: \emph{quantifiers}. Quantifiers take an expression
and a variable as arguments. Classical quantifiers are also predicates:
they restrict the values that the variable can take.

The classical quantifiers are:

\begin{itemize}
\tightlist
\item
  The \emph{universal quantifier} \(\forall\) meaning \emph{``for
  all''}.\footnote{\(\forall = \textsection(\land)\)}
\item
  The \emph{existential quantifier} \(\exists\) meaning \emph{``it
  exists''}.\footnote{\(\exists = \textsection(\lor)\)}
\end{itemize}

They are sometimes extended with :

\begin{itemize}
\tightlist
\item
  The \emph{uniqueness quantifier} \(\exists!\) meaning \emph{``it
  exists a unique''}.\footnote{\(\exists! = \textsection(=(1) \circ +)\)}
\item
  The \emph{exclusive quantifier} \(\nexists\) meaning \emph{``it
  doesn't exist''}.\footnote{\(\nexists = \textsection(\lnot \circ \land)\)}
\end{itemize}

Another exotic quantifier that isn't a predicate can be proven useful
(Hehner \protect\hyperlink{ref-hehner_practical_2012}{2012}):

\begin{itemize}
\tightlist
\item
  The \emph{solution quantifier} \(\textsection\) meaning
  \emph{``those''}.\footnote{\(\textsection = f, x, ? \to \lBrace f(x) : ? \rBrace\)}
\end{itemize}

The last three quantifiers are optional in FOL but will be conducive
later on. It is interesting to note that most quantified expression can
be expressed using the set builder notation discussed in the following
section.

\hypertarget{sec:set}{%
\section{Set Theory}\label{sec:set}}

Since we need to represent knowledge, we will handle more complex data
than simple booleans. One such way to describe more complex knowledge is
by using set theory. It is used as the classical foundation of
mathematics. Most other proposed foundation of mathematics invoke the
concept of sets even before their first formula to describe the kind of
notions they are introducing. The issue is then to define the sets
themselves. At the beginning of his founding work on set theory, Cantor
wrote:

\begin{quote}
``\emph{A set is a gathering together into a whole of definite, distinct
objects of our perception or of our thought--which are called elements
of the set.}''\footnote{Georg Cantor
  (\protect\hyperlink{ref-cantor_beitrage_1895}{1895})}
\end{quote}

For Cantor, a set is a collection of concepts and percepts. In our case
both notions are grouped in what we call \emph{objects}, \emph{entities}
that are all ultimately \emph{functions} in our formalism.

\hypertarget{base-definitions}{%
\subsection{Base Definitions}\label{base-definitions}}

This part is based on the work of Cantor
(\protect\hyperlink{ref-cantor_beitrage_1895}{1895}) and the set theory.
The goal is to define the notions of set theory using our formalism.

\begin{definition}[Set]\label{def:set}

A collection of \emph{distinct} objects considered as an object in its
own right. We define a set one of two ways (always using braces):

\begin{itemize}
\tightlist
\item
  In extension by listing all the elements in the set: \(\{0,1,2,3,4\}\)
\item
  In intention by specifying the rule that all elements follow:
  \(\{n : ?(n)\}\)
\end{itemize}

\end{definition}

Using our functional foundation, we can define any set as a predicate
\(\cal{S} = e \to \top\) with \(e\) being a member of \(\cal{S}\). This
allows us to define the member function noted \(e \in \cal{S}\) to
indicate that \(e\) is an element of \(\cal{S}\).\footnote{\(\in = e, \cal{S} \to \cal{S}(e)\)}

Another, useful definition using sets is the \emph{domain} of a function
\(f\) as the set of all arguments for which the function is defined. We
call \emph{co-domain} the domain of the inverse of a function. We can
note them \(f: \cal{D}(f) \mapsto \cal{D}(\bullet f)\). In the case of
our functional version of sets, they are their own domain.

\begin{definition}[Specification]\label{def:specification}

The \emph{function of specification} (noted \(:\)) is a function that
restricts the validity of an expression given a predicate.\footnote{\((:) = f, ? \to f \triangledown (\cal{D}(? = \bot) \mapsto \cal{D}(\bullet f))\)}
It can be intuitively be read as \emph{``such that''}.

\end{definition}

The specification operator is extensively used in classical mathematics
but informally, it is often seen as an extension of natural language and
can be quite ambiguous. In the present document any usage of \((:)\) in
any mathematical formula will follow the previously discussed
definition.

\hypertarget{set-operations}{%
\subsection{Set Operations}\label{set-operations}}

Along with defining the domains of functions using sets, we can use
function on sets. This is very important in order to define ZFC and is
extensively used in the rest of the document.

In this section, basic set operations are presented. The first one is
the subset.

\begin{definition}[Subset]\label{def:subset}

A subset is a part of a set that is integrally contained within it. We
note
\(\cal{S} \subset \cal{T} \vdash ((e \in \cal{S} \vdash e\in \cal{T}) \land \cal{S} \neq \cal{T})\),
that a set \(\cal{S}\) is a proper subset of a more general set
\(\cal{T}\).

\end{definition}

\begin{definition}[Union]\label{def:union}

The union of two or more sets \(\cal{S}\) and \(\cal{T}\) is the set
that contains all elements in \emph{either} set. We can note it:

\[\cal{S} \cup \cal{T} = \{ e : e \in \cal{S} \lor a \in \cal{T}\}\]

\end{definition}

\begin{definition}[Intersection]\label{def:intersection}

The intersection of two or more sets \(\cal{S}\) and \(\cal{T}\) is the
set that contains only the elements member of \emph{both} set. We can
note it:

\[\cal{S} \cap \cal{T} = \{ e : e \in \cal{S} \land e \in \cal{T}\}\]

\end{definition}

\begin{definition}[Difference]\label{def:difference}

The difference of one set \(\cal{S}\) to another set \(\cal{T}\) is the
set that contains only the elements contained in the first but not the
last. We can note it:

\[\cal{S} \setminus \cal{T} = \{ e : e \in \cal{S} \land e \notin \cal{T}\}\]

\end{definition}

An interesting way to visualize relationships with sets is by using Venn
diagrams (Venn \protect\hyperlink{ref-venn_diagrammatic_1880}{1880}). In
figure~\ref{fig:venn} we present the classical union, intersection and
difference operations. It also introduce a new way to represent more
complicated notions such as the cartesian product by using a
representation for powerset and higher dimensionality inclusion that a
2D Venn diagram cannot represent.

\begin{figure}
\hypertarget{fig:venn}{%
\centering
\includegraphics{graphics/venn.svg}
\caption{Example of an upgraded Venn diagram to illustrate operations on
sets.}\label{fig:venn}
}
\end{figure}

\begin{example}

Figure~\ref{fig:venn} is the graphical representation of these
statements:

\begin{longtable}[]{@{}ll@{}}
\caption{Statements represented in the extended Venn diagram in
figure~\ref{fig:venn}.}\tabularnewline
\toprule
Formula & Description\tabularnewline
\midrule
\endfirsthead
\toprule
Formula & Description\tabularnewline
\midrule
\endhead
\(e_1 \in \cal{S}_1\) & \(e_1\) is an element of the set
\(\cal{S}_1\).\tabularnewline
\(e_2 \in \cal{S}_1 \cap \cal{S}_2\) & \(e_2\) is an element of the
intersection of \(\cal{S}_1\) and \(\cal{S}_2\).\tabularnewline
\(e_3 \in \cal{S}_1 \cap \cal{S}_2 \cap \cal{S}_3\) & \(e_3\) is an
element of the intersection of \(\cal{S}_1\), \(\cal{S}_2\) and
\(\cal{S}_3\).\tabularnewline
\(\cal{S}_5 \subset \cal{S}_2\) & \(\cal{S}_5\) is a subset of
\(\cal{S}_2\).\tabularnewline
\(\cal{S}_6 \subset \cal{S}_2 \cup \cal{S}_3\) & \(\cal{S}_6\) is a
subset of the union of \(\cal{S}_2\) and \(\cal{S}_3\).\tabularnewline
\(f = \cal{S}_5 \mapsto \cal{S}_6\) & \(f\) is a function which domain
is \(\cal{S}_5\) and co-domain is \(\cal{S}_6\).\tabularnewline
\(\cal{S}_4 \subset \powerset(\cal{S}_1)\) & \(\cal{S}_4\) is a
combination of elements of \(\cal{S}_1\).\tabularnewline
\bottomrule
\end{longtable}

\end{example}

These Venn diagrams, originally have a lack of expressivity regarding
complex operations on sets. Indeed, from their dimensionality it is
complicated to express numerous sets having intersection and
disjunctions. For example, it is difficult to represent the following
notion.

\begin{definition}[Cartesian product]\label{def:cartesian}

The Cartesian product of two sets \(\cal{S}\) and \(\cal{T}\) is the set
that contains all possible combination of an element of both sets. These
combinations are a kind of ordered set called \emph{tuples}. We note
this product:

\[\cal{S} \times \cal{T} = \{\langle e_{\cal{S}}, e_{\cal{T}}\rangle : e_{\cal{S}} \in \cal{S} \land e_{\cal{T}} \in \cal{T}\}\]

\end{definition}

From this we can also define the set power recursively by
\(\cal{S}^1 = \cal{S}\) and
\(\cal{S}^n = \cal{S} \times \cal{S}^{n-1}\).

The Cartesian product is the set equivalent of currying as:

\[\cal{S} \times \cal{T} = e_{\cal{S}}, e_{\cal{T}} \to \cal{S}(e_{\cal{S}}) \land \cal{T}(e_{\cal{T}})\]

The angles \(\langle\rangle\) notation is used for tuples, those are
another view on currying by replacing several arguments using a single
one as an ordered list. A tuple of two elements is called a \emph{pair},
of three elements a \emph{triple}, etc. We can access elements in tuples
using their index in the following way
\(e_2 = \langle e_1 , e_2 , e_3 \rangle_2\).

\begin{definition}[Mapping]\label{def:mapping}

The mapping notation \(\lBrace \rBrace\) is a function such that
\(\lBrace f(x): x \in \cal{S} \rBrace\) will give the result of applying
all elements in set \(\cal{S}\) as arguments of the function using the
unCurrying operation recursively. If the function isn't specified, the
mapping will select a member of the set non deterministically. The
function isn't defined on empty sets or on sets with fewer members than
arguments of the provided function.

\end{definition}

\begin{example}

The classical sum operation on numbers can be noted:

\[\sum_{i=1}^3 2i = \lBrace +(2*i) : i \in [1, 3] \rBrace = +(2*1)(+(2*2)(2*3))\]

\end{example}

\hypertarget{the-zfc-theory}{%
\subsection{The ZFC Theory}\label{the-zfc-theory}}

The most common axiomatic set theory is ZFC (Kunen
\protect\hyperlink{ref-kunen_set_1980}{1980}, vol. 102). In that
definition of sets there are a few notions that come from its axioms. By
being able to distinguish elements in the set from one another we assert
that elements have an identity and we can derive equality from there:

\begin{axiom}[Extensionality]\label{axi:extensionality}

\(\forall\cal{S} \forall\cal{T} : \forall e((e\in\cal{S})=(e\in\cal{T})) \vdash \cal{S}=\cal{T}\)

\end{axiom}

This means that two sets are equal if and only if they have all their
members in common.

Another axiom of ZFC that is crucial in avoiding Russel's paradox
(\(\cal{S} \in \cal{S}\)) is the following:

\begin{axiom}[Foundation]\label{axi:fondation}

\(\forall \cal{S} : (\cal{S} \neq \emptyset \vdash \exists \cal{T}\in \cal{S},(\cal{T}\cap \cal{S}=\emptyset))\)

\end{axiom}

This axiom uses the empty set \(\emptyset\) (also noted \(\{\}\)) as the
set with no elements. Since two sets are equal if and only if they have
precisely the same elements, the empty set is unique.

The definition by intention uses the set builder notation to define a
set. It is composed of an expression and a predicate \(?\) that will
make any element \(e\) in a set \(\cal{T}\) satisfying it part of the
resulting set \(\cal{S}\), or as formulated in ZFC:

\begin{axiom}[Specification]\label{axi:specification}

\(\forall ? \forall \cal{T} \exists \cal{S} : \left(\forall e \in \cal{S} : (e \in \cal{T} \land ?(e)) \right)\)

\end{axiom}

The last axiom of ZFC we use is to define the power set \(\wp(\cal{S})\)
as the set containing all subsets of a set \(\cal{S}\):

\begin{axiom}[Power set]\label{axi:powerset}

\(\wp(\cal{S}) = \{\cal{T} : \cal{T} \subseteq \cal{S}\}\)

\end{axiom}

With the symbol
\(\cal{S} \subseteq \cal{T} \vdash (\cal{S} \subset \cal{T} \lor \cal{S} = \cal{T})\).
These symbols have an interesting property as they are often used as a
partial order over sets.

\hypertarget{sec:graph}{%
\section{Graphs}\label{sec:graph}}

With set theory, it is possible to introduce all of standard
mathematics. A field of interest for this thesis is the study of the
structure of data. This interest arrises from the need to encode
semantic information in a knowledge base using a very simple language
(see chapter~\ref{ch:self}). Most of these structures use graphs and
isomorphic derivatives.

\begin{definition}[Graph]\label{def:graph}

A graph is a mathematical structure \(g\) which is defined by its
\emph{connectivity function} \(\chi\) that

\end{definition}

\hypertarget{adjacency-incidence-and-connectivity}{%
\subsection{Adjacency, Incidence and
Connectivity}\label{adjacency-incidence-and-connectivity}}

\begin{definition}[Connectivity]\label{def:connectivity}

The connectivity function is a combination of the classical adjacency
and incidence functions of the graph. It is defined using a circular
definition in the following way:

\begin{itemize}
\tightlist
\item
  \emph{Adjacency}:
  \(\chi_{\adja} = v \to \{ e: v \in \chi_{\inci}(e) \}\)\footnote{Also:
    \(\chi_{\adja} = \bullet \chi_{\inci}\)}
\item
  \emph{Incidence}:
  \(\chi_{\inci} = e \to \{ v: e \in \chi_{\adja}(v) \}\)
\end{itemize}

Defining either function defines the graph. For convenience, the
connectivity function combine the adjacency and incidence:

\[\chi = \chi_{\adja} \comb \chi_{\inci}\]

\end{definition}

Usually, graphs are noted \(g=(V,E)\) with the set of vertices \(V\)
(also called nodes) and edges \(E\) (arcs) that links two vertices
together. Each edge is classically a pair of vertices ordered or not
depending on if the graph is directed or not.\footnote{\(E \subseteq V^2\)}
It is possible to go from the set based definition to the functional
relation using the following equation: \(\dom(\chi_{\inci}) = E\)

\begin{example}

\begin{figure}
\hypertarget{fig:transitive}{%
\centering
\includegraphics{graphics/transitivity.svg}
\caption{Example of the recursive application of the transitive cover to
a graph.}\label{fig:transitive}
}
\end{figure}

A graph is often represented with lines or arrows linking points
together like illustrated in figure~\ref{fig:transitive}. In that
figure, the vertices \(v_1\) and \(v_2\) are connected through an
undirected edge. Similarly \(v_3\) connects to \(v_4\) but not the
opposite since they are bonded with a directed edge. The vertex \(v_8\)
is also connected to itself.

\end{example}

\hypertarget{digraphs}{%
\subsection{Digraphs}\label{digraphs}}

In digraphs or \emph{directional graphs} are a specific case of graphs
where edges have a direction. This means that we can have two vertices
\(v_1\) and \(v_2\) linked by an edge and while it is possible to go
from \(v_1\) to \(v_2\), the inverse is impossible. For such case the
edges are ordered pairs and the incidence function can be decomposed
into:

\[\chi_{\inci} = \chi_{\ingo} \comb \chi_{\outgo}\]

We note \(\chi_{\ingo}\) the \textbf{incoming relation} and
\(\chi_{\outgo}\) the \textbf{outgoing relation}.

In digraphs, classical edges can exist if allowed and will simply be
bi-directional edges.

\hypertarget{path-cycles-and-transitivity}{%
\subsection{Path, cycles and
transitivity}\label{path-cycles-and-transitivity}}

Most of the intrinsic information of a graph is contained within its
structure. Exploring its properties require to study the ``shape'' of a
graph and to find relationships between vertices. That is why graph
properties are easier to explain using the transitive cover \(\chi^+\)
of any graph \(g = (V,E)\).

This transitive cover will create another graph in which two vertices
are connected through an edge if and only if it exists a path between
them in the original graph \(g\). We illustrate this process in
figure~\ref{fig:transitive}. Note how there is no edge in \(\chi^2(g)\)
between \(v_5\) and \(v_6\) and the one in \(\chi^3(g)\) is directed
towards \(v_5\) because there is no path back to \(v_6\) since the edge
between \(v_3\) and \(v_4\) is directed.

\begin{definition}[Path]\label{def:path}

We say that vertices \(v_1\) and \(v_2\) are \emph{connected} if it
exists a path from one to the other. Said otherwise, there is a path
from \(v_1\) to \(v_2\) if and only if
\(\langle v_1, v_2 \rangle \in \dom(\chi^+(g))\).

\end{definition}

The notion of connection can be extended to entire graphs. An undirected
graph \(g\) is said to be \emph{connected} if and only if
\(\forall e \in V^2 ( e \in \dom(\chi^+(g)))\).

Similarly we define \emph{cycles} as the existence of a path from a
given vertex to itself. For example, in figure~\ref{fig:transitive}, the
cycles of the original graph are colored in blue. Some graphs can be
strictly acyclical, enforcing the absence of cycles.

\hypertarget{trees}{%
\subsection{Trees}\label{trees}}

A \textbf{tree} is a special case of a graph. A tree is an acyclical
connected graph. If a special vertex called a \emph{root} is chosen, we
call the tree a \emph{rooted tree}. It can then be a directed graph with
all edges pointing away from the root. When progressing away from the
root, we call the current vertex \emph{parent} of all exterior
\emph{children} vertices. Vertex with no children are called
\emph{leaves} of the tree and the rest are called \emph{branches}.

An interesting application of trees to FOL is called \emph{and/or trees}
where each vertex has two sets of children: one for conjunction and the
other for disjunction. Each vertex is a logic formula and the leaves are
atomic logic propositions. This is often used for logic problem
reduction. In figure~\ref{fig:andor} we illustrate how and/or trees are
often depicted.

\begin{figure}
\hypertarget{fig:andor}{%
\centering
\includegraphics{graphics/and-or.svg}
\caption{Example of and/or tree.}\label{fig:andor}
}
\end{figure}

\hypertarget{quotient}{%
\subsection{Quotient}\label{quotient}}

Another notion often used for reducing big graphs is the quotiening as
illustrated in figure~\ref{fig:quotient}.

\begin{definition}[Graph Quotient]\label{def:quotient}

A quotient over a graph is the act of reducing a subgraph into a node
while preserving the external connections. All internal structure
becomes ignored and the subgraph now acts like a regular node. We note
it \(\div_f(g)= (\{f(v): v \in V \}, \{ f(e) : e\in E \})\) with \(f\)
being a function that maps any vertex either toward itself or toward its
quotiened vertex.

\end{definition}

\begin{figure}
\hypertarget{fig:quotient}{%
\centering
\includegraphics{graphics/quotient.svg}
\caption{Example of graph quotient.}\label{fig:quotient}
}
\end{figure}

A quotion can be thought of as the operation of merging several vertices
into one while concerving their connections with other vertices.

\begin{example}

Figure~\ref{fig:quotient} explains how to do the quotient of a graph by
merging the vertices \(v_2\), \(v_5\) and \(v_8\) into \(v_{\div}\). The
edge beetween \(v_2\) and \(v_5\) is lost since it is inside the
quotienned part of the graph. All other edges are now connected to the
new vertex \(v_{\div}\).

\end{example}

\hypertarget{hypergraphs}{%
\subsection{Hypergraphs}\label{hypergraphs}}

A generalization of graphs are \textbf{hypergraphs} where the edges are
allowed to connect to more than two vertices. They are often represented
using Venn-like representations but can also be represented with edges
``gluing'' several vertex like in figure~\ref{fig:hypergraph}.

\begin{figure}
\hypertarget{fig:hypergraph}{%
\centering
\includegraphics{graphics/hypergraph.svg}
\caption{Example of hypergraph with total freedom on the edges
specification.}\label{fig:hypergraph}
}
\end{figure}

\begin{example}

In figure~\ref{fig:hypergraph}, vertices are the discs and edges are
either lines or gluing surfaces. In hypergraph, classical edges can
exist like \(e_4\), \(e_6\) or \(e_7\). Taking for example \(e_1\), we
can see that it connects 3 vertices: \(v_1\), \(v_2\) and \(v_3\). It is
also possible to have an edge connecting edges like \(e_8\) that
connects \(e_3\) to itself. Edges can also ``glue'' more than two edges
like \(e_2\) connects \(e_1\), \(e_3\) and \(e_4\). The more exotic
structures allows are edge-loops as seen with \(e_9\) and \(e_10\) which
allows edge only graphs.

\end{example}

An hypergraph is said to be \emph{\(n\)-uniform} if the edges are
restricted to connect to only \(n\) vertices together. In that regard,
classical graphs are 2-uniform hypergraphs.

Hypergraphs have a special case where \(E \subset V\). This means that
edges are allowed to connect to other edges. In
figure~\ref{fig:hypergraph}, this is illustrated by the edge \(e_3\)
connecting to three other edges. Information about these kinds of
structures for knowledge representation is hard to come by and rely
mostly on a form of ``folk wisdom'' within the mathematics community
where knowledge is rarely published and mostly transmitted orally during
lessons. One of the closest information available is this forum post
(Kovitz \protect\hyperlink{ref-kovitz_terminology_2018}{2018}) that
associated this type of graph to port graphs (Silberschatz
\protect\hyperlink{ref-silberschatz_port_1981}{1981}). Additional
information was found in the form of a contribution of Vepstas
(\protect\hyperlink{ref-vepstas_hypergraph_2008}{2008}) on an
encyclopedia article about hypergraphs. In that contribution, he says
that a generalization of hypergraph allowing for edge-to-edge
connections violate the axiom of \nameref{axi:fondation} of ZFC by
allowing edge loops. Indeed, like in figure~\ref{fig:hypergraph}, an
edge \(e_9 = \{e_{10}\}\) can connect to another edge
\(e_{10} = \{ e_9 \}\) causing an infinite descent inside the \(\in\)
relation in direct contradiction with ZFC.

This shows the limits of standard mathematics especially on the field of
knowledge representation. Some structures needs higher dimensions than
allowed by the structure of ZFC and FOL. However, it is important not to
be mistaken: such non-standard set theories are more general than ZFC
and therefore contains ZFC as a special case. All is a matter of
restrictions.

\hypertarget{sheaf}{%
\section{Sheaf}\label{sheaf}}

In order to understand sheaves, we need to present a few auxiliary
notions. Most of these definitions are adapted from (Vepštas
\protect\hyperlink{ref-vepstas_sheaves_2008}{2008}). The first of which
is a seed.

\begin{figure}
\hypertarget{fig:seed}{%
\centering
\includegraphics{graphics/seed_section_stalk.svg}
\caption{Example of a seed, a section and a stalk.}\label{fig:seed}
}
\end{figure}

\begin{definition}[Seed]\label{def:seed}

A seed corresponds to a vertex along with the set of adjacent edges.
Formally we note a seed \(\seed = (v, \chi_g(v))\) that means that a
seed build from the vertex \(v\) in the graph \(g\) contains a set of
adjacent edges \(\chi_g(v)\). We call the vertex \(v\) the \emph{germ}
of the seed. All edges in a seed do not connect to the other vertices
but keep the information and are able to match the correct vertices
through typing (often a type of a single individual). We call the edges
in a seed \emph{connectors}.

\end{definition}

Seeds are extracts of graphs that contain all information about a
vertex. Illustrated in the figure~\ref{fig:seed}, seeds have a central
germ (represented with discs) and connectors leading to a typed vertex
(outlined circles). Those external vertices are not directly contained
in the seed but the information about what vertex can fit in them is
kept. It is useful to represent connectors like jigsaw puzzle pieces:
they can match only a restricted number of other pieces that match their
shape.

From there, it is useful to build a kind of partial graph from seeds
called sections.

\begin{definition}[Section]\label{def:section}

A section is a set of seeds that have their common edges connected. This
means that if two seeds have an edge in common connecting both germs,
then the seeds are connected in the section and the edges are merged. We
note \(g_\seed = (V, \lBrace \cup : E_{section} \rBrace)\) the graph
formed by the section.

\end{definition}

In figure~\ref{fig:seed}, a section is represented. It is a connected
section composed of seeds along with the additional seeds of any
vertices they have in common. They are very similar to subgraph but with
an additional border of typed connectors. This tool was originally
mostly meant for big data and categorization over large graphs. As the
graph quotient is often used in that domain, it was ported to sections
instead of graphs allows us to define stalks.

\begin{definition}[Stalk]\label{def:stalk}

Given a projection function \(f:V\to V'\) over the germs of a section
\(\seed\), the stalk above the vertex \(v' \in V'\) is the quotient of
all seeds that have their germ follow \(f(v) = v'\).

\end{definition}

The quotienning is used in stalks for their projection. Indeed, as shown
in figure~\ref{fig:seed}, the stalks are simply a collection of seeds
with their germs quotiened into their common projection. The projection
can be any process of transformation getting a set of seeds in one side
and gives object in any base space called the image. Sheaves are a
generalization of this concept to sections.

\begin{figure}
\hypertarget{fig:sheaf}{%
\centering
\includegraphics{graphics/sheaf.svg}
\caption{Example of sheaves.}\label{fig:sheaf}
}
\end{figure}

\begin{definition}[Sheaf]\label{def:sheaf}

A sheaf is a collection of sections, together with a projection. We note
it \(\cal{F} = \langle g_{\seed}, glue \rangle\) with the function
\(glue\) being the gluing axioms that the projection should respect
depending on the application. The projected sheaf graph is noted as the
fusion of all quotiened sections:

\[glue_{\cal{F}} = \{ \div_{glue_{\seed}} : \{glue_{\seed} \in g_{\seed}\}\]

\end{definition}

By puting several sections into one projection, we can build stack
fields. These fields are simply a subcategory of sheaves. Illustrated in
figure~\ref{fig:sheaf}, a sheaf is a set of sections with a projection
relation that usually merge similarly typed connectors.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this chapter, we shown the limits of the formalization in mathematics
and the possible ways to mitigatee them. We also presented a fondation
of mathematics derived from category theory as well as defining
mathematical tools very useful to formalize computer science notions.
The last notion, the sheaf, is at the heart of the data structure for
the knowledge describtion model presented in the next chapter.

\hypertarget{ch:self}{%
\chapter{Knowledge Representation}\label{ch:self}}

Knowledge representation is at the intersection of maths, logic,
language and computer sciences. Knowledge description systems rely on
syntax to interoperate systems and users to one another. The base of
such languages comes from the formalization of automated grammars by
Chomsky (\protect\hyperlink{ref-chomsky_three_1956}{1956}). \marginpar{%
\includegraphics{portraits/noam_chomsky.jpg}
\captionof{figure}{Noam Chomsky 2017}
} It mostly consists of a set of production rules aiming to describe all
accepted input strings. Usually, the rules are hierarchical and
deconstruct the input using simpler rules until it matches a terminal
symbol. This deconstruction is called parsing and is a common operation
in computer science. More tools for the characterization of computer
language emerged soon after thanks to Backus
(\protect\hyperlink{ref-backus_syntax_1959}{1959}) while working on a
programming language at IBM. This is how the Backus-Naur Form (BNF)
metalanguage was created on top of Chomsky's formalization.

A similar process happened in the 1970s, when logic based knowledge
representation gained popularity among computer scientists (Baader
\emph{et al.} \protect\hyperlink{ref-baader_description_2003}{2003}).
Systems at the time explored notions such as rules and networks to try
and organize knowledge into a rigorous structure. At the same time other
systems were built based on First Order Logic (FOL). Then, around the
1990s, the research began to merge in search of common semantics in what
led to the development of Description Logics (DL). This domain is
expressing knowledge as a hierarchy of classes containing individuals.

From there and with the advent of the world wide web, actors of the
internet were on the lookout for standardization and interoperability of
computer systems. One such standardization took the name of ``semantic
web'' and aimed to create a widespread network of connected services
sharing knowledge between one another in a common language. At the
beginning of the 21\textsuperscript{st} century, several languages were
created, all based on the World Wide Web Consortium (W3C) specifications
called Resource Description Framework (RDF) (Klyne and Carroll
\protect\hyperlink{ref-klyne_resource_2004}{2004}). This language is
based on the notion of statements as triples. Each can express a unit of
knowledge. All the underlying theoretical work of DL continued with it
and created more expressive derivatives. One such derivative is the
family of languages called Web Ontology Language (OWL) (Horrocks
\emph{et al.} \protect\hyperlink{ref-horrocks_shiq_2003}{2003}). The
ontologies and knowledge graphs are more recent names for the
representation and definition of categories (DL classes), properties and
relation between concepts, data and entities.

Nowadays, when designing a knowledge representation, one usually starts
with existing framework. The most popular in practice is certainly the
classical relational database, followed closely by more novel methods
for either big data or more expressive solutions like ontologies.

In this chapter, we present a new tool that is more expressive than
ontologies while remaining efficient. This model is based on dynamic
grammar and basically is defined mosty by the structure of knowledge.
Our model is inspired from RDF tripplets, especially in its Turtle
syntax (W3C \protect\hyperlink{ref-w3c_rdf_2014}{2014}). Of course, this
will lead to compromises, but can also have some interesting properties.
This knowledge representation system will allow us to express
hierarchical planning domains in chapter~\ref{ch:heart}.

\hypertarget{grammar-and-parsing}{%
\section{Grammar and Parsing}\label{grammar-and-parsing}}

Grammar is an old tool that used to be dedicated to linguists. With the
funding works by Chomsky and his Context-Free Grammars (CFG), these
tools became available to mathematicians and shortly after to computer
scientists.

A CFG is a formal grammar that aims to generate a formal language given
a set of hierarchical rules. Each rule is given a symbol as a name. From
any finite input of text in a given alphabet, the grammar should be able
to determine if the input is part of the language it generates.

\hypertarget{bnf}{%
\subsection{BNF}\label{bnf}}

In computer science, popular metalanguage called BNF was created shortly
after Chomsky's work on CFG. The syntax is of the following form :

\begin{lstlisting}
<rule> ::= <other_rule> | <terminal_symbol> | "literals"
\end{lstlisting}

A terminal symbol is a rule that does not depend on any other rule. It
is possible to use recursion, meaning that a rule will use itself in its
definition. This actually allows for infinite languages. Despite its
expressive power, BNF is often used in one of its extended forms.

In this section, we introuce a widely used form of BNF syntax that is
meant to be human readable despite not being very formal. We add the
repetition operators \passthrough{\lstinline!*!} and
\passthrough{\lstinline!+!} that respectively repeat 0 and 1 times or
more the preceding expression. We also add the negation operator
\passthrough{\lstinline!\~!} that matches only if the following
expression does not match. We also add parentheses for grouping
expression and brackets to group literals.

\begin{example}

We can make a grammar for all sequence of \passthrough{\lstinline!A!}
using the rule \passthrough{\lstinline!<scream> ::= "A"+!}. If we want
to make a rule that prevent the use of the letter
\passthrough{\lstinline!z!} we can write
\passthrough{\lstinline!<no-sleep> ::= \~"z"!}.

\end{example}

\hypertarget{tools-for-text-analysis}{%
\subsection{Tools for text analysis}\label{tools-for-text-analysis}}

A regular grammar is static, it is set once and for all and will always
produce the same language. In order to be more flexible we need to talk
about dynamic grammars and their associated tools and explain our choice
of grammatical framework.

\begin{figure}
\hypertarget{fig:parser}{%
\centering
\includegraphics{graphics/parser.svg}
\caption{Process of a parser while analysing text}\label{fig:parser}
}
\end{figure}

One of the main tools for both static and dynamic grammar is a parser.
It is the program that will decode the input into a \emph{syntax tree}.
This process is detailed in figure~\ref{fig:parser}. To do that it first
scans the input text for matching \emph{tokens}. Tokens are akin to
words and are the data unit at the lexical level. Then the tokens are
matched against production rules of the parser (usually in the form of a
grammar). The matching of a rule will add an entry into the resulting
tree that is akin to the hierarchic grammatical description of a
sentence (e.g.~proposition, complement, verb subject, etc).

Most of the time, a parser will be used with an \emph{evaluator}. This
component transform the sytax tree into another similarly structured
result. It can be a storage inside objects or memory, or compiled into
another format, or even just for syntax coloration. Since a lot of usage
requires the same kind of function, a new kind of tool emerged to make
the creation of a compiler simpler. We call those tools
compiler-compilers or parser generators (Paulson
\protect\hyperlink{ref-paulson_semanticsdirected_1982}{1982}). They take
a grammar description as input and gives the program of a compiler of
the generated language as an output. Figure~\ref{fig:compiler}, explains
how both the generation and resulting program work. Each of them uses a
parser linked to an \emph{evaluator}. In the case of a
compiler-compiler, the evaluator is actually a compiler process. It will
transform the syntax tree of the grammar into executable code. This code
is the generated compiler and is subject to our interest in this case.

\begin{figure}
\hypertarget{fig:compiler}{%
\centering
\includegraphics{graphics/compiler-compiler.svg}
\caption{Illustration of the meta-process of compiler
generation}\label{fig:compiler}
}
\end{figure}

\hypertarget{dynamic-grammar}{%
\subsection{Dynamic Grammar}\label{dynamic-grammar}}

For dynamic grammar, these tools can get more complicated. The most
straightforward way to make a parser able to handle a dynamic grammar is
to introduce code in the rule handling that will tweak variables
affecting the parser itself (Souto \emph{et al.}
\protect\hyperlink{ref-souto_dynamic_1998}{1998}). This allows for
handling context in CFG without needing to rewrite the grammar.

Another kind of dynamic grammar is grammar that can modify themselves.
In order to do this a grammar is valuated with reified objects
representing parts of itself (Hutton and Meijer
\protect\hyperlink{ref-hutton_monadic_1996}{1996}). These parts can be
modified dynamically by rules as the input gets parsed (Renggli \emph{et
al.} \protect\hyperlink{ref-renggli_practical_2010}{2010}; Alessandro
and Piumarta \protect\hyperlink{ref-alessandro_ometa_2007}{2007}).
Re-using our prior illustration we can show in
figure~\ref{fig:dynamic-parser}, \marginpar{%
\includegraphics{/tmp/tmp61fh82uq}
\captionof{figure}{Illustration of the dynamic grammar modification}\label{fig:dynamic-parser}
}the particularity of this type of grammar. This approach uses Parsing
Expression Grammars (PEG)(Ford
\protect\hyperlink{ref-ford_parsing_2004}{2004}) with Packrat parsing
that backtracks by ensuring that each production rule in the grammar is
not tested more than once against each position in the input stream
(Ford \protect\hyperlink{ref-ford_packrat_2002}{2002}). While PEG is
easier to implement and more efficient in practice than their classical
counterparts (Loff \emph{et al.}
\protect\hyperlink{ref-loff_computational_2018}{2018}; Henglein and
Rasmussen \protect\hyperlink{ref-henglein_peg_2017}{2017}), it offsets
the computation load in memory making it actually less efficient in
general (Becket and Somogyi
\protect\hyperlink{ref-becket_dcgs_2008}{2008}).

Some tools actually just infer entire grammars from inputs and software
(Höschele and Zeller \protect\hyperlink{ref-hoschele_mining_2017}{2017};
Grünwald \protect\hyperlink{ref-grunwald_minimum_1996}{1996}). However,
these kinds of approaches require a lot of input data to perform well.
They also simply provide the grammar after expensive computations.

My system uses a grammar, composed of classical rules and is extended
using meta-rules that activates once the classical grammar fails.

\hypertarget{description-logics}{%
\section{Description Logics}\label{description-logics}}

On of the most standard and flexible way of representing knowledge is by
using ontologies. They are based mostly on the formalism of Description
Logics (DL) (Krötzsch \emph{et al.}
\protect\hyperlink{ref-krotzsch_description_2013}{2013}). It is based on
the notion of classes (or types) as a way to make the knowledge
hierarchically structured. A class is a set of individuals that are
called instances of the classes. Classes got the same basic properties
as sets but can also be constrained with logic formula. Constraints can
be on anything about the class or its individuals. Knowledge is also
encoded in relations that are predicates over attributes of individuals.

It is common when using DLs to store statements into three boxes (Baader
\emph{et al.} \protect\hyperlink{ref-baader_description_2003}{2003}):

\begin{itemize}
\tightlist
\item
  The TBox for terminology (statements about types)
\item
  The RBox for rules (statements about properties) (Bürckert
  \protect\hyperlink{ref-burckert_terminologies_1994}{1994})
\item
  The ABox for assertions (statements about individual entities)
\end{itemize}

These are used mostly to separate knowledge about general facts
(intentional knowledge) from specific knowledge of individual instances
(extensional knowledge). The extra RBox is for ``knowhow'' or knowledge
about entity behavior. It restricts usages of roles (properties) in the
ABox. The terminology is often hierarchically ordered using a
subsumption relation noted \(\subseteq\). If we represent classes or
type as a set of individuals then this relation is akin to the subset
relation of set theory.

\begin{example}

In the classical genealogy example, the TBox can be a statement similar
to \(\text{Woman} = \text{Person} \cap \text{Female}\). This is a
reasoning about the concept hierarchy and is usually modeled at design
time.

The RBox is often not present in DL systems but have interesting
expressivity properties. For example it is possible to define that the
atomic role \(\textrm{gender}\) so that
\(\textrm{Person} \cap \forall \textrm{gender} \in \{\textrm{Male}, \textrm{Female}, \textrm{NonBinary}\}\).
This will enforce that every person in should have one of the three
genders exposed in the set.

The ABox is about instances like
\(\textrm{Female} \cup \textrm{Person}(\textrm{ALICE})\) stating that
Alice is a female and a person. This statement allows the system to
infer that \(\textrm{Woman}(\textrm{ALICE})\) by applying the rules of
the TBox and RBox.

\end{example}

There are several versions and extensions of DL. They all vary in
expressivity. Improving the expressivity of a DL system often comes at
the cost of less efficient inference engines that can even become
undecidable for some extensions of DL.

\hypertarget{ontologies-and-their-languages}{%
\section{Ontologies and their
Languages}\label{ontologies-and-their-languages}}

Most AI problem needs a way to represent knowledge. The classical way to
do so has been more and more specialized for each AI community. Every
domain uses its Domain Specific Language (DSL) that neatly fit the
specific use it is intended to do.

There was a time when the branch of AI wanted to unify knowledge
description under the banner of the ``semantic web''. This domain has
given numerous works on service composition that is very close to
hierarchical planning. (Rao \emph{et al.}
\protect\hyperlink{ref-jinghairao_logicbased_2004}{2004})

From numerous works, a repeated limitation of the ``semantic web'' seems
to come from the languages used (Dornhege \emph{et al.}
\protect\hyperlink{ref-dornhege_semantic_2012}{2012},
@hirankitti\_metareasoning\_2011). In order to guarantee performance of
generalist inference engines, these languages have been restricted so
much that they became quite complicated to use and quickly cause huge
amounts of recurrent data to be stored because of some forbidden
representation that will push any generalist inference engine into
undecidability.

The most basic of these languages is perhaps RDF Turtle (Beckett and
Berners-Lee \protect\hyperlink{ref-beckett_turtle_2011}{2011}). It is
based on triples with an XML syntax and has a graph as its knowledge
structure (Klyne and Carroll
\protect\hyperlink{ref-klyne_resource_2004}{2004}). A RDF graph is a set
of RDF triples \(\langle sub, pro, obj \rangle\) which fields are
respectively called subject, property and object. It can also be seen as
a partially labeled directed graph \((V, E)\) with \(V\) being the set
of RDF nodes and \(E\) being the set of edges. This graph also comes
with an incomplete label relation that associates a unique string called
a Uniform Resource Identifier (URI) to most nodes. Nodes without an URI
are called blank nodes. It is important that, while not named, blank
nodes have a distinct internal identifier from one another that allows
to differentiate them.

Built on top of RDF, the W3C recommended another standard called OWL
(W3C \protect\hyperlink{ref-w3c_owl_2012}{2012}). It adds the ability to
have hierarchical classes and properties along with more advanced
description of their arrity and constraints. OWL is, in a way, more
expressive than RDF (Van Harmelen \emph{et al.}
\protect\hyperlink{ref-vanharmelen_handbook_2008}{2008}, 1,p825). It
adds most formalism used in knowledge representation and is widely used
and interconnected. OWL comes in three versions: OWL Lite, OWL DL and
OWL Full. The lite version is less advanced but its inference is
decidable, OWL DL contains all notions of DL and the full version
contains all features of OWL but is strongly undecidable.

\begin{lstlisting}[language=XML, caption={Example of RDF ontology from ^[@w3c_examples_2004]}, label=lst:rdf]
  ex:ontology rdf:type owl:Ontology .
  ex:name rdf:type owl:DatatypeProperty .
  ex:author rdf:type owl:ObjectProperty .
  ex:Book rdf:type owl:Class .
  ex:Person rdf:type owl:Class .

  _:x rdf:type ex:Book .
  _:x ex:author _:x1 .
  _:x1 rdf:type ex:Person .
  _:x1 ex:name "Fred"^^xsd:string .
\end{lstlisting}

The expressivity can also come from a lack of restriction. If we allow
some freedom of expression in RDF statements, its inference can quickly
become undecidable (Motik
\protect\hyperlink{ref-motik_properties_2007}{2007}). This kind of
extremely permissive language is better suited for specific usage for
other branches of AI. Even with this expressivity, several works still
deem existing ontology system as not expressive enough, mostly due to
the lack of classical constructs like lists, parameters and quantifiers
that don't fit the triple representation of RDF.

One of the ways which have been explored to overcome these limitations
is by adding a 4\textsuperscript{th} field in RDF. This field is meant
for context and annotations. This field is used for information about
any statement represented as a triple, such as access rights,
probabilities, or most of the time the source of the data (Tolksdorf
\emph{et al.} \protect\hyperlink{ref-tolksdorf_semantic_2004}{2004}).
One of the other uses of the fourth field of RDF is to reify statements
(Hernández \emph{et al.}
\protect\hyperlink{ref-hernandez_reifying_2015}{2015}). Indeed by
identifying each statement, it becomes possible to efficiently form
statements about statements.

Reifying isn't the only way to express reflexivity in ontologies. In the
work of Toro \emph{et al.}
(\protect\hyperlink{ref-toro_reflexive_2008}{2008}), the solution
explored is to encode querries into the ontology. This allows for querry
caching and certainly adds to the expressivity. However, encoding
querries will only be relevant once querries are already executed.

A completely different approach is done by Hart and Goertzel
(\protect\hyperlink{ref-hart_opencog_2008}{2008}) in his framework for
Artificial General Intelligence (AGI) called OpenCog. The structure of
the knowledge is based on a rhizome, a collection of trees, linked to
each other. This structure is called Atomspace. Each vertex in the tree
is an atom, leaf vertexes are nodes, the others are links. Atoms are
immutable, indexed objects. They can be given values that can be dynamic
and, since they are not part of the rhizome, are an order of magnitude
faster to access. Atoms and values alike are typed.

The goal of such a structure is to be able to merge concepts from widely
different domains of AI. The major drawback being that the whole system
is very slow compared to pretty much any domain specific software.

In my system, a similar structure is used but along with ontology
orriented notions.

\hypertarget{self}{%
\section{Self}\label{self}}

As we have seen, the most used knowledge description systems (e.g.~RDF,
Ontologies and relational databases) have a common drawback: they are
static. This means that they are created to be optimized for a specific
use case, or gets general at the cost of efficiency. The main issue is
that such system are unable to adapt to the use case by themselves. To
fix this issue, a new knowledge representation model must be presented.
The goal is to make a minimal language framework that can adapt to its
use to become as specific as needed. If it becomes specific is must
start from a generic base. Since that base language must be able to
evolve to fit the most cases possible, it must be neutral and simple.

To summarize, that framework must maximize the following criteria:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Neutral}: Must be independent from preferences and be
  localization.
\item
  \textbf{Permissive}: Must allow as many data representation as
  possible.
\item
  \textbf{Minimalist}: Must have the minimum number of base axioms and
  as little native notions as possible.
\item
  \textbf{Adaptive}: Must be able to react to user input and be as
  flexible as possible.
\end{enumerate}

In order to respect these requirements, we developed a framework for
knowledge description. This Structurally Expressive Language Framework
(SELF) is our answer to these criteria. SELF is inspired by RDF Turtle
and Description Logic.

\hypertarget{knowledge-structure}{%
\subsection{Knowledge Structure}\label{knowledge-structure}}

SELF extends the RDF graphs by adding another label to the edges of the
graph to uniquely identify each statement. This basically turns the
system into a quadruple storage even if this forth field is transparent
to the user.

\begin{axiom}[Structure]\label{axi:structure}

A SELF graph is a set of statements that transparently include their own
identity. The closest representation of the underlying structure of SELF
is as follows: \[
g_{\bb{U}} = (\bb{U}, S) :
  S = \left\{ s = \langle sub,pro,obj \rangle:
    s \in \cal{D} \vdash s \land \cal{D} \right\}
\]

with:

\begin{itemize}
\tightlist
\item
  \(sub, obj \in \bb{U}\) being entities representing the \emph{subject}
  and \emph{object} of the \emph{statement} \(s\),
\item
  \(pro \in P\) being the \emph{property} of the statement \(s\),
\item
  \(\cal{D} \subset S\) is the \emph{domain} of the \emph{world}
  \(g_{\bb{U}}\),
\item
  \(S, P \subset \bb{U}\) with \(S\) the set of statements and \(P\) the
  set of properties,
\end{itemize}

\end{axiom}

This means that the world \(g_{\bb{U}}\) is a graph with the set of
entities \(\bb{U}\) as vertices and the set of statements \(S\) as
edges. This model also suppose that every statement \(s\) must be true
if they belong to the domain \(\cal{D}\). This graph is a directed
3-uniform hypergraph.

Since sheaves are a representation of hypergraphs, we can encode the
structure of SELF into a sheaf-like form. Each seed is a statement, the
germ being the statement vertex. It is always accompanied of an incoming
connector (its subject), an outgoing connector (its object) and a
non-directed connector (its property). The sections are domains and must
be coherent. Each statement, along with its property, makes a stalk as
illustrated in figure~\ref{fig:selfgraph}.

\begin{figure}
\hypertarget{fig:selfgraph}{%
\centering
\includegraphics{graphics/self_graph.svg}
\caption{Projection of a statement from the SELF to RDF
space.}\label{fig:selfgraph}
}
\end{figure}

The difference with a sheaf is that the projection function is able to
map the pair statement-property into a labeled edge in its projection
space. We map this pair into a classical labeled edge that connects the
subject to the object of the statement in a directed fashion. This
results in the projected structure being a correct RDF graph.

\hypertarget{consequences}{%
\subsubsection{Consequences}\label{consequences}}

The base knowledge structure is more than simply convenience. The fact
that statements have their own identity, changes the degrees of freedom
of the representation. RDF has a way to represent reified statements
that are basically blank nodes with properties that are related to
information about the subject, property and object of a designated
statement. The problem is that such statements are very differently
represented and need 3 regular statements just to define. Using the
fourth field, it becomes possible to make statements about \emph{any}
statements. It also becomes possible to express modal logic about
statements or to express, various traits like the probability or the
access rights of a statement.

The knowledge structure holds several restrictions on the way to express
knowledge. As a direct consequence, we can add several theorems to the
logic system underlying SELF. The axiom of \nameref{axi:structure} is
the only axiom of the system.

\begin{theorem}[Identity]\label{theo:identity}

Any entity is uniquely distinct from any other entity.

\end{theorem}

This theorem comes from the axiom of \nameref{axi:extensionality} of
ZFC. Indeed it is stated that a set is a unordered collection of
distinct objects. Distinction is possible if and only if intrinsic
identity is assumed. This notion of identity entails that a given entity
cannot change in a way that would alter its identifier.

\begin{theorem}[Consistency]\label{theo:consistency}

Any statement in a given domain is consistent with any other statements
of this domain.

\end{theorem}

Consistency comes from the need for a coherent knowledge system and is
often a requirement of such constructs. This theorem also is a
consequence of the axiom of \nameref{axi:structure}:
\(s \in \cal{D} \vdash s \land \cal{D}\).

\begin{theorem}[Uniformity]\label{theo:uniformity}

Any object in SELF is an entity. Any relations in SELF are restricted to
\(\bb{U}\).

\end{theorem}

This also means that all native relations are closed under \(\bb{U}\).
This allows for a uniform knowledge database.

\hypertarget{sec:nativeprop}{%
\subsubsection{Native Properties}\label{sec:nativeprop}}

In the following, we suppose all notions from previous chapter. The
difference is that we define and use only a subset of the functions
defined in the SELF formalism. In relation to the theory of SELF, we use
the functional theory previously defined as the underlying formalism.

\textbf{FIXME: Case variation for crossref}

Theorem~\ref{theo:identity} lead to the need for two native properties
in the system : \emph{equality} and \emph{name}.

The \textbf{equality relation} \(= : \bb{U} \to \bb{U}\), behaves like
the classical operator. Since the knowledge database will be expressed
through text, we also need to add an explicit way to identify entities.
This identification is done through the \textbf{name relation}
\(\nu: \bb{U} \to L_{String}\) that affects a string literal to some
entities. This lead us to introduce literals into SELF that is also
entities that have a native value.

The axiom of \nameref{axi:structure} puts a type restriction on
property. Since it compartments \(\bb{U}\) using various named subsets,
we must adequately introduce an explicit type system into SELF. That
type system requires a \textbf{type relation} (named using the colon)
\(: : \bb{U} \to T\). That relation is complete as all entities have a
type. Theorem~\ref{theo:uniformity} causes the set of entities to be
universal. Type theory, along with Description Logic (DL), introduces a
\textbf{subsumption relation} \(\subseteq : T \to T\) as a partial
ordering relation to the types. Since types can be seen as sets of
instances, we simply use the subset relation from set theory. In our
case, the entity type is the greatest element of the lattice formed by
the set of types with the subsumption relation \((T, \subseteq)\).

The theorem~\ref{theo:uniformity} also allows for some very interesting
meta-constructs. That is why we also introduce a signed \textbf{Meta
relation} \(\mu: \bb{U} \to D\) with \(\mu^\bullet = \bullet \mu\). This
allows to create domain from certain entities and to encapsulate domains
into entities. \(\mu^\bullet\) is for reification and \(\mu\) is for
abstraction. This Meta relation also allows to express value of
entities, like lists or various containers.

To fulfill the principle of adaptability and in order to make the type
system more useful, we introduce the \textbf{parameter relation}
\(\rho: \bb{U} \to \bb{U}\). This relation affects a list of parameters,
using the Meta relation, to some parameterized entities. This also
allows for variables in statements.

Since axiom of \nameref{axi:structure} gives the structure of SELF a
hypergraph shape, we must port some notions of graph theory into our
framework. Introducing the \textbf{statement relation}
\(\chi : S \to \bb{U}\) reusing the same symbol as for the adjacency and
incidence relation of graphs. This isn't a coincidence as this relation
has the same properties.

\begin{example}

Since statements are triplets and edges, \(s_0\) gives the subject of a
statement \(s\). Respectively, \(s_1\) and \(s_2\) give the property and
object of any statement. For adjacencies, \(\chi\) can give the set of
statements any entity is the object or subject of. For any property
\(pro\), the notation \(\chi(pro)\) gives the set of statements using
this property.

\end{example}

This allows us to port all the other notions of graphs using this
relation as a base.

In figure~\ref{fig:typerel}, we present all the native relations along
with their domains and most subsets of \(\bb{U}\).

\begin{figure}
\hypertarget{fig:typerel}{%
\centering
\includegraphics{graphics/self_structure.svg}
\caption{Venn diagram of subsets of \(\bb{U}\) along with their
relations. Dotted lines mean that the sets are defined a subset of the
wider set.}\label{fig:typerel}
}
\end{figure}

\hypertarget{syntax}{%
\subsection{Syntax}\label{syntax}}

Since we need to respect the requirements of the problem, the RDF syntax
cannot be used to express the knowledge. Indeed, RDF states native
properties as English nodes with a specific URI that isn't neutral. It
also isn't minimalist since it uses an XML syntax so verbose that it is
not used for most examples in the documents that defines RDF because it
is too confusing and complex (W3C
\protect\hyperlink{ref-w3c_rdf_2004a}{2004}\protect\hyperlink{ref-w3c_rdf_2004a}{a};
W3C
\protect\hyperlink{ref-w3c_rdf_2004}{2004}\protect\hyperlink{ref-w3c_rdf_2004}{b}).
The XML syntax is also quite restrictive and cannot evolve dynamically
to adapt to the usage.

We need to define a new language that has contradictive qualities. It
must be general, yet specific and minimalistic while expressive.

So the solution to the problem is to actually define two languages that
fit the criteria: one minimalist and one adaptive. The issue is that we
don't want the user to learn two languages and the second kind of
language must be very specific and that violates the principle of
neutrality we try to respect.

The only solution is to make a mechanism to adapt the language as it is
used. We start off with a simple framework that uses a grammar.

The description of \(\bb{g}_0\) is pretty straightforward: it mostly is
just a triple representation separated by whitespaces. The goal is to
add a minimal syntax consistent with the
axiom of \nameref{axi:structure}. In listing~\ref{lst:grammar}, we give
a simplified version of \(\bb{g}_0\). It is written in a pseudo-BNF
fashion, which is extended with the classical repetition operators
\passthrough{\lstinline!*!} and \passthrough{\lstinline!+!} along with
the negation operator \passthrough{\lstinline!\~!}. All tokens have
names in uppercase. We also add the following rule modifiers:

\begin{itemize}
\tightlist
\item
  \passthrough{\lstinline!<\~name>!} are ignored for the parsing.
  However, the tokens are consumed and therefore acts like separators
  for the other rules.
\item
  \passthrough{\lstinline!<?name>!} are inferred rules and tokens. They
  play a key role for the process of derivation explained in
  section~\ref{sec:derivation}.
\end{itemize}

\begin{lstlisting}[caption={Simplified pseudo-BNF description for basic SELF.}, escapechar={$}, label=lst:grammar]
<~COMMENT: <INLINE: "//" (~["\n", "\r"])*>
| <BLOCK: "/*" (~["*/"])*> > //Ignored
<~WHITE_SPACE: " "|"\t"|"\n"|"\r"|"\f">
<LITERAL: <INT> | <FLOAT> | <CHAR> | <STRING>> //Java definition$\label{line:literal}$
<ID: <TYPE: <UPPERCASE>(<LETTERS>|<DIGITS>)* > $\label{line:uppercase}$
| <ENTITY:  <LOWERCASE>(<LETTERS>|<DIGITS>)*>
| <SYMBOL: (~[<LITERALS>, <LETTERS>, <DIGITS>])*>>

<worselfld> ::= <first> <statement>* <EOF>
<first> ::= <subject> <?EQUAL> <?SOLVE> <?EOS> $\label{line:first}$
<statement> ::= <subject> <property> <object> <EOS> $\label{line:statement}$
<subject> ::= <entity>
<property> ::= <ID> | <?meta_property>
<object> ::= <entity>
<entity> ::= <ID> | <LITERAL> | <?meta_entity>
\end{lstlisting}

In order to respect the principle of neutrality, the language must not
suppose of any regional predisposition of the user. There are few
exceptions for the sake of convenience and performance. The first
exception is that the language is meant to be read from left to right
and have an occidental biased
\passthrough{\lstinline!subject verb object!} triple description.
Another exception is for literals that use the same grammar as in
classical Java. This means that the decimal separator is the dot
(\passthrough{\lstinline!.!}). This concession is made for reasons of
simplicity and efficiency, but it is possible to define literals
dynamically in theory (see section~\ref{sec:peano}).

Even if sticking to the ASCII subset of characters is a good idea for
efficiency, SELF can work with UTF-8 and exploits the Unicode Character
Database (UCD) for its token definitions (Unicode Consortium
\protect\hyperlink{ref-unicodeconsortium_unicode_2018}{2018}\protect\hyperlink{ref-unicodeconsortium_unicode_2018}{b}).
This means that SELF comes keywords free and that the definition of each
symbol is left to the user. Each notion and symbol is inferred (with the
exception of the first statement which is closer to an imposed
configuration file).

In \(\bb{g}_0\), the first two token definitions are ignored. This means
that comments and white-spaces will act as separation and won't be
interpreted. Comments are there only for convenience since they do not
serve any real purpose in the language. It was arbitrarily decided to
use Java-style comments. White-spaces are defined against UCD's
definition of the separator category \passthrough{\lstinline!Z\&!} (see
Unicode Consortium
\protect\hyperlink{ref-unicodeconsortium_unicode_2018a}{2018}\protect\hyperlink{ref-unicodeconsortium_unicode_2018a}{a},
chap. 4).

\textbf{FIXME: Lines ref are borken}

Line~\ref{line:literal} uses the basic Java definition for liberals. In
order to keep the independence from any natural language, boolean
laterals are not natively defined (since they are English words).

Another aspect of that language independence is found starting at
line~\ref{line:uppercase} where the definitions of
\passthrough{\lstinline!<UPPERCASE>!},
\passthrough{\lstinline!<LOWERCASE>!},
\passthrough{\lstinline!<LETTERS>!} and
\passthrough{\lstinline!<DIGITS>!} are defined from the UCD
(respectively categories \passthrough{\lstinline!Lu!},
\passthrough{\lstinline!Ll!}, \passthrough{\lstinline!L\&!},
\passthrough{\lstinline!Nd!}). This means that any language's upper case
can be used in that context. For performance and simplicity reasons we
will only use ASCII in our examples and application.

The rule at line~\ref{line:first} is used for the definition of three
tokens that are important for the rest of the input.
\passthrough{\lstinline!<EQUAL>!} is the symbol for equality and
\passthrough{\lstinline!<SOLVE>!} is the symbol for the \emph{solution
quantifier} (and also the language pendant of \(\mu^\bullet\)). The most
useful token \passthrough{\lstinline!<EOS>!} is used as a statement
delimiter. This rule also permits the inclusion of other files if a
string literal is used as a subject. The underlying logic of this first
statement will be presented in section~\ref{sec:quantifier}. In the
following examples we will consider that
\passthrough{\lstinline!<EQUAL> ::= "="!},
\passthrough{\lstinline!<SOLVE> ::= "?"!} and
\passthrough{\lstinline!<EOS> ::= ";"!}.

At line~\ref{line:statement}, we can see one of the most defining
features of \(\bb{g}_0\): statements. The input is nothing but a set of
statements. Each component of the statements are entities. We defined
two specific rules for the subject and object to allow for eventual
runtime modifications. The property rule is more restricted in order to
guarantee the non-ambiguity of the grammar.

\hypertarget{sec:derivation}{%
\subsection{Dynamic Grammar}\label{sec:derivation}}

The syntax we described is only valid for \(\bb{g}_0\). As long as the
input is conforming to these rules, the framework keeps the minimal
behavior. In order to access more features, one needs to break a rule.
We add a second outcome to handling with violations :
\textbf{derivation}. There are several kinds of possible violations that
will interrupt the normal parsing of the input :

\begin{itemize}
\tightlist
\item
  Violations of the \passthrough{\lstinline!<first>!} statement rule :
  This will cause a fatal error.
\item
  Violations of the \passthrough{\lstinline!<statement>!} rule : This
  will cause a derivation if an unexpected additional token is found
  instead of \passthrough{\lstinline!<EOS>!}. If not enough tokens are
  present, a fatal error is triggered.
\item
  Violations of the secondary rules
  (\passthrough{\lstinline!<subject>!},
  \passthrough{\lstinline!<entity>!}, \ldots) : This will cause a fatal
  error except if there is also an excess of token in the current
  statement which will cause derivation to happen.
\end{itemize}

Derivation will cause the current input to be analyzed by a set of
meta-rules. The main restriction of these rules is given in
\(\bb{g}_0\): each statement must be expressible using a triple
notation. This means that the goal of the meta-rules is to find an
interpretation of the input that is reducible to a triple and to augment
\(\bb{g}_0\) by adding an expression to any
\passthrough{\lstinline!<meta\_*>!} rules. If the input has fewer than 3
entities for a statement then the parsing fails. When there is extra
input in a statement, there is a few ways the infringing input can be
reduced back to a triple.

\hypertarget{containers}{%
\subsubsection{Containers}\label{containers}}

The first meta-rule is to infer a container. A container is delimited
by, at least, a left and right delimiter (they can be the same symbol)
and an optional middle delimiter. We infer the delimiters using the
algorithm~\ref{alg:container}.

\begin{algorithm}\caption{Container meta-rule}\label{alg:container}\begin{algorithmic}[1]\Function{container}{Token current}
  \State \Call{lookahead}{current, EOS} \Comment{Populate all tokens of the statement}
  \ForAll{token in horizon}
    \If{token is a new symbol}
      delimiters.\Call{append}{token}
    \EndIf
  \EndFor
  \If{\Call{length}{delimiters} <2 }
    \If{\Call{coherentDelimiters}{horizon, delimiters[0]} }
      \State \Call{inferMiddle}{delimiters[0]} \Comment{New middle delimiter in existing containers}
      \State \Return Success
    \EndIf
    \State \Return Failure
  \EndIf
  \While{\Call{length}{delimiters} > 0}
    \ForAll{(left, middle, right) in \Call{sortedDelimiters}{delimiters}}\label{line:sorteddelim}
      \If{\Call{coherentDelimiters}{horizon, left, middle, right} }\label{line:coherentdelim}
        \State \Call{inferDelimiter}{left, right}
        \State \Call{inferMiddle}{middle} \Comment{Ignored if null}
        \State delimiters.\Call{remove}{left, middle, right}
        \Break
      \EndIf
    \EndFor
    \If{\Call{length}{delimiters} stayed the same }
      \Return Success
    \EndIf
  \EndWhile
  \State \Return Success
\EndFunction\end{algorithmic}\end{algorithm}

The function sortedDelimiters at line~\ref{line:sorteddelim} is used to
generate every ordered possibility and sort them using a few criteria.
The default order is possibilities grouped from left to right. All
coupled delimiters that are mirrors of each other following the UCD are
preferred to other possibilities.

Checking the result of the choice is very important. At
line~\ref{line:coherentdelim} a function checks if the delimiters allow
for triple reduction and enforce restrictions.

\begin{example}

For example, a property cannot be wrapped in a container (except if part
of parameters). This is done in order to avoid a type mismatch later in
the interpretation.

\end{example}

Once the inference is done, the resulting calls to inferDelimiter will
add the rules listed in listing~\ref{lst:container} to \(\bb{g}_0\).
This function will create a \passthrough{\lstinline!<container>!} rule
and add it to the definition of
\passthrough{\lstinline!<meta\_entity>!}. Then it will create a rule for
the container named after the UCD name of the left delimiter (searching
in the \passthrough{\lstinline!NamesList.txt!} file for an entry
starting with ``left'' and the rest of the name or defaulting to the
first entry). Those rules are added as a conjunction list to the rule
\passthrough{\lstinline!<container>!}. It is worthy to note that the
call to inferMiddle will add rules to the token
\passthrough{\lstinline!<MIDDLE>!} independently from any container and
therefore, all containers share the same pool of middle delimiters.

\begin{lstlisting}[caption={Rules added to the current grammar for handling the new container for parenthesis}, escapechar={$}, label=lst:container]
<meta_entity> ::= <container>
<container> :: = <parenthesis> | …
<parenthesis> ::= "(" [<naked_entity>] (<?MIDDLE> <naked_entity>)* ")"
<naked_entity> ::= <statement> | <entity>$\label{line:meta_statement}$
\end{lstlisting}

The rule at line~\ref{line:meta_statement} is added once and enables the
use of meta-statements inside containers. It is the language pendant of
the \(\mu\) relation, allowing to wrap abstraction in a safe way.

\begin{example}

If we parse the expression \passthrough{\lstinline!a = (b,c);!}, we
start by tokenizing it as
\passthrough{\lstinline!<ENTITY> <EQUAL> <SYMBOL><ENTITY><SYMBOL><ENTITY><SYMBOL> <EOS>!}
(ignoring whitespaces and comments). This means that the statement is 4
tokens too long to form a triple. This triggers a parsing error and then
an evaluation using meta-rules. All the \passthrough{\lstinline!<ID>!}
tokens are new symbols, but they don't have the same subtype. This means
that candidate delimiters are \passthrough{\lstinline!(!},
\passthrough{\lstinline!,!} and \passthrough{\lstinline!)!}. To Infer
the inference there's only one combination and the left delimiter and
right delimiters are found via their Unicode description. The comma is
left to be inferred as the middle delimiter. The grammar is rewritten
and the statement becomes
\passthrough{\lstinline!<ENTITY> <EQUAL> <container> <EOS>!} which is a
valid triple statement.

\end{example}

\hypertarget{parameters}{%
\subsubsection{Parameters}\label{parameters}}

If the previous rule didn't fix the parsing of the statement, we
continue with the following meta-rule. Parameters are extra containers
that are used after an entity. Every container can be used as
parameters. We detail the analysis in algorithm~\ref{alg:parameter}.

\begin{algorithm}\caption{Parameter meta-rule}\label{alg:parameter}\begin{algorithmic}[1]\Function{parameter}{Entity[] statement}
  \State reduced = statement
  \While{\Call{length}{reduced} >3}
    \For{i from 0 to \Call{length}{reduced} - 1}
      \If{\Call{name}{reduced[i]} not null and \\
      \Call{type}{reduced[i+1]} = Container and \\
      \Call{coherentParameters}{reduced, i}}
        \State param = \Call{inferParameter}{reduced[i], reduced[i+1]}
        \State reduced.\Call{remove}{reduced[i], reduced[i+1]}
        \State reduced.\Call{insert}{param, i} \Comment{Replace parameterized entity}
        \Break
      \EndIf
    \EndFor
    \If{\Call{length}{statement} stayed the same }
      \Return Success
    \EndIf
  \EndWhile
  \State \Return Failure
\EndFunction\end{algorithmic}\end{algorithm}

The goal is to match extra containers with the preceding named entity.
The container is then combined with the preceding entity into a
parameterized entity.

The call to inferParameter will add the rule in
listing~\ref{lst:parameter}, replacing
\passthrough{\lstinline!<?container>!} with the name of the container
used.

\begin{example}

In this case we have to parse \passthrough{\lstinline!f(x) = x;!}. If we
already have the parenthesis delimiter defined, it becomes
\passthrough{\lstinline!<ENTITY> <container> <EQUAL> <ENTITY> <EOS>!}.
Since the statement isn't a triple we execute the meta-rules. The
container rule finds no new symbols and fails. Then the parameter
meta-rule will reduce the statement by bounding the first entity to the
following container and mark it as a parameterized entity. This gives
\passthrough{\lstinline!<meta\_entity> <EQUAL> <ENTITY> <EOS>!}.

\end{example}

\begin{lstlisting}[language=Java, caption={Rules added to the current grammar for handling parameters}, escapechar={$}, label=lst:parameter]
<meta_entity> ::= <ID> <?container>
<meta_property> ::= <ID> <?container>
\end{lstlisting}

\hypertarget{operators}{%
\subsubsection{Operators}\label{operators}}

A shorthand for parameters is the operator notation. It allows to affect
a single parameter to an entity without using a container. This is
essentially syntaxic sugar: a feature that makes the language easier to
write. It is most used for special entities like quantifiers or
modifications. This is why, once used, the parent entity takes a
polymorphic type, meaning that type inference will not issue errors for
any usage of them. Details of the way the operators are reduced is
exposed in algorithm~\ref{alg:operator}.

\begin{algorithm}\caption{Operator meta-rule}\label{alg:operator}\begin{algorithmic}[1]\Function{operator}{Entity[] statement}
  \State reduced = statement
  \While{\Call{length}{reduced} >3}
    \For{i from 0 to \Call{length}{reduced} - 1}
      \If{\Call{$\nu$}{reduced[i]} not null and \\
      \Call{$\nu$}{reduced[i+1]} not null and \\
      (\Call{$\nu$}{reduced[i]} is a new symbol or \\
      reduced[i] has been parameterized before) and \\
      \Call{coherentOperator}{reduced, i}}
        \State op = \Call{inferOperator}{reduced[i], reduced[i+1]}
        \State reduced.\Call{remove}{reduced[i], reduced[i+1]}
        \State reduced.\Call{insert}{op, i} \Comment{Replace parameterized entity}
        \Break
      \EndIf
    \EndFor
    \If{\Call{length}{statement} stayed the same }
      \Return Success
    \EndIf
  \EndWhile
  \State \Return Failure
\EndFunction\end{algorithmic}\end{algorithm}

\textbf{TODO: More explanation}

From the call of inferOperator, comes new rules explicated in
listing~\ref{lst:operator}. The call also adds the operator entity to an
inferred token \passthrough{\lstinline!<OP>!}.

\begin{lstlisting}[language=Java, caption={Rules added to the current grammar for handling operators}, escapechar={$}, label=lst:operator]
<meta_entity> ::= <?OP> <ID>
<meta_property> ::= <?OP> <ID>
\end{lstlisting}

\begin{example}

With the input \passthrough{\lstinline"!x = 0;"} we parse
\passthrough{\lstinline!<SYMBOL> <ENTITY> <EQUAL> <LITERAL> <EOS>!}.
This cannot be a container since the new symbol is at the beginning
without any mirroring possible. It cannot be a parameter since no
container is present. But this will conclude with the operator meta-rule
as the new symbol precedes an entity. This becomes
\passthrough{\lstinline!<meta\_entity> <EQUAL> <LITERAL> <EOS>!} and
becomes a valid statement.

\end{example}

If all meta-rules fail, then the parsing fails and returns an error to
the user like in classical occurrences.

\hypertarget{contextual-interpretation}{%
\subsection{Contextual Interpretation}\label{contextual-interpretation}}

While parsing another important part of the processing is done after the
success of a grammar rule. The grammar in SELF is valuated, meaning that
each rule has to return an entity. A set of functions are used to then
populate the knowlege description system with the right entities or
retrieve an existing one that corresponds to what is being parsed.

When parsing, the rules \passthrough{\lstinline!<entity>!} and
\passthrough{\lstinline!<property>!} will trigger the creation or
retrieval of an entity. This mechanism will use the name of the entity
to retrieve an entity with the same name in a given scope. If no such
entity exists it is created and added to the current scope.

\hypertarget{naming-and-scope}{%
\subsubsection{Naming and Scope}\label{naming-and-scope}}

When parsing an entity by name, the system will first request for an
existing entity with the same name. If such an entity is retrieved, it
is returned instead of creating a new one. The validity of a name is
limited by the notion of scope.

A scope is the reach of an entity's direct influence. It affects the
naming relation by removing variable names. Scopes are delimited by
containers and statements. This local context is useful when wanting to
restrict the scope of the declaration of an entity. The main goal of
such restriction is to allow for a similar mechanism as the RDF
namespaces. This also makes the use of variables possible, akin to RDF
blank nodes.

The scope of an entity has three special values :

\begin{itemize}
\tightlist
\item
  Variable: This scope restricts the scope of the entity to only the
  other entities in its scope.
\item
  Local: This scope is temporarily bound to a given entity during the
  parsing. This scope is limited to the statement being interpreted.
\item
  Global: This scope means that the name has no scope limitation.
\end{itemize}

The scope of an entity also contains all its parent entities, meaning
all containers or statement the entity is part of. This is used when
choosing between the special values of the scope. The process is
detailed in algorithm~\ref{alg:scope}.

\begin{algorithm}\caption{Determination of the scope of an entity}\label{alg:scope}\begin{algorithmic}[1]\Function{inferScope}{Entity $e$}
  \State Entity[] reach = []
  \If{$:(e) = S$}
    \ForAll{$i \in \chi(e)$}
      reach.\Call{append}{\Call{inferVariable}{$i$}} \Comment{Adding scopes nested in statement $e$}
    \EndFor
  \EndIf
  \ForAll{$i \in \mu^\bullet(e)$}
    reach.\Call{append}{\Call{inferVariable}{$i$}} \Comment{Adding scopes nested in container $e$}
  \EndFor
  \If{$\exists \rho(e)$}
    \State Entity[] param = \Call{inferScope}{$\rho(e)$}
    \ForAll{$i \in $ param}
      param.\Call{remove}{\Call{inferScope}{$i$}} \Comment{Remove duplicate scopes from parameters}
    \EndFor
    \ForAll{$i \in $ param}
      reach.\Call{append}{\Call{inferVariable}{$i$}} \Comment{Adding scopes from paramters of $e$}
    \EndFor
  \EndIf
  \State $\Call{scope}{e} \gets $ reach
  \If{GLOBAL $\notin \Call{scope}{e}$}
    \Call{scope}{$e$} $\gets$ \Call{scope}{$e$} $\cup \{$LOCAL$\}$
  \EndIf
  \Return reach
\EndFunction

\Function{inferVariable}{Entity $e$}
  \State Entity[] reach = []
  \If{LOCAL $\in$ \Call{scope}{$e$}}
    \ForAll{$i \in$ \Call{scope}{$e$}}
      \If{$\exists e_p \in \bb{U} : \rho(p) = i$} \Comment{$e$ is already a parameter of another entity $e_p$}
        \State \Call{scope}{$e$} $\gets$ \Call{scope}{$e$} $\setminus \{$LOCAL$\}$
        \State \Call{scope}{$e_p$} $\gets$ \Call{scope}{$e_p$} $\cup$ \Call{scope}{$e$}
        \State \Call{scope}{$e$} $\gets$ \Call{scope}{$e$} $\cup \{$VARIABLE$, p\}$
      \EndIf
    \EndFor
    \State reach.\Call{append}{$e$}
    \State reach.\Call{append}{\Call{scope}{$e$}}
  \EndIf
  \Return reach
\EndFunction\end{algorithmic}\end{algorithm}

The process happens for each entity created or requested by the parser.
If a given entity is part of any other entity, the enclosing entity is
added to its scope. When an entity is enclosed in any entity while
already being a parameter of another entity, it becomes a variable since
it is referenced twice in the same statement.

\hypertarget{instanciation-identification}{%
\subsubsection{Instanciation
identification}\label{instanciation-identification}}

When a parameterized entity is parsed, another process starts to
identify if a compatible instance already exists. From
theorem~\ref{theo:identity}, it is impossible for two entities to share
the same identifier. This makes mandatory to avoid creating an entity
that is equal to an existing one. Given the order of which parsing is
done, it is not always possible to determine the parameter of an entity
before its creation. In that case a later examination will merge the new
entity onto the older one and discard the new identifier.

\hypertarget{structure-as-a-definition}{%
\subsection{Structure as a Definition}\label{structure-as-a-definition}}

The derivation feature on its own does not allow to define most of the
native properties. For that, one needs a light inference mechanism. This
mechanism is part of the default inference engine. This engine only
works on the principle of structure as a definition. Since all names
must be neutral from any language, that engine cannot rely on classical
mechanisms like configuration files with keys and values or predefined
keywords.

To use SELF correctly, one must be familiar with the native properties
and their structure or implement their own inference engine to override
the default one.

\hypertarget{sec:quantifier}{%
\subsubsection{Quantifiers}\label{sec:quantifier}}

In SELF quantifiers differ from their mathematical counterparts. The
quantifiers are special entities that are meant to be of a generic type
that matches any entities including quantifiers. There are infinitely
many quantifiers in SELF but they are all derived from a special one
called the \emph{solution quantifier}. We mentioned it briefly during
the definition of the grammar \(\bb{g}_0\). It is the language
equivalent of \(\mu^\bullet\) and is used to extract and evaluate
reified knowledge (see section~\ref{sec:nativeprop}).

\begin{example}

The statement \passthrough{\lstinline!bob is <SOLVE>(x)!} will give
either a default container filled with every value that the variable
\passthrough{\lstinline!x!} can take or if the value is unique, it will
take that value. If there is no value it will default to
\passthrough{\lstinline!<NULL>!}, the exclusion quantifier.

\end{example}

How are other quantifiers defined? We use a definition akin to Lindstöm
quantifiers (\protect\hyperlink{ref-lindstrom_first_1966}{1966}) which
is a generalization of counting quantifiers (Gradel \emph{et al.}
\protect\hyperlink{ref-gradel_twovariable_1997}{1997}). Meaning that a
quantifier is defined as a constrained range over the quantified
variable. We suppose five quantifiers as existing in SELF as native
entities.

\begin{itemize}
\tightlist
\item
  The \textbf{solution quantifier} \passthrough{\lstinline!<SOLVE>!}
  noted \(\textsection\) in classical mathematics, turns the expression
  into the possible value range of its variable. It is like replacing it
  by the natural expression ``those \(x\) that''.
\item
  The \textbf{universal quantifier} \passthrough{\lstinline!<ALL>!}
  behaves like \(\forall\) and forces the expression to take every
  possible value of its variable.
\item
  The \textbf{existential quantifier} \passthrough{\lstinline!<SOME>!}
  behaves like \(\exists\) and forces the expression to match \emph{at
  least one} arbitrary value for its variable.
\item
  The \textbf{uniqueness quantifier} \passthrough{\lstinline!<ONE>!}
  behaves like \(!\exists\) and forces the expression to match
  \emph{exactly one} arbitrary value for its variable.
\item
  The \textbf{exclusion quantifier} \passthrough{\lstinline!<NULL>!}
  behaves like \(\lnot \exists\) and forces the expression to not match
  the value of its variable.
\end{itemize}

The last four quantifiers are inspired from Aristotle's square of
opposition (D'Alfonso
\protect\hyperlink{ref-dalfonso_generalized_2011}{2011}).

\begin{figure}
\hypertarget{fig:aristotle}{%
\centering
\includegraphics{graphics/aristotle_square.svg}
\caption{Aristotle's square of opposition}\label{fig:aristotle}
}
\end{figure}

In SELF, quantifiers are not always followed by a quantified variable
and can be used as a value. In that case the variable is simply
anonymous. We use the exclusion quantifier as a value to indicate that
there is no value, sort of like \passthrough{\lstinline!null!} or
\passthrough{\lstinline!nil!} in programming languages.

\begin{example}

If we want to express the fact that a glass of water is not empty we can
write either \passthrough{\lstinline!glass contains \~(\~);!} or
\passthrough{\lstinline!glass \~(contains) \~!} with
\passthrough{\lstinline!<NULL> = \~!}. This shows that
\passthrough{\lstinline!<NULL>!} is used for negation and to indicate
the absence of value.

\marginpar{%
\includegraphics{/tmp/tmpwqnb8ia3}

}

\end{example}

This property is quite handy as it require only one symbol and allows
for complex constructs that are difficult to explain using available
paradigms.

In listing~\ref{lst:lang}, we present an example file that is meant to
define most of the useful native properties along with default
quantifiers.

\begin{lstlisting}[language=Java, caption={The default lang.w file.}, escapechar={$}, label=lst:lang]
* =? ;$\label{line:first}$
?(x) = x; //Optional definition
?~ = { };
?_ ~(=) ~;
?!_ = {_};$\label{line:endquantifier}$

(*e, !T) : (e :: T); *T : (T :: Type);$\label{line:typing}$
*T : (Entity / T);$\label{line:subsumption}$

:: :: Property(Entity, Type);
(___) :: Statement;
(~, !, _, *) :: Quantifier;
( )::Group;
{ }::Set;
[ ]::List;
< >::Tuple;
Collection/(Set,List,Tuple);
0 :: Integer; 0.0::Float;
'\0'::Character; ""::String;
Literal/(Boolean, Integer, Float, Character, String);

(*e, !(s::String)) : (e named s);$\label{line:naming}$
(*e(p), !p) : (e param p);$\label{line:param}$
*(s p o):(((s p o) subject s),((s p o) property p),((s p o) object o));$\label{line:incidence}$
\end{lstlisting}

At line~\ref{line:first}, we give the first statement that defines the
solution quantifier's symbol. The reason this first statement is shaped
like this is that global statements are always evaluated to be a true
statement. Since domains are sets of statements, this means that
anything equaling the solution quantifier at this level will be
evaluated as a domain. This is because the entity is a domain \textbf{by
structure}. If it is a single entity then it becomes synonymous to the
entire SELF domain and therefore contains everything. We can infer that
it becomes the universal quantifier.

If it is a string literal, then it must be either a file path or URL or
a valid SELF expression.

\begin{example}

Using the first statement, we can include external domains akin to the
\passthrough{\lstinline!import!} directive in Java. Writing
\passthrough{\lstinline!"path/lang.w" = ? ;!} as a first statement will
make the process parse the file located at
\passthrough{\lstinline!path/lang.w!} and insert it at this spot.

\end{example}

All statements up to line~\ref{line:endquantifier} are quantifiers
definitions. On the left side we got the quantifier symbol used as a
parameter to the solution quantifier using the operator notation. On the
right we got the domain of the quantifier. The exclusive quantifier has
as a range the empty set. For the existential quantifier we have only a
restriction of it not having an empty range. At last, the uniqueness
quantifier got a set with only one element matching its variable (noting
that anonymous variables doesn't match necessarily other anonymous
variables in the same statement).

In listing~\ref{lst:lang} the type hierarchy can be illustrated by the
figure~\ref{fig:hierarchy}. It consists of entities that are either
parameterized or not and that has a value or not.

\begin{figure}
\hypertarget{fig:hierarchy}{%
\centering
\includegraphics{graphics/hierarchy.svg}
\caption{Hierarchy of types in SELF}\label{fig:hierarchy}
}
\end{figure}

\textbf{TODO: Explain figure}

\hypertarget{inferring-native-properties}{%
\subsubsection{Inferring Native
Properties}\label{inferring-native-properties}}

All native properties can be inferred by structure using quantified
statements. Here is the structural definition for each of them:

\begin{itemize}
\tightlist
\item
  \(=\) (at line~\ref{line:first}) is the equality relation given in the
  first statement.
\item
  \(\subseteq\) (at line~\ref{line:subsumption}) is the first property
  to relate a particular type to all types. That type becomes the entity
  type.
\item
  \(\mu^\bullet\) (at line~\ref{line:first}) is the solution quantifier
  discussed above given in the first statement.
\item
  \(\mu\) is represented using containers.
\item
  \(\nu\) (at line~\ref{line:naming}) is the first property affecting a
  string literal uniquely to each entity.
\item
  \(\rho\) (at line~\ref{line:param}) is the first property to effect to
  all entities a possible parameter list.
\item
  \(:\) (at line~\ref{line:typing}) is the first property that matches
  every entity to a type.
\item
  \(\chi\) (at line~\ref{line:incidence}) is the first property to match
  for all statements.
\end{itemize}

We limit the inference to one symbol to eliminate ambiguities an prevent
accidental re-definition of native properties. This also improve
performance as the inference is stopped after finding a first matching
entity that can be used programatically using a single constant.

\hypertarget{extended-inference-mechanisms}{%
\subsection{Extended Inference
Mechanisms}\label{extended-inference-mechanisms}}

In this section we present the default inference engine. It is quite
limited since it is meant to be universal and the goal of SELF is to
provide a framework that can be used by specialists to define and code
exactly what tools they need.

Inference engines need to create new knowledge but this knowledge
shouldn't be simply merged with the explicit user provided domain. Since
this knowledge is inferred, it is not exactly part of the domain but
must remain consistent with it. This knowledge is stored in a special
scope dedicated to each inference engine. This way, inference engines
can use defeasible logic or have dynamic inference from any knowledge
insertion in real time.

\hypertarget{type-inference}{%
\subsubsection{Type Inference}\label{type-inference}}

Type inference works on matching types in statements. The main mechanism
consists in inferring the type of properties in a restrictive way.
Properties have a parameterized type with the type of their subject and
object. The goal is to make that type match the input subject and
object.

For that we start by trying to match the types. If the types differ, the
process tries to reduce the more general type against the lesser one
(subsumption-wise). If they are incompatible, the inference uses some
light defeasible logic to undo previous inferences. In that case the
types are changed to the last common type in the subsumption tree.

However, this may not always be possible. Indeed, types can be
explicitly specified as a safeguard against mistakes. If that's the
case, an error is raised and the parsing or knowledge insertion is
interrupted.

\hypertarget{instanciation}{%
\subsubsection{Instanciation}\label{instanciation}}

Another inference mechanism is instantiation. Since entities can be
parameterized, they can also be defined against their parameters. When
those parameters are variables, they allow entities to be instantiated
later.

Since entities are immutable, updating their instance can be quite
tricky. Indeed, parsing happens from left to right and therefore an
entity is often created before all the instantiation information are
available. Even harder are completion of definition in several separate
statements. In all cases, a new entity is created and then the inference
realize that it is either matching a previous definition and will need
to be merged with the older entity or it is a new instance and needs all
properties duplicated and instantiated.

This gives us two mechanisms to take into account: merging and
instanciating.

Merging is pretty straightforward: the new entity is replaced with the
old one in all of the knowledge graph. containers, parameterized
entities, quantifiers and statements must be duplicated with the correct
value and the original destroyed. This is a heavy and complicated
process but seemingly the only way to implement such a case with
immutable entities.

Instanciating is similar to merging but even more complicated. It starts
with computing a relation that maps each variable that needs replacing
with their grounded value. Then it duplicates all knowledge about the
parent entity while applying the replacement map.

\hypertarget{example}{%
\section{Example}\label{example}}

In the following section, a use case of the framework will be presented.
First we have to explain a few notions.

\hypertarget{modality-of-statements}{%
\subsection{Modality of Statements}\label{modality-of-statements}}

In the field of logic there exists one special flavor of it called
\emph{modal logic}. It lays the emphasis upon the qualifications of
statements, and especially the way they are interpreted. This is a very
appropriate example for SELF. The modality of a statement acts like a
modifier, it specifies a property regarding its plausibility, origin or
validity.

\begin{example}

In the figure~\ref{fig:gossip}, we present a case of three persons
gossiping, Alice, Becky and Carol. The presentation is inspired by the
work of Schwarzentruber
(\protect\hyperlink{ref-schwarzentruber_hintikka_2018}{2018}). Here is a
list of the statements in this example:

\begin{itemize}
\tightlist
\item
  Alice said to Becky that Carol should \emph{probably} change her style
  from \(C_1\) to \(C_2\).
\item
  Becky said to Alice that she finds the Carol's style \emph{usually}
  good.
\item
  Alice told Carol that Becky told her that she should \emph{sometimes}
  change her style to \(C_2\).
\end{itemize}

The following statement can be inferred:

\begin{itemize}
\tightlist
\item
  Carol \emph{possibly} thinks that Becky thinks that the style \(C_2\)
  is \emph{often} good.
\end{itemize}

\begin{figure}
\hypertarget{fig:gossip}{%
\centering
\includegraphics{graphics/gossip.svg}
\caption{Example of modal logic propositions: Alice gossips about what
Beatrice said about Claire}\label{fig:gossip}
}
\end{figure}

\end{example}

In the example, all modalities are \emph{emphasized}. One can notice an
interesting property of these statements in that they are about other
statements. This kind of description is called \emph{higher order
knowledge}.

\hypertarget{higher-order-knowledge}{%
\subsection{Higher order knowledge}\label{higher-order-knowledge}}

SELF is based on the ability to easily process higher order knowledge.
In that case the term \emph{order} refers to the level of abstraction of
a statement (Schwarzentruber
\protect\hyperlink{ref-schwarzentruber_hintikka_2018}{2018}). For such
usages, a hypergraph structures such as SELF is using is a clear
advantage in terms of expressivity and ease of manipulation of those
statements. This is due to the higher dimentionality of sheaves (and by
extension hypergraphs) that makes meta-statement as simple to express as
any other statement. This chain of abstractions using meta-statements is
where the higher order knowledge is encoded.

In the following listings, we present the previous example using RDF and
SELF to describe knowledge of the gossip.

\begin{lstlisting}
@prefix : <http://genn.io/self/gossip#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix xml: <http://www.w3.org/XML/1998/namespace> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@base <http://genn.io/self/gossip> .

<http://genn.io/self/gossip> rdf:type owl:Ontology ;
                              owl:imports rdf: .

:modality rdf:type owl:AnnotationProperty ;
          rdfs:range :Modality .

:told_a rdf:type owl:ObjectProperty .
:told_b rdf:type owl:ObjectProperty .
:told_c rdf:type owl:ObjectProperty .
:Modality rdf:type owl:Class .
:good rdf:type owl:NamedIndividual .
:is rdf:type owl:NamedIndividual ,
             rdf:Property .
:probably rdf:type owl:NamedIndividual ,
                   :Modality .
:s1 rdf:type owl:NamedIndividual ,
             rdf:Statement ;
    rdf:object :c2 ;
    rdf:predicate :worsethan ;
    rdf:subject :c ;
    :modality :probably .
:s2 rdf:type owl:NamedIndividual ;
    rdf:object :good ;
    rdf:predicate :is ;
    rdf:subject :c ;
    :modality :usually .
:s3 rdf:type owl:NamedIndividual ,
             rdf:Statement ;
    rdf:object :s4 ;
    rdf:predicate :told_a ;
    rdf:subject :b .
:s4 rdf:type owl:NamedIndividual ,
             rdf:Statement ;
    rdf:object :c2 ;
    rdf:predicate :should ;
    rdf:subject :c ;
    :modality :sometimes .
:should rdf:type owl:NamedIndividual ,
                 rdf:Property .
:sometimes rdf:type owl:NamedIndividual ,
                    :Modality .
:told_a rdf:type owl:NamedIndividual ,
                 rdf:Property .
:usually rdf:type owl:NamedIndividual ,
                  :Modality .
:worsethan rdf:type owl:NamedIndividual ,
                    rdf:Property .
:a rdf:type owl:NamedIndividual ;
   :told_b :s1 ;
   :told_c :s3 .
:b rdf:type owl:NamedIndividual ;
   :told_a :s2 .
:c rdf:type owl:NamedIndividual .
:c2 rdf:type owl:NamedIndividual .
\end{lstlisting}

\begin{lstlisting}
"lang.s" = ? ;
a told(b) probably(c worsethan ctwo);
b told(a) usually(c is good);
a told(c) (b told(a) sometimes(c should ctwo));
\end{lstlisting}

It is obvious that the SELF version is an order of magnitude more
concise than RDF to express modal logic. The 4 lines of SELF are
\textbf{equivalent} to the 62 lines of RDF. In the RDF version we use
the reified statements \passthrough{\lstinline!:s1!},
\passthrough{\lstinline!:s2!}, \passthrough{\lstinline!:s3!} and
\passthrough{\lstinline!:s4!} along with a
\passthrough{\lstinline!:modality!} annotation to express high order
knowledge and modalities. In SELF, everything is inferred by structure
and one can start exploiting their database right away.

\hypertarget{ch:color}{%
\chapter{General Planning Framework}\label{ch:color}}

When designing intelligent systems, an important feature is the ability
to make decisions and act accordingly. To act, one should plan ahead.
This is why the field of automated planning is being actively researched
in order to find efficient algorithms to find the best course of action
in any given situation. The previous chapter allowed to lay the basis of
knowledge representation. How knowledge about the planning domains are
represented is a main factor to take into account in order to conceive
most planning algorithm.

Automated planning really started being formally investigated after the
creation of the Stanford Research Institute Problem Solver (STRIPS) by
Fikes and Nilsson (\protect\hyperlink{ref-fikes_strips_1971}{1971}).
This is one of the most influential planners, not because of its
algorithm but because of its input language. Any planning system needs a
way to express the information related to the input problem. Any
language done for this purpose is called an \emph{action language}.
STRIPS will be mostly remembered for its eponymous action language that
is at the base of any modern derivatives.

All action languages are based on mainly two notions: \emph{actions} and
\emph{states}. A state is a set of \emph{fluents} that describe aspects
of the world modeled by the domain. Each action has a logic formula over
states that allows its correct execution. This requirement is called
\emph{precondition}. The mirror image of this notion is called possible
\emph{effects} which are logic formulas that are enforced on the current
state after the action is executed. The domain is completed with a
problem, most of the time specified in a separate file. The problem
basically contains two states: the \emph{initial} and \emph{goal}
states.

\hypertarget{sec:plan_example}{%
\section{Illustration}\label{sec:plan_example}}

To illustrate how automated planners works, we introduce a typical
planning problem called \textbf{block world}.

\begin{example}

In this example, a robotic grabbing arm tries to stack blocks on a table
in a specific order. The arm is only capable of handling one block at a
time. We suppose that the table is large enough so that all the blocks
can be put on it without any stacks. Figure~\ref{fig:blockworld}
illustrates the setup of this domain.

\begin{figure}
\hypertarget{fig:blockworld}{%
\centering
\includegraphics{graphics/blockworld.svg}
\caption{The block world domain setup.}\label{fig:blockworld}
}
\end{figure}

The possible actions are \passthrough{\lstinline!pickup!},
\passthrough{\lstinline!putdown!}, \passthrough{\lstinline!stack!} and
\passthrough{\lstinline!unstack!}. There are at least three fluents
needed:

\begin{itemize}
\tightlist
\item
  one to state if a given block is \passthrough{\lstinline!down!} on the
  table,
\item
  one to specify which block is \passthrough{\lstinline!held!} at any
  moment and
\item
  one to describe which block is stacked \passthrough{\lstinline!on!}
  which block.
\end{itemize}

We also need a special block to state when
\passthrough{\lstinline!noblock!} is held or on top of another block.
This block is a constant.

\end{example}

The knowledge we just described is called \emph{planning domain}.

In that example, the initial state is described as stacks and a set of
blocks directly on the table. The goal state is usually the
specification of one or many stacks that must be present on the table.
This part of the description is called \emph{planning problem}.

In order to solve it we must find a valid sequence of actions called a
\emph{plan}. If this plan can be executed in the initial state and
result in the goal state, it is called a \emph{solution} of the planning
problem. To be executed, each action must be done in a state satisfying
its precondition and will alter that state according to its effects. A
plan can be executed if all its actions can be executed in the sequence
of the plan.

\begin{example}

For example, in the block world domain we can have an initial state with
the \(blockB\) on top of \(blockA\) and the \(blockC\) being on the
table. In figure~\ref{fig:plan}, we give a plan solution to the problem
consisting of having the stack
\(\langle blockA, blockB, blockC \rangle\) from that initial state.

\begin{figure}
\hypertarget{fig:plan}{%
\centering
\includegraphics{graphics/plan1.svg}
\caption{An example of a solution to a planning problem with a goal that
requires three blocks stacked in alphabetical order.}\label{fig:plan}
}
\end{figure}

\end{example}

Every automated planner aims to find at least one such solution in any
way shape or form in the least amount of time with the best plan
quality. The quality of a plan is often measured by how hard it is to
execute, whether by its execution time or by the resources needed to
accomplish it. This metric is often called \emph{cost} of a plan and is
often simply the sum of the costs of its actions.

Automated planning is very diverse. A lot of paradigms shift the
definition of the domain, actions and even plan to widely varying
extents. This is the reason why making a general planning formalism was
deemed so hard or even impossible:

\begin{quote}
``\emph{It would be unreasonable to assume there is one single compact
and correct syntax for specifying all useful planning problems.}''
\hfill Sanner (\protect\hyperlink{ref-sanner_relational_2010}{2010})
\end{quote}

Indeed, the block world example domain we give is mostly theoretical
since there is infinitely more subtlety into this problem such as
mechatronic engineering, balancing issues and partial ability to observe
the environment and predict its evolution as well as failure in the
execution. In our example, we didn't mention the misplaced \(blockD\)
that could very well interfere with any execution in unpredictable ways.
This is why so many planning paradigms exist and why they are all so
diverse: they try to address an infinitely complex problem, one
sub-problem at a time. In doing so we lose the general view of the
problem and by simply stating that this is the only way to resolve it we
close ourselves to other approaches that can become successful. Like
once said:

\begin{quote}
``\emph{The easiest way to solve a problem is to deny it exists.}''
\hfill Asimov (\protect\hyperlink{ref-asimov_gods_1973}{1973})
\end{quote}

However, in the next section we aim to create such a general planning
formalism. The main goal is to design the automated planning community
with a general unifying framework it so badly needs.

\hypertarget{formalism}{%
\section{Formalism}\label{formalism}}

In this section, a general formalism of automated planning is proposed.
The goal is to explain what is planning and how it works. First we must
express the knowledge domain formalism, then we describe how problems
are represented and lastly how a general planning algorithm can be
envisioned.

\hypertarget{planning-domain}{%
\subsection{Planning domain}\label{planning-domain}}

In order to conceive a general formalism for planning domains, we base
its definition on the formalism of SELF. This means that all parts of
the domain must be a member of the universe of discourse \(\bb{U}\).

\hypertarget{fluents}{%
\subsubsection{Fluents}\label{fluents}}

First, we need to define the smallest unit of knowledge in planning, the
fluents.

\begin{definition}[Fluent]

A planning fluent is a predicate \(f \in F\).

Fluents are signed. Negative fluents are noted \(\neg f\) and behave as
a logical complement. We do not use the closed world hypothesis: fluents
are only satisfied when another compatible fluent is provided.

\end{definition}

The name ``fluent'' comes from their fluctuating value. Indeed the truth
value of a fluent is meant to vary with time and specifically by acting
on it. In this formalism we represent fluents using either parameterized
entities or using statements for binary fluents.

\begin{example}

In our example we have four predicates. They can form countless fluents
like \(held(no-block)\), \(on(blockA, blockB)\) or
\(\neg down(blockA)\). Their when expressing a fluent we suppose its
truth value is \(\top\) and denote falsehood using the negation
\(\neg\).

\end{example}

\hypertarget{states}{%
\subsubsection{States}\label{states}}

When expressing states, we need a formalism to express sets of fluents
as formulas.

\begin{definition}[State]

A state is a logical formula of fluents. Since all logical formulas can
be reduced to a simple form using only \(\land\), \(\lor\), and
\(\neg\), we can represent states as \emph{and/or trees}. This means
that the leaves are fluents and the other nodes are states. We note
states using small squares \(\state\) as it is often the symbol used in
the representation of automates and grafcets.

\end{definition}

\begin{example}

In the domain block world, we can express a couple of states as :

\begin{itemize}
\tightlist
\item
  \(\state_1 = held(noblock) \land on(blockA, blockB) \land down(blockC)\)
\item
  \(\state_2 = held(blockC) \land down(blockA) \land down(blockB)\)
\end{itemize}

In such a case, both state \(\state_1\) and \(\state_2\) have their
truth value being the conjunction of all their fluents. We can express a
disjunction in the following way: \(\state_3 = \state_1 \lor \state_2\).
In that case \(\state_3\) is the root of the and/or tree and all its
direct children are or vertices. The states \(\state_1\) and
\(\state_2\) have their children as \emph{and vertices}. All the leaves
are fluents.

\begin{figure}
\hypertarget{fig:state_tree}{%
\centering
\includegraphics{graphics/and-or-state.svg}
\caption{Example of a state encoded as an and/or
tree.}\label{fig:state_tree}
}
\end{figure}

\end{example}

\hypertarget{verification-and-binding-constraints}{%
\subsubsection{Verification and binding
constraints}\label{verification-and-binding-constraints}}

When planning, there are two operations that are usually done on states:
verify if a precondition fits a given state and then apply the effects
of an action. In our model we consider preconditions and effects as
states.

The verification is the operation \(\state_\pre \models \state\) that
has either no value when the verification fails or a binding map for
variables and fluents with their respective values.

\marginpar{%
\includegraphics{/tmp/tmpqzqgml66}
\captionof{figure}{Example of verification and/or tree with binding constraints}
}

The algorithm is a regular and/or tree exploration and evaluation
applied on the state \(\cstate = \state_{\pre} \land \state\). During
the valuation, if an inconsistency is found then the algorithm returns
nothing. Otherwise, at each node of the tree, the algorithm will
populate the binding map and verify if the truth value of the node holds
under those constraints. All quantified variables are also registered in
the binding map to enforce coherence in the root state. If the node is a
state, the algorithm recursively applies until it reaches fluents. Once
\(\cstate\) is valuated as true, the binding map is returned.

\begin{example}

Using previously defined example states \(\state_{1,2,3}\), and adding
the following:

\begin{itemize}
\tightlist
\item
  \(\state_4(x) = \{held(noblock), down(x)\}\) and
\item
  \(\state_5(y) = \{held(y), \neg down(y)\}\),
\end{itemize}

We can express a few examples of fluent verification:

\begin{itemize}
\tightlist
\item
  \(held(noblock) \models held(x) = \{x=noblock\}\)
\item
  \(\neg held(x) \models held(x) = \emptyset\)
\end{itemize}

\end{example}

\hypertarget{application}{%
\subsection{Application}\label{application}}

Once the verification is done, the binding map is kept until the planner
needs to apply the action to the state. The application of an effect
state is noted \(\state_{\eff}(\state) = \state'\) and is very similar
to the verification. The algorithm will traverse the state \(\state\)
and use the binding map to force the values inside it. The binding map
is previously completed using \(\state_{\eff}\) to enforce the
application of its new value in the current state. This leads to
changing the state \(\state\) progressively into \(\state'\) and the
application algorithm will return this state.

\marginpar{%
\includegraphics{/tmp/tmpc16qvmfa}
\captionof{figure}{Example of application of a state with the updated binding constraints}
}

\hypertarget{actions}{%
\subsubsection{Actions}\label{actions}}

Actions are the main mechanism behind automated planning, they describe
what can be done and how it can be done.

\begin{definition}[Action]\label{def:action}

An action is a parameterized tuple
\(a(args)=\langle \pre, \eff, \gamma, ¢, d, \proba, \plans \rangle\)
where:

\begin{itemize}
\tightlist
\item
  \(\pre\) and \(\eff\) are states that are respectively the
  \textbf{preconditions and the effects} of the action.
\item
  \(\gamma\) is the state representing the \textbf{constraints}.
\item
  \(¢\) is the intrinsic \textbf{cost} of the action.
\item
  \(d\) is the intrinsic \textbf{duration} of the action.
\item
  \(\proba\) is the prior \textbf{probability} of the action succeeding.
\item
  \(\plans\) is a set of \textbf{methods} that decompose the action into
  smaller simpler ones.
\end{itemize}

\end{definition}

Operators take many names in difference planning paradigm: actions,
steps, tasks, etc. In our case we call operators, all fully lifted
actions and actions are all the possible instances (including
operators).

In order to be more generalist, we allow in the constraints description,
any time constraints, equalities or inequalities, as well as
probabilistic distributions. These constraints can also express derived
predicates. It is even possible to place arbitrary constraints on order
and selection of actions.

Actions are often represented as state operators that can be applied in
a given state to alter it. The application of actions is done by using
the action as a relation on the set of states
\(a : \states \to \states\) defined as follows:

\[a(\state) =
\begin{cases}
  \emptyset,& \text{if } \pre \models \state =\emptyset\\
  \eff(\state),& \text{using the binding map otherwise}
\end{cases}\]

\begin{example}

A useful action we can define from previously defined states is the
following:

\[pickup(x) = \langle \state_4(x), \state_5(x), (x: Block), 1.0¢, 3.5s, 75\%, \emptyset \rangle\]

That action can pick up a block \(x\) in \(3.5\) seconds using a cost of
\(1.0\) with a prior success probability of \(75\%\).

\end{example}

\hypertarget{domain}{%
\subsubsection{Domain}\label{domain}}

\begin{figure}
\hypertarget{fig:color}{%
\centering
\includegraphics{graphics/color_structure.svg}
\caption{Venn diagram extended from the one from SELF to add all
planning knowledge representation.}\label{fig:color}
}
\end{figure}

The planning domain specifies the allowed operators that can be used to
plan and all the fluents they use as preconditions and effects.

\begin{definition}[Domain]

A planning domain \(\dom\) is a set of \textbf{operators} which are
fully lifted \emph{actions}, along with all the relations and entities
needed to describe their preconditions and effects.

\end{definition}

\begin{example}

In the previous examples the domain was named block world. It consists
in four actions: \(pickup, putdown, stack\) and \(unstack\). Usually the
domain is self contained, meaning that all fluents, types, constants and
operators are contained in it.

\end{example}

\hypertarget{planning-problem}{%
\subsection{Planning problem}\label{planning-problem}}

The aim of an automated planner is to find a plan to satisfy the goal.
This plan can be of multiple forms, and there can even be multiple plans
that meet the demand of the problem.

\hypertarget{solution-to-planning-problems}{%
\subsubsection{Solution to Planning
Problems}\label{solution-to-planning-problems}}

\begin{definition}[Partial Plan / Method]

A partially ordered plan is an \emph{acyclic} directed graph
\(\plan = (A_{\plan}, E)\), with:

\begin{itemize}
\tightlist
\item
  \(A_{\plan}\) the set of \textbf{steps} of the plan as vertices. A
  step is an action belonging in the plan. \(A_{\plan}\) must contain an
  initial step \(a_{\plan}^0\) and goal step \(a_{\plan}^*\) as
  convenience for certain planning paradigms.
\item
  \(E\) the set of \textbf{causal links} of the plan as edges. We note
  \(l = a_s \xrightarrow{\state} a_t\) the link between its source
  \(a_s\) and its target \(a_t\) caused by the set of fluents
  \(\state\). If \(\state = \emptyset\) then the link is used as an
  ordering constraint.
\end{itemize}

\end{definition}

This definition can express any kind of plans, either temporal, fully or
partially ordered or even hierarchical plans (using the methods of the
actions \(\plans\)). It can even express diverse planning results.

The notation can be reminicent of functional affectation and it is on
purpose. Indeed, those links can be seen as relations that only affect
their source to their target and the plan is a graph with its adjacence
function being the combination of all links.

In our framework, \emph{ordering constraints} are defined as the
transitive cover of causal links over the set of steps. We note ordering
constraints: \(a_a \succ a_s\), with \(a_a\) being \emph{anterior} to
its \emph{successor} \(a_s\). Ordering constraints cannot form cycles,
meaning that the steps must be different and that the successor cannot
also be anterior to its anterior steps:
\(a_a \neq a_s \land a_s \not \succ a_a\). If we need to enforce order,
we simply add a link without specifying a cause. The use of graphs and
implicit order constraints help to simplify the model while maintaining
its properties. Totally ordered plans are made by specifying links
between all successive actions of the sequence.

\begin{example}

In the section~\ref{sec:plan_example}, we described a classical fully
ordered plan, illustrated in figure~\ref{fig:plan}. A partially ordered
plan has a tree-like structure except that it also meets in a ``sink''
vertex (goal step). We explicit this structure in
figure~\ref{fig:poplan}.

\begin{figure}
\hypertarget{fig:poplan}{%
\centering
\includegraphics{graphics/poplan.svg}
\caption{Structure of a partially ordered plan.}\label{fig:poplan}
}
\end{figure}

\end{example}

\hypertarget{planning-problem-1}{%
\subsubsection{Planning Problem}\label{planning-problem-1}}

With this formalism, the problem is very simplified but still general.

\begin{definition}[Problem]

The planning problem is defined as the \textbf{root operator} \(\omega\)
which methods are potential solutions of the problem. Its preconditions
and effects are respectively used as initial state and goal description.

\end{definition}

As actions are very general, it is interesting to make the problem and
domain space homogenous by using an action to describe any problem. Most
of the specific notions of this framework are optional. Any planner
using it will probably define what features it supports when compiling
input domains and problems.

All notions explained so far are represented in the
figure~\ref{fig:color} adding to the SELF Venn diagram.

\hypertarget{planning-search}{%
\section{Planning search}\label{planning-search}}

A general planning algorithm can be described as a guided exploration of
a search space. The detailed structure of the search space as well as
search iterators is dependent on the planning paradigm used and
therefore are parameters of the algorithm.

\hypertarget{search-space}{%
\subsection{Search space}\label{search-space}}

\begin{definition}[Planner]

A planning algorithm, often called planner, is an exploration of a
search space \(\searches\) partially ordered by an iterator
\(\chi_{\searches}\) guided by a heuristic \(h\). From any problem
\(\pb\) every planner can derive two pieces of information immediately:

\begin{itemize}
\tightlist
\item
  the starting point \(\search_0 \in \searches\) and
\item
  the solution predicate \(?_{\search^*}\) that gives the validity of
  any potential solution in the search space.
\end{itemize}

Formally the problem can be viewed as a path-finding problem in the
directed graph \(g_{\searches}\) formed by the vertex set \(\searches\)
and the adjacency function \(\chi_{\searches}\). The set of solutions is
therefore expressed as:

\[\searches^* =
\left \{ \search^* :
  \langle \search_0, \search^* \rangle \in \chi^+_{\searches}(\search_0)
  \land ?_{\search^*}
\right \}\]

\end{definition}

We note a provided heuristic \(h(\search)\). It gives off the shortest
predicted distance to any point of the solution space. The exploration
is guided by it by minimizing its value.

The search in automated planning can have a lot of requirements. Often
time is limited and results may need to meet a certain set of
specifications.

\hypertarget{solution-constraints}{%
\subsection{Solution constraints}\label{solution-constraints}}

As finding a plan is computationally expensive, it is sometimes better
to try to find either a more generally applicable plan or a set of
alternatives. This is especially important in the case of execution
monitoring or human interactions as proposing several relevant solutions
to pick from is a very interesting feature.

For this, one will prefer either probabilistic or diverse planning. The
main problem is the additional parameters required and the change of
behavior of the algorithm. In order to handle such formalism, the
planning algorithm needs additional specifications.

We note \(\gamma_{\searches}\) the set of constraints on the nature of
the solution. This contains notably the following optional elements:

\begin{itemize}
\tightlist
\item
  \(\Delta\) the plan deviation metric to compare how much two plans are
  different.
\item
  \(k\) is the number of expected different solutions. This simply makes
  the process return when either it found \(k\) solutions or when it
  determined that \(k > |\searches^*|\).
\end{itemize}

For probabilistic planning, all elements of the probability
distributions used are typically included in the domain but the solution
constraints parameter can also contain all required information from any
planning paradigm, present or future as long as it is expressible in
SELF.

\hypertarget{temporal-constraints}{%
\subsection{Temporal constraints}\label{temporal-constraints}}

Another aspect of planning lies in its timing. Indeed sometimes acting
needs to be done before a deadline and planning are useful only during a
finite timeframe. This is done even in optimal planning as researchers
evaluating algorithms often need to set a timeout in order to be able to
complete a study in a reasonable amount of time. Indeed, often in
efficiency graphs, planning instances are stopped after a defined amount
of time.

This time component is quite important as it often determines the
planning paradigm used. It is expressed as two parameters:

\begin{itemize}
\tightlist
\item
  \(t_{\searches}\) the allotted time for the algorithm to find at least
  a fiting solution.
\item
  \(t^*\) additional time for plan optimization.
\end{itemize}

This means that if the planner cannot find a fitting solution in time it
will either return a timeout error or a partial or abstract solution
that needs to be refined. Anytime planners will also use the extra time
parameter to optimize the solution some more. If the amount of time is
either unknown or unrestricted the parameters can be omitted and their
value will be set to infinity.

\hypertarget{general-planner}{%
\section{General planner}\label{general-planner}}

A general planner
\(\plans^*(g_{\searches}, \search_0, ?_{\search^*}, h, \gamma_{\searches}, \dom, t_{\searches}, t^*)\)
is an algorithm that can find solutions using any valid instance of the
planning formalism.

For the planning algorithm itself, we simply use a parameterized
instance of any search algorithms. In our case we chose the K* algorithm
(Aljazzar and Leue
\protect\hyperlink{ref-aljazzar_heuristic_2011}{2011}, alg. 1). This
algorithm uses the classical algorithm A* to explore the graph while
using Dijkstra on some sections to find the \(k\) shortest paths. The
parameters are as follows:
\(K^*(g_{\searches}, \search_0, ?_{\search^*}, h)\). In this case, the
solution predicate contains the solution and time constraints.

Of course this algorithm is merely an example of a general planner
algorithm. The algorithm has been chosen to be general and its
efficiency hasn't been tested.

\begin{figure}
\hypertarget{fig:gplanner}{%
\centering
\includegraphics{graphics/general_planner.svg}
\caption{Venn diagram extended with general planning
formalism.}\label{fig:gplanner}
}
\end{figure}

\hypertarget{classical-formalism}{%
\section{Classical Formalism}\label{classical-formalism}}

One of the most comprehensive work on summarizing the automated planning
domain was done by Ghallab \emph{et al.}
(\protect\hyperlink{ref-ghallab_automated_2004}{2004}). This book
explains the different planning paradigm of its time and gives formal
description of some of them. This work has been updated later (Ghallab
\emph{et al.} \protect\hyperlink{ref-ghallab_automated_2016}{2016}) to
reflect the changes occurring in the planning community.

\hypertarget{state-transition-planning}{%
\subsection{State-transition planning}\label{state-transition-planning}}

The most classical representation of automated planning is using the
state transition approach: actions are operators on the set of states
and a plan is a finite-state automaton. We can also see any planning
problem as either a graph exploration problem or even a constraint
satisfaction problem. In any way that problem is isomorph to its
original formulation and most efficient algorithms use a derivative of
A* exploration techniques on the state space.

The parameters for state space planning are trivial:

\[\plans^*_{\states} = \plans^* \left( (\states, A), \pre(\omega), \eff(\omega) \right)\]

This formulation takes advantage of several tools previously described.
It uses the partial application of a function to omit the last
parameters. It also defines the search graph using the set of all states
\(\states\) as the vertices set and the set of all available actions
\(A\) as the set of edges while considering actions as relations that
can be applied to states to make the search progress toward an eventual
solution. We also use the binary nature of states to use the effects of
the root operator as the solution predicate.

Usually, we would set both \(t_{\searches}\) and \(t^*\) to an infinite
amount as it is often the case for such planners. These parameters are
left to the user of the planner.

State based planning usually suppose total knowledge of the state space
and action behavior. No concurrence or time constraints are expressed
and the state and action space must be finite as well as the resulting
state graph. This process is also deterministic and doesn't allow
uncertainty. The result of such planning is a totally ordered sequence
of actions called a plan. The total order needs to be enforced even if
it is unnecessary.

All those features are important in practice and lead to other planning
paradigms that are more complex than classical state-based planning.

\hypertarget{plan-space-planning}{%
\subsection{Plan space planning}\label{plan-space-planning}}

Plan Space Planning (PSP) is a form of planning that uses plan space as
its search space. It starts with an empty plan and tries to iteratively
refine that plan into a solution.

The transformation into a general planner is more complicated than for
state-based planning as the progress is made through refinements. We
note the set of possible refinement of a given plan
\(r= \plan \to \lBrace \odot : \otimes(\plan) \rBrace\). Each refinement
is a new plan in which we fixed a \emph{flaw} using one of the possible
\emph{resolvers} (see \textbf{LATER}).

\[\plans^*_{\plans} = \plans^* \left( \left(\plans, \{ r(\plan) : \plan \in \plans \} \right), (\{a^0, a^*\},\{a^0\rightarrow a^*\} ), \otimes(\search) = \emptyset  \right)\]

with \(a^0\) and \(a^*\) being the initial and goal steps of the plan
corresponding \(\search_0\) such that \(\eff(a^0) = \pre(\omega)\) and
\(\pre(a^*) = \eff(\omega)\). The iterator is all the possible
resolutions of all flaws on any plan in the search space and the
solution predicate is true when the plan has no more flaws.

Details about flaws, resolvers and the overall Partial Order Causal
Links (POCL) algorithm will be presented \textbf{LATER}.

This approach can usually give a partial plan if we set \(t_{\search}\)
too low for the algorithm to complete. This plan is not a solution but
can eventually be usefull as an approximation for certain use cases
(like intent recognition, see \textbf{LATER}).

\hypertarget{case-based-planning}{%
\subsection{Case based planning}\label{case-based-planning}}

Another plan oriented planning is called Case-Based Planning (CBP). This
kind of planning relies on a library \(\cal{C}\) of already complete
plans and try to find the most appropriate one to repair.

\[\plans_{\cal{C}} = \plans^* \left( (\cal{C}, \odot), (\plan : \plan \in\cal{C} \land \Delta(\plan, \omega) = \lBrace min : \cal{C} \times \{\omega\} \rBrace, \search(\pre(\omega)) \neq \emptyset \right)\]

The planner selects a plan that fits the best efficiently with the
initial and goal state of the problem. This plan is then repaired and
validated iteratively. The problem with this approach is that it may be
unable to find a valid plan or might need to populate and maintain a
good plan library. For such case an auxiliary planner is used
(preferably a diverse planner; that gives several solution).

\hypertarget{probabilistic-planning}{%
\subsection{Probabilistic planning}\label{probabilistic-planning}}

Probabilistic planning tries to deal with uncertainty by generating a
policy instead of a plan. The initial problem holds probability laws
that govern the execution of any actions. It is sometimes accompanied
with a reward function instead of a deterministic goal. We use the set
of a pair of states with applicable actions as the search space:
\(\states + A = \{ \langle \state, a \rangle : a \in A \land \state \in \states \land a(\state) \neq \emptyset\}\).
We can also note the policy iteration
\(pol = \langle \state, a \rangle \to \langle a(\state), \lBrace a' \in A \land a'(a(\state)) \neq \emptyset \rBrace \rangle\)

\[\plans_{\proba} = \plans^* \left ( ( \states + A , pol ), \langle \pre(\omega), \lBrace a \in A \land a(\pre(\omega)) \neq \emptyset \rBrace \rangle, \forall \search_1 \models \eff(\omega) \right )\]

At each iteration a state is chosen from the frontier. The frontier is
updated with the application of a non-deterministically chosen pair of
the last policy insertion. The search stops when all elements in the
frontier are goal states.

\hypertarget{hierarchical-planning}{%
\subsection{Hierarchical planning}\label{hierarchical-planning}}

Hierarchical Task Networks (HTN) are a totally different kind of
planning paradigm. Instead of a goal description, HTN uses a root task
that needs to be decomposed. The task decomposition is an operation that
replaces a task (action) by one of its methods \(\plans\).

\[\plans_{\omega} = \plans^* = \left ( (\plans, \search \to \lBrace \plan \in \plans(\lBrace a \in A_{\search} \land \plans(a) \neq \empty \rBrace) \rBrace),  \right ), \omega, \forall a \in A_{\search} : \plans(a) = \emptyset\]

\textbf{TODO: Explain more}

\hypertarget{existing-languages-and-frameworks}{%
\section{Existing Languages and
Frameworks}\label{existing-languages-and-frameworks}}

\hypertarget{classical}{%
\subsection{Classical}\label{classical}}

After STRIPS, one of the first languages to be introduced to express
planning domains like ADL (Pednault
\protect\hyperlink{ref-pednault_adl_1989}{1989}). That formalism adds
negation and conjunctions into literals to STRIPS. It also drops the
closed world hypothesis for an open world one: anything not stated in
conditions (initials or action effects) is unknown.

The current standard was strongly inspired by Penberthy \emph{et al.}
(\protect\hyperlink{ref-penberthy_ucpop_1992}{1992}) and his UCPOP
planner. Like STRIPS, UCPOP had a planning domain language that was
probably the most expressive of its time. It differs from ADL by merging
the add and delete lists in effects and to change both preconditions and
effects of actions into logic formula instead of simple states.

The PDDL language was created for the first major automated planning
competition hosted by AIPS in 1998 (Ghallab \emph{et al.}
\protect\hyperlink{ref-ghallab_pddl_1998}{1998}). It came along with
syntax and solution checker written in Lisp. It was introduced as a way
to standardize the notation of planning domains and problems so that
libraries of standard problems can be used for benchmarks. The main goal
of the language was to be able to express most of the planning problems
of the time.

With time, the planning competitions became known under the name of
International Planning Competitions (IPC) regularly hosted by the ICAPS
conference. With each installment, the language evolved to address
issues encountered the previous years. The current version of PDDL is
3.1 (Kovacs \protect\hyperlink{ref-kovacs_bnf_2011}{2011}). Its syntax
goes similarly as described in listing~\ref{lst:pddl_syntax}.

\begin{lstlisting}[caption={Simplified explanation of the syntax of PDDL.}, escapechar={$}, label=lst:pddl_syntax]
(define (domain <domain-name>)
  (:requirements :<requirement-name>)
  (:types <type-name>)
  (:constants <constant-name> - <constant-type>)
  (:predicates (<predicate-name> ?<var> - <var-type>))
  (:functions (<function-name> ?<var> - <var-type>) - <function-type>)

  (:action <action-name>
      :parameters (?<var> - <var-type>)
      :precondition (and (= (<function-name> ?<var>) <value>) (<predicate-name> ?<var>))
      :effect
      (and (not (<predicate-name> ?<var>))
     (assign (<function-name> ?<var>) ?<var>)))
\end{lstlisting}

PDDL uses the functional notation style of LISP. It defines usually two
files: one for the domain and one for the problem instance. The domain
describes constants, fluents and all possible actions. The problem lays
the initial and goal states description.

\begin{example}

For example, consider the classic block world domain expressed in
listing~\ref{lst:block_pddl}. It uses a predicate to express whether a
block is on the table because several blocks can be on the table at
once. However it uses a 0-ary function to describe the one block allowed
to be held at a time. The description of the stack of blocks is done
with an unary function to give the block that is on top of another one.
To be able to express the absence of blocks it uses a constant named
\passthrough{\lstinline!no-block!}. All the actions described are pretty
straightforward: \passthrough{\lstinline!stack!} and
\passthrough{\lstinline!unstack!} make sure it is possible to add or
remove a block before doing it and \passthrough{\lstinline!pick-up!} and
\passthrough{\lstinline!put-down!} manages the handling operations.

\begin{lstlisting}[caption={Classical PDDL 3.0 definition of the domain Block world}, escapechar={$}, label=lst:block_pddl]
(define (domain BLOCKS-object-fluents)
  (:requirements :typing :equality :object-fluents)
  (:types block)
  (:constants no-block - block)
  (:predicates (on-table ?x - block))
  (:functions (in-hand) - block
  (on-block ?x - block) - block) ;;what is in top of block ?x

  (:action pick-up
      :parameters (?x - block)
      :precondition (and (= (on-block ?x) no-block) (on-table ?x) (= (in-hand) no-block))
      :effect
      (and (not (on-table ?x))
     (assign (in-hand) ?x)))

  (:action put-down
      :parameters (?x - block)
      :precondition (= (in-hand) ?x)
      :effect
      (and (assign (in-hand) no-block)
     (on-table ?x)))

  (:action stack
      :parameters (?x - block ?y - block)
      :precondition (and (= (in-hand) ?x) (= (on-block ?y) no-block))
      :effect
      (and (assign (in-hand) no-block)
       (assign (on-block ?y) ?x)))

  (:action unstack
      :parameters (?x - block ?y - block)
      :precondition (and (= (on-block ?y) ?x) (= (on-block ?x) no-block) (= (in-hand) no-block))
      :effect
      (and (assign (in-hand) ?x)
    (assign (on-block ?y) no-block))))
\end{lstlisting}

\end{example}

However, PDDL is far from a universal standard. Some efforts have been
made to try and standardize the domain of automated planning in the form
of optional requirements. The latest of the PDDL standard is the version
3.1 (Kovacs \protect\hyperlink{ref-kovacs_bnf_2011}{2011}). It has 18
atomic requirements as represented in figure~\ref{fig:pddl_req}. Most
requirements are parts of PDDL that either increase the complexity of
planning significantly or that require extra implementation effort to
meet.

\begin{figure}
\hypertarget{fig:pddl_req}{%
\centering
\includegraphics{graphics/pddl_requirements.svg}
\caption{Dependencies and grouping of PDDL
requirements.}\label{fig:pddl_req}
}
\end{figure}

Even with that flexibility, PDDL is unable to cover all of automated
planning paradigms. This caused most subdomains of automated planning to
be left in a state similar to before PDDL: a collection of languages and
derivatives that aren't interoperable. The reason for this is the fact
that PDDL isn't expressive enough to encode more than a limited
variation in action and fluent description.

Another problem is that PDDL isn't made to be used by planners to help
with their planning process. Most planners will totally separate the
compilation of PDDL before doing any planning, so much so that most
planners of the latest IPC used a framework that translates PDDL into a
useful form before planning, adding computation time to the planning
process. The list of participating planners and their use of language is
presented in table~\ref{tbl:ipc}.

\begin{longtable}[]{@{}llllll@{}}
\caption{Planners participating in the Classic track of the 2018
International Planning Competition (IPC). The table states whether the
planner used a translation and a pre-processing system to handle PDDL.
Most of the planners are based on FastDownward directly.
\{\#tbl:ipc\}}\tabularnewline
\toprule
Name & Trans & Pre & Lang & Base & Rank\tabularnewline
\midrule
\endfirsthead
\toprule
Name & Trans & Pre & Lang & Base & Rank\tabularnewline
\midrule
\endhead
Delfi & Yes & Yes & C++ & FD & 1\tabularnewline
Complementary & Yes & Yes & C++ & FD & 2\tabularnewline
Planning-PDBs & Yes & Yes & C++ & FD & 3\tabularnewline
Scorpion & Yes & Yes & C++ & FD & 4\tabularnewline
FDMS & Yes & Yes & C++ & FD & 5\tabularnewline
DecStar & Yes & Yes & C++ & LAMA & 6\tabularnewline
Metis & Yes & Yes & C++ & FD & 7\tabularnewline
MSP & Yes & Yes & Lisp & FD & 8\tabularnewline
Symple & Yes & Yes & C++ & FD & 9\tabularnewline
Ma-plan & No & Yes & C & None & 10\tabularnewline
\bottomrule
\end{longtable}

The domain is so diverse that attempts to unify it haven't succeeded so
far. The main reason behind this is that some paradigms are vastly
different from the classical planning description. Sometimes just adding
a seemingly small feature like probabilities or plan reuse can make for
a totally different planning problem. In the next section we describe
planning paradigms and how they differ from classical planning along
with their associated languages.

\hypertarget{temporality-oriented}{%
\subsection{Temporality oriented}\label{temporality-oriented}}

When planning, time can become a sensitive constraint. Some critical
tasks may require to be completed within a certain time. Actions with
duration are already a feature of PDDL 3.1. However, PDDL might not
provide support for external events (i.e.~events occurring independent
from the agent). To do this one must use another language.

\hypertarget{pddl}{%
\subsubsection{PDDL+}\label{pddl}}

PDDL+ is an extension of PDDL 2.1 that handles process and events (Fox
and Long \protect\hyperlink{ref-fox_pddl_2002}{2002}). It can be viewed
as similar to PDDL 3.1 continuous effects but it differs on the
expressivity. A process can have an effect on fluents at any time. They
can happen either from the agent's own doing or being purely
environmental. It might be possible in certain cases to model this using
the durative actions, continuous effects and timed initial literals of
PDDL 3.1.

In listing~\ref{lst:pddl_plus}, we reproduce an example from Fox and
Long (\protect\hyperlink{ref-fox_pddl_2002}{2002}). It shows the syntax
of durative actions in PDDL+. The timed preconditions are also available
in PDDL 3.1, but the \passthrough{\lstinline!increase!} and
\passthrough{\lstinline!decrease!} rate of fluents is an exclusive
feature of PDDL+.

\begin{lstlisting}[caption={Example of PDDL+ durative action from Fox's paper.}, escapechar={$}, label=lst:pddl_plus]
(:durative-action downlink
    :parameters (?r - recorder ?g - groundStation)
    :duration (> ?duration 0)
    :condition (and (at start (inView ?g))
                    (over all (inView ?g))
                    (over all (> (data ?r) 0)))
    :effect (and (increase (downlinked)
                      (* #t (transmissionRate ?g)))
                 (decrease (data ?r)
                      (* #t (transmissionRate ?g)))))
\end{lstlisting}

The main issue with durative actions is that time becomes a continuous
resource that may change the values of fluents. The search for a plan in
that context has a higher complexity than regular planning.

\hypertarget{probabilistic}{%
\subsection{Probabilistic}\label{probabilistic}}

Sometimes, acting can become unpredictable. An action can fail for many
reasons, from logical errors down to physical constraints. This call for
a way to plan using probabilities with the ability to recover from any
predicted failures. PDDL doesn't support using probabilities. That is
why all IPC's tracks dealing with it always used another language than
PDDL.

\hypertarget{ppddl}{%
\subsubsection{PPDDL}\label{ppddl}}

PPDDL is such a language. It was used during the 4\textsuperscript{th}
and 5\textsuperscript{th} IPC for its probabilistic track (Younes and
Littman \protect\hyperlink{ref-younes_ppddl_2004}{2004}). It allows for
probabilistic effects as demonstrated in listing~\ref{lst:ppddl}. The
planner must take into account the probability when choosing an action.
The plan must be the most likely to succeed. But even with the best
plan, failure can occur. This is why probabilistic planning often gives
policies instead of a plan. A policy dictates the best choice in any
given state, failure or not. While this allows for much more resilient
execution, computation of policies are exponentially harder than
classical planning. Indeed the planner needs to take into account every
outcome of every action in the plan and react accordingly.

\begin{lstlisting}[caption={Example of PPDDL use of probabilistic effects from Younes's paper.}, escapechar={$}, label=lst:ppddl]
(define (domain bomb-and-toilet)
    (:requirements :conditional-effects :probabilistic-effects)
    (:predicates (bomb-in-package ?pkg) (toilet-clogged)
                  (bomb-defused))
    (:action dunk-package
             :parameters (?pkg)
             :effect (and (when (bomb-in-package ?pkg)
                                (bomb-defused))
                          (probabilistic 0.05 (toilet-clogged)))))
\end{lstlisting}

\hypertarget{rddl}{%
\subsubsection{RDDL}\label{rddl}}

Another language used by the 7\textsuperscript{th} IPC's uncertainty
track is RDDL (Sanner
\protect\hyperlink{ref-sanner_relational_2010}{2010}). This language has
been chosen because of its ability to express problems that are hard to
encode in PDDL or PPDDL. Indeed, RDDL is capable of expressing Partially
Observable Markovian Decision Process (POMDP) and Dynamic Bayesian
Networks (DBN) in planning domains. This along with complex probability
laws allows for easy implementation of most probabilistic planning
problems. Its syntax differs greatly from PDDL, and seems closer to
Scala or C++. An example is provided in listing~\ref{lst:rddl} from
Sanner (\protect\hyperlink{ref-sanner_relational_2010}{2010}). In it, we
can see that actions in RDDL don't need preconditions or effects. In
that case the reward is the closest information to the classical goal
and the action is simply a parameter that will influence the probability
distribution of the events that conditioned the reward.

\begin{lstlisting}[caption={Example of RDDL syntax by Sanner.}, escapechar={$}, label=lst:rddl]
////////////////////////////////////////////////////////////////////////
// A simple propositional 2-slice DBN (variables are not parameterized).
//
// Author: Scott Sanner (ssanner [at] gmail.com)
////////////////////////////////////////////////////////////////////////
domain prop_dbn {

 requirements = { reward-deterministic };

 pvariables {
  p : { state-fluent,  bool, default = false };
  q : { state-fluent,  bool, default = false };
  r : { state-fluent,  bool, default = false };
  a : { action-fluent, bool, default = false };
 };

 cpfs {
  // Some standard Bernoulli conditional probability tables
  p´ = if (p ^ r) then Bernoulli(.9) else Bernoulli(.3);

  q´ = if (q ^ r) then Bernoulli(.9)
      else if (a) then Bernoulli(.3) else Bernoulli(.8);

  // KronDelta is like a DiracDelta, but for discrete data (boolean or int)
  r´ = if (~q) then KronDelta(r) else KronDelta(r <=> q);          
 };

 // A boolean functions as a 0/1 integer when a numerical value is needed
 reward = p + q - r; // a boolean functions as a 0/1 integer when a numerical value is needed
}

instance inst_dbn {

 domain = prop_dbn;
 init-state {
  p = true;  // could also just say 'p' by itself
  q = false; // default so unnecessary, could also say '~q' by itself
  r;         // same as r = true
 };

 max-nondef-actions = 1;
 horizon  = 20;
 discount = 0.9;
}
\end{lstlisting}

\hypertarget{multi-agent}{%
\subsection{Multi-agent}\label{multi-agent}}

Planning can also be a collective effort. In some cases, a system must
account for other agents trying to either cooperate or compete in
achieving similar goals. The problem that arise is coordination. How to
make a plan meant to be executed with several agents concurently ?
Several multi-agent action languages have been proposed to answer that
question.

\hypertarget{mapl}{%
\subsubsection{MAPL}\label{mapl}}

Another extension of PDDL 2.1, MAPL was introduced to handle
synchronization of actions (Brenner
\protect\hyperlink{ref-brenner_multiagent_2003}{2003}). This is done
using modal operators over fluents. In that regard, MAPL is closer to
the PDDL+ extension proposed earlier. It encodes durative actions that
will later be integrated into the PDDL 3.0 standard. MAPL also introduce
a synchronization mechanism using speech as a comunication vector. This
seems very specific as explicit comunication isn't a requirement of
collaborative work. Listing~\ref{lst:mapl} is an example of the syntax
of MAPL domains. PDDL 3.0 seems to share a similar syntax.

\begin{lstlisting}[caption={Example of MAPL syntax by Brenner.}, escapechar={$}, label=lst:mapl]
(:state-variables
  (pos ?a - agent) - location
  (connection ?p1 ?p2 - place) - road
  (clear ?r - road) - boolean)
(:durative-action Move
  :parameters (?a - agent ?dst - place)
  :duration (:= ?duration (interval 2 4))
  :condition
    (at start (clear (connection (pos ?a) ?dst)))
  :effect (and
    (at start (:= (pos ?a) (connection (pos ?a) ?dst)))
    (at end (:= (pos ?a) ?dst))))
\end{lstlisting}

\hypertarget{ma-pddl}{%
\subsubsection{MA-PDDL}\label{ma-pddl}}

Another aspect of multi-agent planning is the ability to affect tasks
and to manage interactions between agents efficiently. For this MA-PDDL
seems more adapted than MAPL. It is an extension of PDDL 3.1, that makes
easier to plan for a team of heteroneous agents (Kovács
\protect\hyperlink{ref-kovacs_multiagent_2012}{2012}). In the example in
listing~\ref{lst:ma-pddl}, we can see how action can be affected to
agents. While it makes the representation easier, it is possible to
obtain similar effect by passing an agent object as parameters of an
action in PDDL 3.1. More complex expressions are possible in MA-PDDL,
like referencing the action of other agents in the preconditions of
actions or the ability to affect different goals to different agents.
Later on, MA-PDDL was extended with probabilistic capabilities inspired
by PPDDL (Kovács and Dobrowiecki
\protect\hyperlink{ref-kovacs_converting_2013}{2013}).

\begin{lstlisting}[caption={Example of MA-PDDL syntax by Kovacs.}, escapechar={$}, label=lst:ma-pddl]
(define (domain ma-lift-table)
(:requirements :equality :negative-preconditions
               :existential-preconditions :typing :multi-agent)
(:types agent) (:constants table)
(:predicates (lifted (?x - object) (at ?a - agent ?o - object))
(:action lift :agent ?a - agent :parameters ()
:precondition (and (not (lifted table)) (at ?a table)
              (exists (?b - agent)
               (and (not (= ?a ?b)) (at ?b table) (lift ?b))))
:effect (lifted table)))
\end{lstlisting}

\hypertarget{hierarchical}{%
\subsection{Hierarchical}\label{hierarchical}}

Another approach to planning is using Hierarchical Tasks Networks (HTN)
to resolve some planning problems. Instead of searching to satisfy a
goal, HTNs try to find a decomposition to a root task that fits the
initial state requirements and that generate an executable plan.

\hypertarget{umcp}{%
\subsubsection{UMCP}\label{umcp}}

One of the first planner to support HTN domains was UCMP by Erol
\emph{et al.} (\protect\hyperlink{ref-erol_umcp_1994}{1994}). It uses
Lisp like most of the early planning systems. Apparently PDDL was in
part inspired by UCMP's syntax. Like for PDDL, the domain file describes
action (called operators here) and their preconditions and effects
(called postconditions). The syntax is exposed in
listing~\ref{lst:ucmp}. The interesting part of that language is the way
decomposition is handled. Each task is expressed as a set of methods.
Each method has an expansion expression that specifies how the plan
should be constructed. It also has a pseudo precondition with modal
operators on the temporality of the validity of the literals.

\begin{lstlisting}[language=Lisp, caption={Example of the syntax used by UCMP.}, escapechar={$}, label=lst:ucmp]
(constants a b c table) ; declare constant symbols
(predicates on clear) ; declare predicate symbols
(compound-tasks move) ; declare compound task symbols
(primitive-tasks unstack dostack restack) ; declare primitive task symbols
(variables x y z) ; declare variable symbols

(operator unstack(x y)
          :pre ((clear x)(on x y))
          :post ((~on x y)(on x table)(clear y)))
(operator dostack (x y)
          :pre ((clear x)(on x table)(clear y))
          :post ((~on x table)(on x y)(~clear y)))
(operator restack (x y z)
          :pre ((clear x)(on x y)(clear z))
          :post ((~on x y)(~clear z)(clear y)(on x z)))

(declare-method move(x y z)
                :expansion ((n restack x y z))
                :formula (and (not (veq y table))
                              (not (veq x table))
                              (not (veq z table))
                              (before (clear x) n)
                              (before (clear z) n)
                              (before (on x y) n)))

(declare-method move(x y z)
                :expansion ((n dostack x z))
                :formula (and (veq y table)
                              (before (clear x) n)
                              (before (on x y) n)))
\end{lstlisting}

\hypertarget{shop2}{%
\subsubsection{SHOP2}\label{shop2}}

The next HTN planner is SHOP2 by Nau \emph{et al.}
(\protect\hyperlink{ref-nau_shop2_2003}{2003}). It remains to this day,
one of the reference implementation of an HTN planner. The SHOP2
formalism is quite similar to UCMP's: each method has a signature, a
precondition formula and eventually a decomposition description. This
decomposition is a set of methods like in UCMP. The methods can also be
partially ordered allowing more expressive plans. An example of the
syntax of a method is given in listing~\ref{lst:shop2}.

\begin{lstlisting}[language=Lisp, caption={Example of method in the SHOP2 language.}, escapechar={$}, label=lst:shop2]
(:method
  ; head
    (transport-person ?p ?c2)
  ; precondition
    (and
      (at ?p ?c1)
      (aircraft ?a)
      (at ?a ?c3)
      (different ?c1 ?c3))
  ; subtasks
    (:ordered
      (move-aircraft ?a ?c1)
      (board ?p ?a ?c1)
      (move-aircraft ?a ?c2)
      (debark ?p ?a ?c2)))
\end{lstlisting}

\hypertarget{hddl}{%
\subsubsection{HDDL}\label{hddl}}

A more recent example of HTN formalism comes from the PANDA framework by
Bercher \emph{et al.}
(\protect\hyperlink{ref-bercher_hybrid_2014}{2014}). This framework is
considered the current standard of HTN planning and allows for great
flexibility in domain description. PANDA takes previous formalism and
generalize them into a new language exposed in listing~\ref{lst:hddl}.
That language was called HDDL.

\begin{lstlisting}[caption={Example of HDDL syntax as used in the PANDA framework.}, escapechar={$}, label=lst:hddl]
(define (domain transport)
  (:requirements :typing :action-costs)
  (:types
        location target locatable - object
        vehicle package - locatable
        capacity-number - object
  )
  (:predicates
     (road ?l1 ?l2 - location)
     (at ?x - locatable ?v - location)
     (in ?x - package ?v - vehicle)
     (capacity ?v - vehicle ?s1 - capacity-number)
     (capacity-predecessor ?s1 ?s2 - capacity-number)
  )

  (:task deliver :parameters (?p - package ?l - location))
  (:task unload :parameters (?v - vehicle ?l - location ?p - package))

  (:method m-deliver
    :parameters (?p - package ?l1 ?l2 - location ?v - vehicle)
    :task (deliver ?p ?l2)
     :ordered-subtasks (and
      (get-to ?v ?l1)
      (load ?v ?l1 ?p)
      (get-to ?v ?l2)
      (unload ?v ?l2 ?p))
  )
  (:method m-unload
    :parameters (?v - vehicle ?l - location ?p - package ?s1 ?s2 - capacity-number)
    :task (unload ?v ?l ?p)
    :subtasks (drop ?v ?l ?p ?s1 ?s2)
  )

  (:action drop
    :parameters (?v - vehicle ?l - location ?p - package ?s1 ?s2 - capacity-number)
    :precondition (and
        (at ?v ?l)
        (in ?p ?v)
        (capacity-predecessor ?s1 ?s2)
        (capacity ?v ?s1)
      )
    :effect (and
        (not (in ?p ?v))
        (at ?p ?l)
        (capacity ?v ?s2)
        (not (capacity ?v ?s1))
      )
  )
)
\end{lstlisting}

\hypertarget{hpddl}{%
\subsubsection{HPDDL}\label{hpddl}}

A very recent language proposition was done by RAMOUL
(\protect\hyperlink{ref-ramoul_mixedinitiative_2018}{2018}). He proposes
HPDDL with a simple syntax similar to the one of UCMP. In
listing~\ref{lst:hpddl} we give an example of HPDDL method. Its
expressive power seems similar to that of UCMP and SHOP.

\begin{lstlisting}[caption={Example of HPDDL syntax as described by Ramoul.}, escapechar={$}, label=lst:hpddl]
(:method do_navigate
  :parameters(?x - rover ?from ?to - waypoint)
  :expansion((tag t1 (navigate ?x ?from ?mid))
             (tag t2 (visit ?mid))
             (tag t3 (do_navigate ?x ?mid ?to))
             (tag t4 (unvisited ?mid)))
  :constraints((before (and (not (can_traverse ?x ?from ?to)) (not (visited ?mid))
                            (can_traverse ?x ?from ?mid)) t1)))
\end{lstlisting}

\hypertarget{ontological}{%
\subsection{Ontological}\label{ontological}}

Another idea is to merge automated planning and other artificial
intelligence fields with knowledge representation and more specifically
ontologies. Indeed, since the ``semantic web'' is already widespread for
service description, why not make planning compatible with it to ease
service composition ?

\textbf{TODO: cite Eva}

\hypertarget{webpddl}{%
\subsubsection{WebPDDL}\label{webpddl}}

This question finds it first answer in 2002 with WebPDDL. This language,
explicited in listing~\ref{lst:webpddl}, is meant to be compatible with
RDF by using URI identifiers for domains (McDermott and Dou
\protect\hyperlink{ref-mcdermott_representing_2002}{2002}). The syntax
is inspired by PDDL, but axioms are added as constraints on the
knowledge domain. Actions also have a return value and can have
variables that aren't dependant on their parameters. This allows for
greater expressivity than regular PDDL, but can be partially emulated
using PDDL 3.1 constraints and object fluents.

\begin{lstlisting}[caption={Example of WebPDDL syntax by Mc Dermott.}, escapechar={$}, label=lst:webpddl]
(define (domain www-agents)
  (:extends (uri "http://www.yale.edu/domains/knowing")
            (uri "http://www.yale.edu/domains/regression-planning")
            (uri "http://www.yale.edu/domains/commerce"))
  (:requirements :existential-preconditions :conditional-effects)
  (:types Message - Obj Message-id - String)
  (:functions  (price-quote ?m - Money)
               (query-in-stock ?pid - Product-id)
               (reply-in-stock ?b - Boolean) - Message)
  (:predicates (web-agent ?x - Agent)
               (reply-pending a - Agent id - Message-id msg - Message)
               (message-exchange ?interlocutor - Agent
                                 ?sent ?received - Message
                                 ?eff - Prop)
               (expected-reply a - Agent sent expect-back - Message))
  (:axiom
      :vars (?agt - Agent ?msg-id - Message-id ?sent ?reply - Message)
      :implies (normal-step-value (receive ?agt ?msg-id) ?reply)
      :context (and (web-agent ?agt)
                    (reply-pending ?agt ?msg-id ?sent)
                    (expected-reply ?agt ?sent ?reply)))
  (:action send
      :parameters (?agt - Agent ?sent - Message)
      :value (?sid - Message-id)
      :precondition (web-agent ?agt)
      :effect (reply-pending ?agt ?sid ?sent))
  (:action receive
    :parameters (?agt - Agent ?sid - Message-id)
    :vars (?sent - Message ?eff - Prop)
    :precondition (and (web-agent ?agt) (reply-pending ?agt ?sid ?sent))
    :value (?received - Message)
    :effect (when (message-exchange ?agt ?sent ?received ?eff) ?eff)))
\end{lstlisting}

\hypertarget{opt}{%
\subsubsection{OPT}\label{opt}}

This previous work was updated by McDermott
(\protect\hyperlink{ref-mcdermott_opt_2005}{2005}). The new version is
called OPT and allows for some further expressivity. It can express
hierarchical domains with links between actions and even advanced data
structure. The syntax is mostly an update of WebPDDL. In
listing~\ref{lst:opt}, we can see that the URI was replaced by simpler
names, the action notation was simplified to make the parameter and
return value more natural. Axioms were replaced by facts with a
different notation.

\begin{lstlisting}[language=Lisp, caption={Example of the updated OPT syntax as described by Mc Dermott.}, escapechar={$}, label=lst:opt]
(define (domain www-agents)
  (:extends knowing regression-planning commerce)
  (:requirements :existential-preconditions :conditional-effects)
  (:types Message - Obj Message-id - String )
  (:type-fun (Key t) (Feature-type (keytype t)))
  (:type-fun (Key-pair t) (Tup (Key t) t))
  (:functions (price-quote ?m - Money)
              (query-in-stock ?pid - Product-id)
              (reply-in-stock ?b - Boolean) - Message)
  (:predicates (web-agent ?x - Agent)
               (reply-pending a - Agent id - Message-id msg - Message)
               (message-exchange ?interlocutor - Agent
                                 ?sent ?received - Message
                                 ?eff - Prop)
               (expected-reply a - Agent sent expect-back - Message))
  (:facts
    (freevars (?agt - Agent ?msg-id - Message-id
               ?sent ?reply - Message)
      (<- (and (web-agent ?agt)$<!-- -->$
               (reply-pending ?agt ?msg-id ?sent)
               (expected-reply ?agt ?sent ?reply))
               (normal-value (receive ?agt ?msg-id) ?reply))))
  (:action (send ?agt - Agent ?sent - Message) - (?sid - Message-id)
    :precondition (web-agent ?agt)
    :effect (reply-pending ?agt ?sid ?sent))
  (:action (receive ?agt - Agent ?sid - Message-id) - (?received - Message)
    :vars (?sent - Message ?eff - Prop)
    :precondition (and (web-agent ?agt)
                       (reply-pending ?agt ?sid ?sent))
    :effect (when (message-exchange ?agt ?sent ?received ?eff) ?eff)))
\end{lstlisting}

\hypertarget{color-and-general-planning-representation}{%
\section{Color and general planning
representation}\label{color-and-general-planning-representation}}

From the general formalism of planning proposed earlier, it is possible
to create an instanciation of the SELF language for expressing planning
domains. This extension was the primary goal of creating SELF and uses
almost all features of the language.

\hypertarget{framework}{%
\subsection{Framework}\label{framework}}

In order to describe this planning framework into SELF, we simply put
all fields of the actions into properties. Entities are used as fluents,
and the entire knwoledge domain as constraints. We use parameterized
types as specified \textbf{BEFORE}.

\begin{lstlisting}[language=Java, caption={Content of the file "planning.w"}, escapechar={$}, label=lst:planning]
"lang.w" = ? ; //include default language file.
Fluent = Entity;
State = (Group(Fluent), Statement);
BooleanOperator = (&,|);
(pre,eff, constr)::Property(Action,State);$\label{line:preeff}$
(costs,lasts,probability) ::Property(Action,Float);$\label{line:attributes}$
Plan = Group(Statement);$\label{line:plan}$
-> ::Property(Action,Action); //Causal links$\label{line:causallinks}$
methods ::Property(Action,Plan);
\end{lstlisting}

The file presented in listing~\ref{lst:planning}, gives the definition
of the syntax of fluents and actions in SELF. The first line includes
the default syntax file using the first statement syntax. The fluents
are simply typed as entities. This allows them to be either
parameterized entities or statements. States are either a set of fluents
or a logical statement between states or fluents. When a state is
represented as a set, it represents the conjunction of all fluents in
the set.

Then at line~\ref{line:preeff}, we define the preconditions, effects and
constraint formalism. They are represented as simple properties between
actions and states. This allows for the simple expression of commonly
expressed formalism like the ones found in PDDL.
Line~\ref{line:attributes} expresses the other attributes of actions
like its cost, duration and prior probability of success.

Plans are needed to be represented in the files, especially for case
based and hierarchical paradigms. They are expressed using statements
for causal link representation. The property
\passthrough{\lstinline!->!} is used in these statements and the causes
are either given explicitly as parameters of the property or they can be
inferred by the planner. We add a last property to express methods
relative to their actions.

\hypertarget{example-domain}{%
\subsection{Example domain}\label{example-domain}}

Using the classical example domain used earlier, we can write the
following file in listing~\ref{lst:block_world}.

\begin{lstlisting}[language=Java, caption={Blockworld written in SELF to work with Color}, escapechar={$}, label=lst:block_world]
"planning.w" = ? ; //include base terminology $\label{line:include}$

(! on !, held(!), down(_)) :: Fluent;$\label{line:arity}$

pickUp(x) pre (~ on x, down(x), held(~));$\label{line:pickup}$
pickUp(x) eff (~(down(x)), held(~));

putDown(x) pre (held(x));
putDown(x) eff (held(~), down(x));

stack(x, y) pre (held(x), ~ on y);
stack(x, y) eff (held(~), x on y);

unstack(x, y) pre (held(~), x on y);
unstack(x, y) eff (held(x), ~ on y);
\end{lstlisting}

At line line~\ref{line:include}, We need to include the file defined in
listing~\ref{lst:planning}. After that line~\ref{line:arity} defines the
allowed arrity of each relation/function used by fluents. This restricts
eventually the cardinality between parameters (one to many, many to one,
etc).

Line~\ref{line:pickup} encodes the action \(pickup\) defined earlier. It
is interesting to note that instead of using a constant to denote the
absence of block, we can use an anonymous exclusive quantifier to make
sure no block is held. This is quite useful to make concise domains that
stay expressive and intuitive.

\hypertarget{differences-with-pddl}{%
\subsection{Differences with PDDL}\label{differences-with-pddl}}

SELF+Color is more consice than PDDL. It will infere most types and
declaration. Variables are also inferred if they are used more than once
in a statement and also part of parameters.

While PDDL uses a fixed set of extensions to specify the capabilities of
the domain, SELF uses inclusion of other files to allow for greater
flexibility. In PDDL, everything must be declared while in SELF, type
inference allows for usage without definition. It is interesting to note
that the use of variables names \passthrough{\lstinline!x!} and
\passthrough{\lstinline!y!} are arbitrary and can be changed for each
statement and the domain will still be functionally the same. The line 3
in listing~\ref{lst:block_pddl} is a specific feature of SELF that is
absent in PDDL. It is possible to specify constraints on the cardinality
of properties. This limits the number of different combinations of
values that can be true at once. This is typically done in PDDL using
several predicate or constraints.

Most of the differences can be sumarized saying that `SELF do it once,
PDDL needs it twice'. This doesn't only mean that SELF is more compact
but also that the expressivity allows for a drastic reduction of the
search space if taken into account. Thiébaux \emph{et al.}
(\protect\hyperlink{ref-thiebaux_defense_2005}{2005}) advocate for the
recognition of the fact that expressivity isn't just a convenience but
is crucial for some problems and that treating it like an obstacle by
trying to compile it away only makes the problem worse. If a planner is
agnostic to the domain and problem, it cannot take advantages of clues
that the instanciation of an action or even its name can hold (Babli
\emph{et al.} \protect\hyperlink{ref-babli_use_2015}{2015}).

Whatever the time and work that an expert spends on a planning domain it
will always be incomplete and fixed. SELF allows for dynamical extension
and even adresses the use of reified actions as parameters. Such a
framework can be useful in multi-agent systems where agents can
communicate composite actions to instruct another agent. It can also be
useful for macro-action learning that allows to improve hierarchical
domains from repeating observations. It can also be used in online
planning to repair a plan that failed. And at last this framework can be
used for explanation or inference by making easy to map two similar
domains together. (lots of \textbf{CITATION}).

Also another difference between SELF and PDDL is the underlying planning
framework. We presented the one of SELF (listing~\ref{lst:planning}) but
PDDL seems to suppose a more classical state based formalism. For
example the fluents are of two kinds depending if they are used as
preconditions or effects. In the first case, the fluent is a formula
that is evaluated like a predicate to know if the action can be executed
in any given state. Effects are formula enforcing the values of existing
fluent in the state. SELF just suppose that the new knowledge is
enforcing and that the fluents are of the same kind since verification
about the coherence of the actions are made prior to its application in
planning.

\hypertarget{ch:heart}{%
\chapter{Online and Flexible Planning Algorithms}\label{ch:heart}}

In this chapter, we present planners and approaches to inverted planning
and intent recognition. To do that we must first have a efficient online
planning algorithm that can take into account observed plans or fluents
and find the most likely plan to be pursued by an external agent. The
planning process must be done in real time and take into account new
observations to make new predictions. This requires the use of online
planners. In such cases, the planning process works using distinct
phases. In figure~\ref{fig:phases}, we illustrate the components of the
process. Only the planning part is meant to have real-time constraints
on its execution the rest is usually a linear process and can be
negligible in terms of execution times.

\begin{figure}
\hypertarget{fig:phases}{%
\centering
\includegraphics{graphics/planning_phases.svg}
\caption{Planning phases for online planning}\label{fig:phases}
}
\end{figure}

Classical planning can be used for such a work but lacks flexibility
when needing to replan at high frequency. The planner must be either
able to reuse previously found plans or be able to compute quickly plans
that are good approximation of the intended goal. We could use
probabilistic planning, especially Partially Observable Markovian
Decision Process (POMDP) to directly encode the intent recognition
problem but that approach has been explored in great detail already,
including numerous Bayesian network approaches. Further discussions of
inverted planning and intent recognition can be found \textbf{LATER}.

It was decided to explore more expressive and flexible approaches to use
the semantics of the planning domain to attempt to guide the search to a
more logical plan. This approach uses either repair heuristics or
explanation to provide fast predictions of intended goals.

\hypertarget{existing-algorithms}{%
\section{Existing Algorithms}\label{existing-algorithms}}

In order to make a planner capable of repairing plans, the most fitting
paradigm is PSP as described \textbf{BEFORE}. Using the plan space for
search allows to modify the refinement process into repairing existing
plans.

The second approach using explanations is hierarchical. The planner will
use a HTN planning domain that contains composite actions (or tasks)
that have several methods (as plans) to realize them.

First, PSP will be presented in more details regarding its classical
formulation and definition.

\hypertarget{plan-space-planning-1}{%
\subsection{Plan Space Planning}\label{plan-space-planning-1}}

All PSP algorithms work in a similar way: their search space is the set
of all plans and its iteration operation is plan refinement. This means
that every PSP planner searches for \emph{flaws} in the current plan and
then computes a set of \emph{resolvers} that potentially fix each of
them. The algorithm usually starts with an empty plan only having the
initial and goal step and recursively refine the plan until all flaws
have been solved.

In general, PSP is faster than naive classical planning. However, with
the advent of efficient state based heuristics used in Fast Forward (FF)
(Hoffmann \protect\hyperlink{ref-hoffmann_ff_2001}{2001}) and LAMA
(Richter and Westphal \protect\hyperlink{ref-richter_lama_2010}{2010}),
plan space planning has been left behind regarding raw performance.
While PSP delays commitment and therefore can make very efficient
choices that can be faster than classical planning, most formulation of
PSP problems leads to significant increase in complexity (Tan and
Gruninger \protect\hyperlink{ref-tan_complexity_2014}{2014}). The
backtracking in PSP algorithms along with heavy data structures such as
plans to modify at each iteration makes the approach slower by design
without an excellent heuristic.

Works on PSP didn't stop at that point (Nguyen and Kambhampati
\protect\hyperlink{ref-nguyen_reviving_2001}{2001}) since it has unique
advantages over classical planning. Indeed, by using backward chaining,
PSP algorithms have are sound and complete and therefore guarantee to
find a solution if it exists (Sjöberg and Nissar
\protect\hyperlink{ref-sjoberg_automated_2015}{2015}).

As PSP finds partially ordered plans, it is also by nature more
flexible. Indeed, multiple totally ordered plans are contained within a
partially ordered one, they are called linearizations. So, when wanting
to have several plans with low diversity, PSP is the way to go.

\hypertarget{definitions}{%
\subsubsection{Definitions}\label{definitions}}

In \textbf{BEFORE} we have formalized how PSP works in the general
planning formalism. However, since this formalism is being introduced in
the present document, it isn't used by the rest of the community. This
means that we still need to define the classical Partially Ordered with
Causal Link (POCL algorithm. In order to define this algorithm, we need
to explain the notions of flaws and resolvers.

\begin{definition}[Flaws]\label{def:flaws}

Flaws are constraints violations within a plan. The set of flaws in a
plan \(\bb{\pi}\) is noted \(\otimes_{\bb{\pi}}\). There are different
kinds of flaws in classical PSP and additional ones can be defined
depending on the application.

Classical flaws often have a few common features. They are often
\textbf{constructive} since they require an \emph{addition} of causal
links and steps in a plan to be fixed. They have a \emph{proper fluent}
\(f\) that is the cause of the violation in the plan the flaw is
representing and a \emph{needer} \(a_n\) that is the action requiring
the proper fluent be fulfilled. In classical PSP flaws are either:

\begin{itemize}
\tightlist
\item
  \textbf{Subgoals}, also called \emph{open condition} that are yet to
  be supported by a \emph{provider} \(a_p\). We note subgoals
  \(\otimes^\downarrowbarred_{a_n}(f)\).
\item
  \textbf{Threats} are caused by steps that can break a causal link with
  their effects. They are called \emph{breakers} of the threatened link.
  A step \(a_b\) threatens a causal link
  \(l_t = a_p \xrightarrow{f} a_n\) if and only if
  \((\eff(a_b) \not\models f) \land (a_b \not\succ a_p \land a_n \not\succ a_b)\).
  Said otherwise, the breaker can potentially cancel an effect of a
  providing step \(a_p\), before it gets used by its needer \(a_n\). We
  note threats \(\otimes^\dagger_{a_n}(f, a_b)\).
\end{itemize}

\end{definition}

\begin{example}

\begin{figure}
\hypertarget{fig:flaws}{%
\centering
\includegraphics{graphics/plan-flaws.svg}
\caption{Example of partial plan having flaws}\label{fig:flaws}
}
\end{figure}

In figure~\ref{fig:flaws} we present a partially ordered plan with two
typical flaws. The first is a subgoal missing from the plan to fulfill
the \(f_5\) precondition of \(a_1\).

The second flaw in the figure~\ref{fig:flaws} is the threat between
\(a_2\) and a causal link outgoing from \(a_3\). This happens because
nothing prevents \(a_2\) to be executed after \(a_3\) and negate the
fluent \(f_2\) needed by the next step.

\end{example}

These flaws need to be fixed in order for the plan to be valid. In POCL
it is done by finding their resolvers.

\begin{definition}[Resolvers]\label{def:resolvers}

A resolver is a plan refinement that attempts to solve a flaw
\(\otimes_{a_n}\). Since classical flaws are constructive, the classical
resolvers are called \emph{positive}. They are defined as follows:

\begin{itemize}
\tightlist
\item
  \emph{For subgoals}, the resolvers are a potential causal link
  containing the proper fluent \(f\) of a given subgoal in their causes
  while taking the needer step \(a_n\) as their target and a
  \textbf{provider} step \(a_p\) as their source. They are noted
  \(\odot^+_{a_p}(\otimes^\downarrowbarred_{a_n}(f)) = a_p \xrightarrow{f} a_n\).
\item
  \emph{For threats}, we usually consider only two resolvers:
  \textbf{demotion} (\(a_b \succ a_p\)) and \textbf{promotion}
  (\(a_n \succ a_b\)) of the breaker relative to the threatened link. We
  call the added causeless causal link a \textbf{guarding} link. The
  resolvers for threats are noted
  \(\odot^+_{\succ}(\otimes^\dagger_{a_n}(f, a_b)) = a_p \rightarrow a_b\)
  for promotion and
  \(\odot^+_{\prec}(\otimes^\dagger_{a_n}(f, a_b)) = a_b \rightarrow a_n\)
  for demotion.
\end{itemize}

It is possible to introduce extra resolvers to fix custom flaws. In such
a case we call positive resolvers, those which add causal links and
steps to the plan and negative those that removes causal links and
steps. It is preferable to engineer flaws and resolver not to mix
positive and negative aspect at once because of the complicated side
effects that might result from it.

\end{definition}

\begin{example}

\begin{figure}
\hypertarget{fig:resolvers}{%
\centering
\includegraphics{graphics/plan-resolver.svg}
\caption{Example of resolvers that fixes the previously illustrated
flaws}\label{fig:resolvers}
}
\end{figure}

From our previous example, we present the complete plan in
figure~\ref{fig:resolvers}. The subgoal needs to be fixed by inserting
another causal link to provide the missing fluent and inserting any
necessary steps to do so. In that case the initial state happens to
provide the necessary fluent so we simply add a causal link for it.

For the threat, the solution is to either promote or demote \(a_2\) so
that it doesn't interfere with the causal link between \(a_3\) and
\(a_4\). We chose here to demote \(a_2\) so it requires \(a_4\) to be
executed before it.

\end{example}

The application of a resolver does not necessarily mean progress. It can
have consequences that may require reverting its application in order to
respect the backtracking of the POCL algorithm.

\begin{definition}[Side effects]\label{def:side-effects}

Flaws that are caused by the application of a resolver are called
\emph{related flaws}. They are inserted into the \emph{agenda}\footnote{An
  agenda is a flaw container used for the flaw selection of POCL.} with
each application of a resolver:

\begin{itemize}
\tightlist
\item
  \emph{Related subgoals} are all the new open conditions inserted by
  new steps.
\item
  \emph{Related threats} are the causal links threatened by the
  insertion of a new step or the deletion of a guarding link.
\end{itemize}

Flaws can also become irrelevant when a resolver is applied. It is
always the case for the targeted flaw, but this can also affect other
flaws. Those \emph{invalidated flaws} are removed from the agenda upon
detection:

\begin{itemize}
\tightlist
\item
  \emph{Invalidated subgoals} are subgoals satisfied by the new causal
  links or the removal of their needer.
\item
  \emph{Invalidated threats} happen when the breaker no longer threatens
  the causal link because the order guards the threatened causal link or
  either of them have been removed.
\end{itemize}

\end{definition}

\begin{example}

\begin{figure}
\hypertarget{fig:plan_sideeffects}{%
\centering
\includegraphics{graphics/plan-sideeffects.svg}
\caption{Example of the side effects of the application of a
resolver}\label{fig:plan_sideeffects}
}
\end{figure}

In our example, by adding the step \(a_2\) to fix an unsupported subgoal
needed by \(a_1\), we introduced another subgoal to support the new step
that also threatens the causal link between \(a_3\) and \(a_4\).

\end{example}

\hypertarget{classical-pocl-algorithm}{%
\subsubsection{Classical POCL
Algorithm}\label{classical-pocl-algorithm}}

In algorithm~\ref{alg:pocl} we present a generic version of POCL
inspired by Ghallab \emph{et al.}
(\protect\hyperlink{ref-ghallab_automated_2004}{2004}, sec. 5.4.2).

\begin{algorithm}\caption{POCL Algorithm}\label{alg:pocl}\begin{algorithmic}[1]\Function{POCL}{Agenda $\cal{A}$, Action $\omega$}
    \If{$\cal{A} = \emptyset$} \Comment{Populated agenda needs to be provided}
        \State \Return Success \Comment{Stops all recursion}
    \EndIf
    \State Flaw $\otimes \gets \lBrace \cal{A} \rBrace$ \label{line:flawselection}
    \Comment{Heuristically chosen flaw}
    \State Resolvers $\bigodot \gets$ \Call{solve}{$\otimes$, $\lBrace \plans(\omega) \rBrace$} \Comment{The root operator has only one method for PSP} \label{line:resolverselection}
    \ForAll{$\odot \in \bigodot$} \Comment{Non-deterministic choice operator}
        \State \Call{apply}{$\odot$, $\plan$} \label{line:resolverapplication}
        \Comment{Apply resolver to partial plan}
        \State Agenda $\cal{A}' \gets$ \Call{update}{$\cal{A}$} \label{line:updateagenda}
        \If{\protect\Call{POCL}{$\cal{A}'$, $\omega$} = Success} \Comment{Refining recursively}
            \State \Return Success
        \EndIf
        \State \Call{revert}{$\cal{A}$, $\plan$} \Comment{Failure, undo resolver application} \label{line:revert}
    \EndFor
    \State $\cal{A} \gets \cal{A} \cup \{\otimes\}$ \Comment{Flaw was not resolved}
    \State \Return Failure \Comment{Revert to last non-deterministic choice}
\EndFunction\end{algorithmic}\end{algorithm}

For our version of POCL we follow a refinement procedure that works in
several generic steps. In figure~\ref{fig:refinement} we detail the
resolution of a subgoal as done in the algorithm~\ref{alg:pocl}.

The first is the search for resolvers. It is often done in two separate
steps: first, select the candidates and then check each of them for
validity. This is done using the polymorphic function
\passthrough{\lstinline!solve!} at line~\ref{line:resolverselection}.

In the case of subgoals, variable unification is performed to ensure the
compatibility of the resolvers. Since this step is time-consuming, the
operator is instantiated accordingly at this step to factories the
computational effort. Composite operators have also all their methods
instantiated at this step if they are selected as a candidate.

Then a resolver is picked non-deterministically for applications (this
can be heuristically driven). At line~\ref{line:resolverapplication} the
resolver is effectively applied to the current plan. All side effects
and invalidations are handled during the update of the agenda at
line~\ref{line:updateagenda}. If a problem occurs,
line~\ref{line:revert} backtracks and tries other resolvers. If no
resolver fits the flaw, the algorithm backtracks to previous resolver
choices to explore all the possible plans and ensure completeness.

\hypertarget{existing-psp-planners}{%
\subsubsection{Existing PSP Planners}\label{existing-psp-planners}}

Related works already tried to explore new ideas to make PSP an
attractive alternative to regular state-based planners like the
appropriately named ``Reviving partial order planning'' (Nguyen and
Kambhampati \protect\hyperlink{ref-nguyen_reviving_2001}{2001}) and
VHPOP (Younes and Simmons
\protect\hyperlink{ref-younes_vhpop_2003}{2003}). More recent efforts
(Coles \emph{et al.} \protect\hyperlink{ref-coles_popf2_2011}{2011};
Sapena \emph{et al.}
\protect\hyperlink{ref-sapena_combining_2014}{2014}) are trying to adapt
the powerful heuristics from state-based planning to PSP's approach. An
interesting approach of these last efforts is found in (Shekhar and
Khemani \protect\hyperlink{ref-shekhar_learning_2016}{2016}) with
meta-heuristics based on offline training on the domain. Yet, we clearly
note that only a few papers lay the emphasis upon plan quality using PSP
(Ambite and Knoblock \protect\hyperlink{ref-ambite_planning_1997}{1997};
Say \emph{et al.} \protect\hyperlink{ref-say_mathematical_2016}{2016}).

\hypertarget{plan-repair-reuse}{%
\subsection{Plan Repair \& Reuse}\label{plan-repair-reuse}}

In online planning, the plan is computed frequently from a changing
initial state. This means that the previous plan is very often
available. In order to take advantage of the effort invested in previous
plans, it is tempting to simply reuse the existing plan instead of
replanning from scratch. Most work on the field focus on monitoring
execution and find ways to make resilient plans.

In such a case, a lot of the planning model can be subject to
uncertainty. Indeed, the execution of an action can fail because an
external event changed a precondition required to do it or the model
itself can be inaccurate.

The idea of reusing plan emerged early on (Nebel and Koehler
\protect\hyperlink{ref-nebel_plan_1995}{1995}) but with a caveat: often
repairing needed more effort than replanning. So plan repair became more
of a gamble and needed incentives to reuse an existing plan given an
application. For example, such process is useful for multi-agent
planning where a significant change of plan is expansive among agents
(Ephrati and Rosenschein
\protect\hyperlink{ref-ephrati_multiagent_1993}{1993}; Alami \emph{et
al.} \protect\hyperlink{ref-alami_multirobot_1995}{1995}; Sugawara
\protect\hyperlink{ref-sugawara_reusing_1995}{1995}; Borrajo
\protect\hyperlink{ref-borrajo_multiagent_2013}{2013}; Luis and Borrajo
\protect\hyperlink{ref-luis_plan_2014}{2014}). This motivation for plan
repair comes from plan merging and is needed in cooperative
environments.

The question of the efficiency of replanning vs.~repairing has been
since studied extensively under several aspects (Van Der Krogt and De
Weerdt \protect\hyperlink{ref-vanderkrogt_plan_2005}{2005}; Fox \emph{et
al.} \protect\hyperlink{ref-fox_plan_2006}{2006}). The emergence of
diverse planning and requirement on plan stability of execution
monitoring gave new research on the subject. At this point, most of the
literature focuses on a case-based planning, where a plan library is
already provided and the planner must select a plan and repair it to fit
a given case (Gerevini \emph{et al.}
\protect\hyperlink{ref-gerevini_planlibrary_2013}{2013}; Borrajo
\emph{et al.} \protect\hyperlink{ref-borrajo_progress_2015}{2015}).

A recent work of Zhuo and Kambhampati
(\protect\hyperlink{ref-zhuo_modellite_2017}{2017}) gives an interesting
approach to the problem by questioning the domain. Indeed, the need to
re-plan can be an opportunity to revised issues in the current model and
to improve it by adding newly found a solution to complete the given
planning model.

In our case, we focus on PSP and how to make plans repair efficiently
using that technique. Classical PSP algorithms don't take as an input an
existing plan but can be enhanced to fit plan to repair, as for instance
in (Van Der Krogt and De Weerdt
\protect\hyperlink{ref-vanderkrogt_plan_2005}{2005}). Usually, PSP
algorithms take a problem as an input and use a loop or a recursive
function to refine the plan into a solution. We can't solely use the
refining recursive function to be able to use our existing partial plan.
This causes multiples side effects if the input plan is suboptimal. This
problem was already explored as of LGP-adapt (Borrajo
\protect\hyperlink{ref-borrajo_multiagent_2013}{2013}) that explains how
reusing a partial plan often implies replanning parts of the plan.

\hypertarget{htn}{%
\subsection{HTN}\label{htn}}

\begin{quote}
``HTN planners differ from classical planners in what they plan for and
how they plan for it. In an HTN planner, the objective is not to achieve
a set of goals but instead to perform some set of tasks.''\footnote{Ghallab
  \emph{et al.} (\protect\hyperlink{ref-ghallab_automated_2004}{2004})}
\end{quote}

Planning using HTN gives a completely different approach to the problem
and its formulation. In this formalism, actions are composite tasks and
there is no goal other than to complete the root task. One can find
similarities with our general planning formalism and it isn't a
coincidence. Indeed, HTN is more general than planning and therefore one
need to be able to allow for this level of expressivity.

\begin{figure}
\centering
\includegraphics{graphics/htn-expressivity.svg}
\caption{Venn diagram of the expressivity class of HTN paradigms.}
\end{figure}

This expressivity comes at a cost. HTN problems are on a complexity
category that is significantly harder than regular STRIPS
planning:\footnote{From Bercher and Höller
  (\protect\hyperlink{ref-bercher_tutorial_2018}{2018}) HTN Tutorial at
  ICAPS 2018}

\begin{longtable}[]{@{}ll@{}}
\caption{HTN Expressivity associated with their respective complexity
classes \{\#tbl:htn\}}\tabularnewline
\toprule
\emph{Restrictions} & \emph{Complexity}\tabularnewline
\midrule
\endfirsthead
\toprule
\emph{Restrictions} & \emph{Complexity}\tabularnewline
\midrule
\endhead
Classical & PSPACE\tabularnewline
Task Insertion & NEXPTIME\tabularnewline
Totally Ordered & EXPTIME\tabularnewline
Regular & PSPACE\tabularnewline
Acyclic & NEXPTIME\tabularnewline
Tail-recursive & EXPSPACE\tabularnewline
\bottomrule
\end{longtable}

HTN is often combined with classical approaches since it allows for a
more natural expression of domains making expert knowledge easier to
encode. These kinds of planners are named \textbf{decompositional
planners} when no initial plan is provided (Fox
\protect\hyperlink{ref-fox_natural_1997}{1997}). Most of the time the
integration of HTN simply consists in calling another algorithm when
introducing a composite operator during the planning process. The DUET
planner by Gerevini \emph{et al.}
(\protect\hyperlink{ref-gerevini_combining_2008}{2008}) does so by
calling an instance of a HTN planner based on the task insertion called
SHOP2 (Nau \emph{et al.} \protect\hyperlink{ref-nau_shop2_2003}{2003})
to decompose composite actions. Some planners take the integration
further by making the decomposition of composite actions into a special
step in their refinement process. Such works include the discourse
generation oriented DPOCL (Young and Moore
\protect\hyperlink{ref-young_dpocl_1994}{1994}) and the work of
Kambhampati \emph{et al.}
(\protect\hyperlink{ref-kambhampati_hybrid_1998}{1998}) generalizing the
practice for decompositional planners.

In our case, we chose a class of hierarchical planners based on
Plan-Space Planning (PSP) algorithms (Bechon \emph{et al.}
\protect\hyperlink{ref-bechon_hipop_2014}{2014}; Dvorak \emph{et al.}
\protect\hyperlink{ref-dvorak_flexible_2014}{2014}; Bercher \emph{et
al.} \protect\hyperlink{ref-bercher_hybrid_2014}{2014}) as a reference
approach. The main difference here is that the decomposition is
integrated into the classical POCL algorithm by only adding new types of
flaws. This allows keeping all the flexibility and properties of POCL
while adding the expressivity and abstraction capabilities of HTN.

\hypertarget{lollipop}{%
\section{LOLLIPOP}\label{lollipop}}

That first planner is a prototype destined to test the feasibility of a
plan to repair approach using PSP. This is done through the addition of
new flaw types in a classical POCL algorithm.

Another issue is caused by the need to get any existing partial plan as
an input. This plan can contain any problems and inconsistency since
there are trivial ways to ensure consistence between the plan given and
the new initial state or goal.

\hypertarget{operator-graph}{%
\subsection{Operator Graph}\label{operator-graph}}

In order to make POCL faster we experimented a means to build a
dependency graph for operators in the domain, computed at domain
compilation time. The idea is to extract a partial plan from this graph
by pruning it based on the provided goal and use that plan to quick
start the planning process. First we need to define the operator graph.

\begin{definition}[Operator Graph]

An operator graph \(g_O\) of a set of operators \(O\) is a labeled
directed graph that binds two operators with the causal link
\(o_1 \xrightarrow{f} o_2\) if and only if there exists at least one
fluent so that \((f \in \eff(o_1)) \land f \models \pre(o_2)\).

\end{definition}

This definition was inspired by the notion of domain causal graph as
explained in (Göbelbecker \emph{et al.}
\protect\hyperlink{ref-gobelbecker_coming_2010}{2010}) and originally
used as a heuristic in (Helmert \emph{et al.}
\protect\hyperlink{ref-helmert_fast_2011}{2011}). Causal graphs have
fluents as their nodes and operators as their edges. Operator graphs are
the opposite: an \emph{operator dependency graph} for a set of actions.
A similar structure was used in (Peot and Smith
\protect\hyperlink{ref-peot_postponing_1994}{1994}) that builds the
operator dependency graph of goals and uses precondition nodes instead
of labels. We call \emph{co-dependent} operators that form a cycle. If
the cycle is made of only one operator (self-loop), then it is called
\emph{auto-dependent}.

While building this operator graph, we need a \textbf{providing map}
that indicates, for each fluent, the list of operators that can provide
it. This is a simpler version of the causal graphs that is reduced to an
associative table easier to maintain. The list of providers can be
sorted to drive resolver selection (as detailed in
section~\ref{sec:selection}). A \textbf{needing map} is also built but
is only used for operator graph generation. We note \(g_{\cal{D}}\) the
operator graph built with the set of operators in the domain
\(\cal{D}\).

\begin{example}

In the figure~\ref{fig:operatorgraph}, we illustrate the application of
this mechanism on our example from figure~\ref{fig:example}. Continuous
lines correspond to the \emph{domain operator graph} computed during
domain compilation time.

\begin{figure}
\hypertarget{fig:operatorgraph}{%
\centering
\includegraphics{graphics/operator_graph.svg}
\caption{Diagram of the operator graph of example domain. Full arrows
represent the domain operator graph and dotted arrows the dependencies
added to inject the initial and goal steps.}\label{fig:operatorgraph}
}
\end{figure}

\end{example}

The generation of the operator graph is detailed in
algorithm~\ref{alg:operatorgraph}. It explores the operator space and
builds a providing and a needing map that gives the provided and needed
fluents for each operator. Once done it iterates on every precondition
and searches for a satisfying cause to add the causal links to the
operator graph.

\begin{algorithm}\caption{Operator graph generation and update algorithm}\label{alg:operatorgraph}\begin{algorithmic}\footnotesize
\Function{addVertex}{Action $a$}
    \State \Call{cache}{$a$} \Comment{Update of the providing and needing map}
    \If {binding} \Comment{boolean that indicates if the binding was requested}
        \State \Call{bind}{$a$}
    \EndIf
\EndFunction
\Function{cache}{Action $a$}
    \ForAll{$f \in \eff(a)$} \Comment{Adds $a$ to the list of providers of $f$}
        \State \Call{add}{$A_p, f, a$}
    \EndFor
    \State ... \Comment{Same operation with needing and preconditions}
\EndFunction
\Function{bind}{Action $a$}
    \ForAll{$f \in \pre(a)$}
        \If{$f \in A_p$}
            \ForAll{$\plan \in$ \Call{get}{$A_p$, $f$}}
                \State Link $l \gets$ \Call{getEdge}{$\plan$, $a$} \Comment{Create the link if needed}
                \State \Call{addCause}{$l$, $f$} \Comment{Add the fluent as a cause}
            \EndFor
        \EndIf
    \EndFor
    \State ... \Comment{Same operation with needing and effects}
\EndFunction\end{algorithmic}\end{algorithm}

To apply the notion of operator graphs to planning problems, we just
need to add the initial and goal steps to the operator graph. In
figure~\ref{fig:operatorgraph}, we depict this insertion with our
previous example using dotted lines. However, since operator graphs may
have cycles, they can't be used directly as input to POCL algorithms to
ease the initial back chaining. Moreover, the process of refining an
operator graph into a usable one could be more computationally expensive
than POCL itself.

In order to give a head start to the LOLLIPOP algorithm, we propose to
build operator graphs differently with the algorithm detailed in
algorithm~\ref{alg:safeoperatorgraph}. A similar notion was already
presented as ``basic plans'' in (Sebastia \emph{et al.}
\protect\hyperlink{ref-sebastia_graphbased_2000}{2000}). These ``basic''
partial plans use a more complete but slower solution for the generation
that ensures that each selected steps are \emph{necessary} for the
solution. In our case, we built a simpler solution that can solve some
basic planning problems but that also makes early assumptions (since our
algorithm can handle them). It does a simple and fast backward
construction of a partial plan driven by the providing map. Therefore,
it can be tweaked with the powerful heuristics of state search planning.

\begin{algorithm}\caption{Safe operator graph generation algorithm}\label{alg:safeoperatorgraph}\begin{algorithmic}\footnotesize
\Function{safe}{Action $\omega$}
    \State Stack<Action> $open \gets [a^*]$
    \State Stack<Action> $closed \gets \emptyset$
    \While{$open \neq \emptyset$}
        \State Action $a \gets$ \Call{pocl}{$open$} \Comment{Remove $a$ from $open$}
        \State \Call{push}{$closed$, $a$}
        \ForAll {$f \in pre(a)$}
            \State Actions $A_p \gets$ \Call{getProviding}{$\plan$, $f$} \Comment{Sorted by usefulness}
            \If{$A_p = \emptyset$} \Comment{(see section~\ref{sec:selection})}
                \State $S \gets S \setminus \{\plan\}$
                \Continue
            \EndIf
            \State Action $a' \gets$ \Call{getFirst}{$\plan$} \label{line:safefirst}
            \If{$a' \in closed$}
                \Continue
            \EndIf
            \If{$a' \not \in S$}
                \State \Call{push}{$open$, $a'$}
            \EndIf
            \State $S \gets S \cup \{a'\}$
            \State Link $l \gets$ \Call{getEdge}{$a'$, $a$} \Comment{Create the link if needed}
            \State \Call{addCause}{$l$, $f$} \Comment{Add the fluent as a cause}
        \EndFor
    \EndWhile
\EndFunction\end{algorithmic}\end{algorithm}

This algorithm is useful since it is specifically used on goals. The
result is a valid partial plan that can be used as input to POCL
algorithms.

\hypertarget{negative-refinements}{%
\subsection{Negative Refinements}\label{negative-refinements}}

The classical POCL algorithm works upon a principle of positive plan
refinements. The two standard flaws (subgoals and threats) are fixed by
\emph{adding} steps, causal links, or variable binding constraints to
the partial plan. Online planning needs to be able to \emph{remove}
parts of the plan that are not necessary for the solution. Since we
assume that the input partial plan is quite complete, we need to define
new flaws to optimize and fix this plan. These flaws are called
\emph{negative} as their resolvers apply subtractive refinements on
partial plans.

\begin{definition}[Alternative]\label{def:alternative}

An alternative is a negative flaw that occurs when there is a better
provider choice for a given link. An alternative to a causal link
\(a_p \xrightarrow{f} a_n\) is a provider \(a_b\) that has a better
\emph{utility value} than \(a_p\).

\end{definition}

The \textbf{utility value} of an operator is a measure of usefulness
being the base of our ranking mechanism detailed in
section~\ref{sec:selection}. It uses the incoming and outgoing degrees
of the operator in the domain operator graph to measure its usefulness.

Finding an alternative to an operator is computationally expensive. It
requires searching a better provider for every fluent needed by a step.
To simplify that search, we select only the best provider for a given
fluent and check if the one used is the same. If not, we add the
alternative as a flaw. This search is done only on updated steps for
online planning. Indeed, the safe operator graph mechanism is guaranteed
to only choose the best provider (algorithm~\ref{alg:safeoperatorgraph}
at line~\ref{line:safefirst}). Furthermore, subgoals won't introduce new
fixable alternatives as they are guaranteed to select the best possible
provider.

\begin{definition}[Orphan]

An orphan is a negative flaw that occurs when a step in the partial plan
(other than the initial or goal step) is not participating in the plan.
Formally \(a_o\) is an orphan if and only if
\(a_o \neq a^0 \land a_o \neq a^* \land \left( |\chi_{\outgo}(a_o)| = 0 \right) \lor \lBrace =(\emptyset) : \chi_{\outgo}(a_o) \rBrace\).

\end{definition}

With \(\chi_{\outgo}(a_o)\) being the set of \emph{outgoing causal
links} of \(a_o\) in \(\pi\). This last condition checks for
\emph{dangling orphans} that are linked to the goal with only bare
causal links (introduced by threat resolution).

The solution to an alternative is a negative refinement that simply
removes the targeted causal link. This causes a new subgoal as a side
effect, which will focus on its resolver by its rank (explained in
section~\ref{sec:selection}) and then pick the first provider (the most
useful one). The resolver for orphans is the negative refinement that is
meant to remove a step and its incoming causal link while tagging its
providers as potential orphans.

\begin{figure}
\hypertarget{fig:sideeffects}{%
\centering
\includegraphics{graphics/sideeffects.svg}
\caption{Schema representing flaws with their signs, resolvers and side
effects relative to each other}\label{fig:sideeffects}
}
\end{figure}

The side effects mechanism also needs an upgrade since the new kinds of
flaws can interfere with one another. This is why we extend the side
effect definition (cf.~definition~\ref{def:sideeffect}) with a notion of
sign.

\begin{definition}[Signed Side Effects]

A signed side effect is either a regular \emph{causal side effect} or an
\emph{invalidating side effect}. The sign of a side effect indicates if
the related flaw needs to be added or removed from the agenda.

\end{definition}

The figure~\ref{fig:sideeffects} exposes the extended notion of signed
resolvers and side effects. When treating positive resolvers, nothing
needs to change from the classical method. When dealing with negative
resolvers, we need to search for extra subgoals and threats. Deletion of
causal links and steps can cause orphan flaws that need to be identified
for removal.

In the method described in (Peot and Smith
\protect\hyperlink{ref-peot_threatremoval_1993}{1993}), a
\textbf{invalidating side effect} is explained under the name of
\emph{DEnd} strategy. In classical POCL, it has been noticed that
threats can disappear in some cases if subgoals or other threats were
applied before them. For our mechanisms, we decide to gather under this
notion every side effect that removes the need to consider a flaw. For
example, orphans can be invalidated if a subgoal selects the considered
step. Alternatives can remove the need to compute further subgoal of an
orphan step as orphans simply remove the need to fix any flaws that
concern the selected step.

These interactions between flaws are decisive for the validity and
efficiency of the whole model, that is why we aim to drive flaw
selection in a rigorous manner.

\hypertarget{usefulness-heuristic}{%
\subsection{Usefulness Heuristic}\label{usefulness-heuristic}}

\textbf{FIXME: Redo this section and be clear about what it is and when
it is computed. Needing an execution schema too with phases and stuff}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Resolvers and flaws selection are the keys to improving performances.
Choosing a good resolver helps to reduce the branching factor that
accounts for most of the time spent on running POCL algorithms
(Kambhampati \protect\hyperlink{ref-kambhampati_design_1994}{1994} ).
Flaw selection is also important for efficiency, especially when
considering negative flaws which can conflict with other flaws.

Conflicts between flaws occur when two flaws of opposite sign target the
same element of the partial plan. This can happen, for example, if an
orphan flaw needs to remove a step needed by a subgoal or when a threat
resolver tries to add a promoting link against an alternative. The use
of side effects will prevent most of these occurrences in the agenda but
a base ordering will increase the general efficiency of the algorithm.

Based on the figure~\ref{fig:sideeffects}, we define a base ordering of
flaws by type. This order takes into account the number of flaw types
affected by causal side effects.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Alternatives} will cut causal links that have a better
  provider. It is necessary to identify them early since they will add
  at least another subgoal to be fixed as a related flaw.
\item
  \textbf{Subgoals} are the flaws that cause most of the branching
  factor in POCL algorithms. This is why we need to make sure that all
  open conditions are fixed before proceeding on finer refinements.
\item
  \textbf{Orphans} remove unneeded branches of the plan. Yet, these
  branches can be found out to be necessary for the plan to meet a
  subgoal. Since a branch can contain many actions, it is preferable to
  leave the orphan in the plan until they are no longer needed. Also,
  threats involving orphans are invalidated if the orphan is resolved
  first.
\item
  \textbf{Threats} occur quite often in the computation. Searching and
  solving them is computationally expensive since the resolvers need to
  check if there are no paths that fix the flaw already. Many threats
  are generated without the need of resolver application (Peot and Smith
  \protect\hyperlink{ref-peot_threatremoval_1993}{1993}). That is why we
  rank all related subgoals and orphans before threats because they can
  add causal links or remove threatening actions that will fix the
  threat.
\end{enumerate}

Resolvers need to be ordered as well, especially for the subgoal flaws.
Ordering resolvers for a subgoal is the same operation as choosing a
provider. Therefore, the problem becomes ``how to rank operators?''.
Usually, each operator has an assigned cost in the domain, but more
often than not, costs are hard to estimate manually. In our case we need
an automated way to rank operators. The most relevant information on an
operator is how useful it may be to other actions in the plan and how
hard is it to realize.

Since this may be computationally expensive to compute while planning,
the evaluation of the cost of an operator is done offline using the
operator graph.

The first metric to compute this heuristic is the degree of the
operator.

\begin{definition}[Degree of an operator]

Degrees are a measurement of the usefulness of an operator. Such a
notion is derived from the incoming and outgoing degrees of a node in
the operator graph.

We note \(|\chi_{\outgo}(a)|\) being the \emph{outgoing degree} of \(a\)
in the directed graph formed by \(\plan\) and \(|\chi_{\ingo}(a)|\)
being the \emph{incoming degree} of \(a\) in the directed graph formed
by \(\plan\) respectively the outgoing and incoming degrees of an
operator in a plan \(\plan\). These represent the number of causal links
that goes out or toward the operator. We call proper degree of an
operator \(|\eff(a)|\) and \(|\pre(a)|\) the number of preconditions and
effects that reflect its intrinsic usefulness.

\end{definition}

There are several ways to use the degrees as indicators. The
\emph{utility value} increases with every outgoing degree, since this
reflects a positive participation in the plan. It decreases with every
negative degree since actions with higher incoming degrees are harder to
satisfy. The utility value bounds are useful when selecting special
operators. For example, a user-specified constraint could be laid upon
an operator to ensure it is only selected as a last resort. This
operator will be affected with the smallest utility value possible. More
commonly, the highest value is used for initial and goal steps to ensure
their selection.

Our ranking mechanism is based on scores noted \(\reward(a)\). A score
is a tuple of metrics:\footnote{In this section \(\chi\) is the
  connectivity of the operator graph \(g_O\).}

\begin{itemize}
\tightlist
\item
  \(\reward_1(a) = |\chi_{\outgo}(a)|\) is the positive degree of \(a\)
  in the domain operator graph. This will give a measurement of the
  predicted usefulness of the operator.
\item
  \(\reward_2(a) = |\otimes^\downarrowbarred_{a}|\) is the number of
  open conditions of \(a\) in the domain operator graph. This is
  symptomatic of action that can't be satisfied without a compliant
  initial step.
\item
  \(\reward_3(a) = |\pre(a)|\) is the proper negative degree of \(a\).
  Having more preconditions will likely add subgoals.
\item
  \(\reward_4(a) = \lBrace \min_{+\infty}(n) : n \in \bb{N} \land (a \rightarrow a) \in \chi^n \land \chi^n \neq \chi^+ \rBrace\)
  is the size of the shortest cycle involving \(a\) in the operator
  graph or \(+\infty\) if there is none. Having this value at \(1\) is
  usually symptomatic of a \emph{toxic operator}
  (cf.~definition~\ref{def:toxic}). Having an operator behaving this way
  can lead to backtracking because of operator instantiation.
\end{itemize}

The computation of the cost of the operator is done by multiplying the
score tuple with a weighted parameter tuple \(\alpha\) given by the
user. The cost is then:

\[\cost(a) = - \sum_{i=1}^4 \alpha_i\reward_i(a)\]

In practice, \(\alpha_1\) is positive, and the rest is negative. It is
also better to make sure that \(-1 \leq \alpha_4 \leq 0\) so that the
penalties goes down as the cycles gets bigger.

This respects the criteria of having a bound for the \emph{utility
value} as it ensures that it remains positive with \(0\) as a minimum
bound and \(+\infty\) for a maximum. The initial and goal steps have
their utility values set to the upper bound to ensure their selection
over other steps.

Choosing to compute the resolver selection at operator level has some
positive consequences on the performances. Indeed, this computation is
much lighter than approaches with heuristics on plan space (Shekhar and
Khemani \protect\hyperlink{ref-shekhar_learning_2016}{2016}) as it
reduces the overhead caused by real time computation of heuristics on
complex data. In order to reduce this overhead more, the algorithm sorts
the providing associative array to easily retrieve the best operator for
each fluent. This means that the evaluation of the heuristic is done
only once for each operator. This reduces the overhead and allows for
faster results on smaller plans.

\hypertarget{algorithm}{%
\subsection{Algorithm}\label{algorithm}}

The LOLLIPOP algorithm uses the same refinement algorithm as described
in algorithm~\ref{alg:pocl}. The differences reside in the changes made
on the behavior of resolvers and side effects. In
line~\ref{line:resolverapplication} of algorithm~\ref{alg:pocl},
LOLLIPOP algorithm applies negative resolvers if the selected flaw is
negative. In line~\ref{line:sideeffectapplication}, it searches for both
signs of side effects. Another change resides in the initialization of
the solving mechanism and the domain as detailed in
algorithm~\ref{alg:lollipopinit}. This algorithm contains several parts.
First, the \passthrough{\lstinline!domainInit!} function corresponds to
the code computed during the domain compilation time. It will prepare
the rankings, the operator graph, and its caching mechanisms. It will
also use strongly connected component detection algorithm to detect
cycles. These cycles are used during the base score computation
(line~\ref{line:basescore}). We add a detection of illegal fluents and
operators in our domain initialization (line~\ref{line:isillegal}).
Illegal operators are either inconsistent or toxic.

\begin{algorithm}\caption{LOLLIPOP initialization (preprocessing) mechanisms}\label{alg:lollipopinit}\begin{algorithmic}\footnotesize
\Function{domainInit}{Operators $A$}
    \State operatorgraph $g_O$
    \State Score $S$
    \ForAll{Operator $a \in A$}
        \If{\Call{isIllegal}{$a$}} \Comment{Remove toxic and useless fluents} \label{line:isillegal}
            \State $A \gets A \setminus \{a\}$  \Comment{If entirely toxic or useless}
            \Continue
        \EndIf
        \State \Call{addVertex}{$a, g_O$} \Comment{Add and bind all operators}
        \State \Call{cache}{$p, a$} \Comment{Cache operator in providing map}
    \EndFor
    \State Cycles $C \gets$ \Call{stronglyConnectedComponent}{$g_O$} \Comment{Using DFS}
    \State $S \gets$ \Call{baseScores}{$A$, $\mathcal{D}^\Pi$} \label{line:basescore}
    \State $i \gets$ \Call{inapplicables}{$\mathcal{D}^\Pi$}
    \State $e \gets$ \Call{eagers}{$\mathcal{D}^\Pi$}
\EndFunction
\Function{lollipopInit}{Problem $\mathcal{P}$}
    \State \Call{realize}{$S, \mathcal{P}$} \Comment{Realize the scores}
    \State \Call{cache}{$providing, I$} \Comment{Cache initial step in providing ...}
    \State \Call{cache}{$providing, G$} \Comment{... as well as goal step}
    \State \Call{sort}{$providing, S$} \Comment{Sort the providing map}
    \If{$L = \emptyset$}
        \State $\mathcal{P}^\Pi \gets$ \Call{safe}{$\mathcal{P}$} \Comment{Computing the safe operator graph if the plan is empty}
    \EndIf
    \State \Call{populate}{$a$, $\mathcal{P}$} \Comment{populate agenda with first flaws} \label{line:populate}
\EndFunction
\Function{populate}{Agenda $a$, Problem $\mathcal{P}$}
    \ForAll{Update $u \in U$} \Comment{Updates due to online planning}
        \State Fluents $F \gets eff(u.new) \setminus eff(u.old)$ \Comment{Added effects}
        \ForAll{Fluent $f \in F$}
            \ForAll{Operator $o \in$ \Call{better}{$providing$, $f$, $o$}}
                \ForAll{Link $l \in L^+(o)$}
                    \If{$f \in l$}
                        \State \Call{addAlternative}{$a$, $f$, $o$, $l_{\leftarrow}$, $\mathcal{P}$} \Comment{With $l_{\leftarrow}$ the target of $l$}
                    \EndIf
                \EndFor
            \EndFor
        \EndFor
        \State $F \gets eff(u.old) \setminus eff(u.new)$ \Comment{Removed effects}
        \ForAll{Fluent $f \in F$}
            \ForAll{Link $l \in L^+(u.new)$}
                \If{\Call{isLiar}{$l$}}
                    \State $L \gets L \setminus \{l\}$
                    \State \Call{addOrphans}{$a$, $u$, $\mathcal{P}$}
                \EndIf
            \EndFor
        \EndFor
        \State ... \Comment{Same with removed preconditions and incomming liar links}
    \EndFor
    \ForAll{Operator $o \in S$}
        \State \Call{addSubgoals}{$a$, $o$, $\mathcal{P}$}
        \State \Call{addThreats}{$a$, $o$, $\mathcal{P}$}
    \EndFor
\EndFunction\end{algorithmic}\end{algorithm}

\begin{definition}[Inconsistent operators]

An operator \(a\) is contradictory iff
\(\exists f \{f, \lnot f \} \in eff(o) \lor \{f, \lnot f \} \in pre(o)\).

\end{definition}

\begin{definition}[Toxic operators]\label{def:toxic}

Toxic operators have effects that are already in their preconditions or
empty effects. An operator \(o\) is toxic iff
\(pre(o) \cap eff(o) \neq \emptyset \lor eff(o) = \emptyset\).

\end{definition}

Toxic actions can damage a plan as well as make the execution of POCL
algorithm longer than necessary. This is fixed by removing the toxic
fluents (\(pre(a) \nsubseteq eff(a)\)) and by updating the effects with
\(eff(a) = eff(a) \setminus pre(a)\). If the effects become empty, the
operator is removed from the domain.

The {lollipopInit} function is executed during the initialization of the
solving algorithm. We start by realizing the scores, then we add the
initial and goal steps in the providing map by caching them. Once the
ranking mechanism is ready, we sort the providing map. With the ordered
providing map, the algorithm runs the fast generation of the safe
operator graph for the problem's goal.

The last part of this initialization (line~\ref{line:populate}) is the
agenda population that is detailed in the {populate} function. During
this step, we perform a search of alternatives based on the list of
updated fluents. Online updates can make the plan outdated relative to
the domain. This forms liar links :

\begin{definition}[Liar links]

A liar link is a link that doesn't hold a fluent in the preconditions or
effect of its source and target. We note:
\[a_i \xrightarrow{f} a_j | f \notin eff(a_i) \cap pre(a_j)\]

\end{definition}

A liar link can be created by the removal of an effect or preconditions
during online updates (with the causal link still remaining).

We call lies the fluents that are held by links without being in the
connected operators. To resolve the problem, we remove all lies. We
delete the link altogether if it doesn't bear any fluent as a result of
this operation. This removal triggers the addition of orphan flaws as
side effects.

While the list of updated operators is crucial for solving online
planning problems, a complementary mechanism is used to ensure that
LOLLIPOP is complete. User provided plans have their steps tagged. If
the failure has backtracked to a user-provided step, then it is removed
and replaced by subgoals that represent each of its participation in the
plan. This mechanism loops until every user provided steps have been
removed.

\hypertarget{theoretical-and-empirical-results}{%
\subsection{Theoretical and Empirical
Results}\label{theoretical-and-empirical-results}}

As proven in (Penberthy \emph{et al.}
\protect\hyperlink{ref-penberthy_ucpop_1992}{1992}), the classical POCL
algorithm is \emph{sound} and \emph{complete}.

First, we define some new properties of partial plans. The following
properties are taken from the original proof. We present them again for
convenience.

\begin{definition}[Full Support]\label{def:fullsupport}

A partial plan \(\pi\) is fully supported if each of its steps
\(o \in S\) is fully supported. A step is fully supported if each of its
preconditions \(f \in pre(o)\) is supported. A precondition is fully
supported if there exists a causal link \(l\) that provides it. We note:
\[\Downarrow \pi \equiv
\begin{array}{l}
    \forall o \in S \thickspace \forall f \in pre(o) \thickspace \exists l \in L_\pi^-(o): \\
        \left(f \in l \land \not \exists t \in S (l_{\rightarrow} \succ t \succ o \land \lnot f \in eff(t))\right)
\end{array}\] with \(L_\pi^-(o)\) being the incoming causal links of
\(o\) in \(\pi\) and \(l_{\rightarrow}\) being the source of the link.

\end{definition}

\begin{definition}[Partial Plan Validity]\label{def:partialplanvalidity}

A partial plan is a \textbf{valid solution} of a problem \(\mathcal{P}\)
if it is \emph{fully supported} and \emph{contains no cycles}. The
validity of \(\pi\) regarding a problem \(\mathcal{P}\) is noted
\(\pi \models \left( \mathcal{P} \equiv \Downarrow \pi \land \left(C(\pi) = \emptyset \right) \right)\)
with \(C(\pi)\) being the set of cycles in \(\pi\).

\end{definition}

\hypertarget{proof-of-soundness}{%
\subsubsection{Proof of Soundness}\label{proof-of-soundness}}

In order to prove that this property applies to LOLLIPOP, we need to
introduce some hypothesis:

\begin{itemize}
\tightlist
\item
  operators updated by online planning are known.
\item
  user provided steps are known.
\item
  user provided plans don't contain illegal artifacts. This includes
  toxic or inconsistent actions, lying links and cycles.
\end{itemize}

Based on the definition~\ref{def:partialplanvalidity} we state that: \[
\left(
\begin{array}{l}
    \forall pre \in pre(G): \\
    \Downarrow pre \land
    \begin{array}{l}
        \forall o \in L_\pi^-(G)_{\rightarrow} \thickspace \forall pre' \in pre(o): \\
        \left(\Downarrow pre'\land C_o(\pi) = \emptyset\right)
    \end{array}
\end{array}
\right) \implies \pi \models \mathcal{P}\] \{\#eq:recursivevalidity\}
where \(L_\pi^-(G)_{\rightarrow}\) is the set of direct antecedents of
\(G\) and \(C_o(\pi)\) is the set of fluents containing \(o\) in
\(\pi\).

This means that \(\pi\) is a solution if all preconditions of \(G\) are
satisfied. We can satisfy these preconditions using operators if and
only if their preconditions are all satisfied and if there is no other
operator that threatens their supporting links.

First, we need to prove that equation~\ref{eq:recursivevalidity} holds
on LOLLIPOP initialization. We use our hypothesis to rule out the case
when the input plan is invalid. The
algorithm~\ref{alg:safeoperatorgraph} will only solve open conditions in
the same way subgoals do it. Thus, safe operator graphs are valid input
plans.

Since the soundness is proven for regular refinements and flaw
selection, we need to consider the effects of the added mechanisms of
LOLLIPOP. The newly introduced refinements are negative, they don't add
new links:

\[\forall f \in \mathcal{F}(\pi) \thickspace \forall r \in r(f): C_\pi(f.n) = C_{f(\pi)}(f.n)\]
\{\#eq:nocycle\} with \(\mathcal{F}(\pi)\) being the set of flaws in
\(\pi\), \(r(f)\) being the set of resolvers of \(f\), \(f.n\) being the
needer of the flaw and \(f(\pi)\) being the resulting partial plan after
the application of the flaw. Said otherwise, an iteration of LOLLIPOP
won't add cycles inside a partial plan.

The orphan flaw targets steps that have no path to the goal and so can't
add new open conditions or threats. The alternative targets existing
causal links. Removing a causal link in a plan breaks the full support
of the target step. This is why an alternative will always insert a
subgoal in the agenda corresponding to the target of the removed causal
link. Invalidating side effects also doesn't affect the soundness of the
algorithm since the removed flaws are already solved. This makes: \[
\forall f \in \mathcal{F}^-(\pi): \Downarrow \pi \implies \Downarrow f(\pi)
\] \{\#eq:conssupport\} with \(\mathcal{F}^-(\pi)\) being the set of
negative flaws in the plan \(\pi\). This means that negative flaws don't
compromise the full support of the plan.

Equation~\ref{eq:nocycle} lead to equation~\ref{eq:recursivevalidity}
being valid after the execution of LOLLIPOP. The algorithm is sound.

\hypertarget{proof-of-completeness}{%
\subsubsection{Proof of Completeness}\label{proof-of-completeness}}

The soundness proof shows that LOLLIPOP's refinements don't affect the
support of plans in terms of validity. It was proven that POCL is
complete. There are several cases to explore to transpose the property
to LOLLIPOP:

\begin{lemma}[Conservation of Validity]

If the input plan is a valid solution, LOLLIPOP returns a valid
solution.

\end{lemma}

\begin{proof}

With equation~\ref{eq:nocycle} and the proof of soundness, the
conservation of validity is already proven. \qedhere

\end{proof}

\begin{lemma}[Reaching Validity with incomplete partial plans]\label{lem:incompletevalidity}

If the input plan is incomplete, LOLLIPOP returns a valid solution if it
exists.

\end{lemma}

\begin{proof}

Since POCL is complete and the equation~\ref{eq:conssupport} proves the
conservation of support by LOLLIPOP, then the algorithm will return a
valid solution if the provided plan is an incomplete plan and the
problem is solvable. \qedhere

\end{proof}

\begin{lemma}[Reaching Validity with empty partial plans]

If the input plan is empty and the problem is solvable, LOLLIPOP returns
a valid solution.

\end{lemma}

\begin{proof}

This is proven using \textbf{BEFORE} and POCL's completeness. However,
we want to add a trivial case to the proof: \(pre(G) = \emptyset\). In
this case the line~\ref{line:emptygoal} of the algorithm~\ref{alg:pocl}
will return a valid plan.

\end{proof}

\begin{lemma}[Reaching Validity with a dead-end partial plan]\label{lem:deadend-validity}

If the input plan is in a dead-end, LOLLIPOP returns a valid solution.

\end{lemma}

\begin{proof}

Using input plans that can be in an undetermined state is not covered by
the original proof. The problem lies in the existing steps in the input
plan. Still, using our hypothesis we add a failure mechanism that makes
LOLLIPOP complete. On failure, the needer of the last flaw is deleted if
it wasn't added by LOLLIPOP. User defined steps are deleted until the
input plan acts like an empty plan. Each deletion will cause
corresponding subgoals to be added to the agenda. In this case, the
backtracking is preserved and all possibilities are explored as in POCL.
\qedhere

\end{proof}

Since all cases are covered, this proves the property of completeness.

\hypertarget{sec:results}{%
\subsubsection{Experimental Results}\label{sec:results}}

The experimental results focused on the properties of LOLLIPOP for
online planning. Since classical POCL is unable to perform online
planning, we tested our algorithm considering the time taken for solving
the problem for the first time. We profiled the algorithm on a benchmark
problem containing each of the possible issues described earlier.

\begin{figure}
\hypertarget{fig:experiment}{%
\centering
\includegraphics{graphics/lollipop_domain.svg}
\caption{Domain used to compute the results. First line is the initial
and goal steps along with the useful actions. Second line contains a
threatening action \(t\), two co-dependent actions \(n\) and \(l\), a
useless action \(u\), a toxic action \(v\), a dead-end action \(w\) and
an inconsistent action \(x\)}\label{fig:experiment}
}
\end{figure}

In figure~\ref{fig:experiment}, we expose the planning domain used for
the experiments. During the domain initialization, the actions \(u\) and
\(v\) are eliminated from the domain since they serve no purpose in the
solving process. The action \(x\) is stripped of its negative effect
because it is inconsistent with the effect \(f_2\).

As the solving starts, LOLLIPOP computes a safe operator graph (full
lines in figure~\ref{fig:experimentplan}). As we can see, this partial
plan is nearly complete already. When the main refining function starts
it receives an agenda with only a few flaws remaining.

\begin{figure}
\hypertarget{fig:experimentplan}{%
\centering
\includegraphics{graphics/lollipop_solution.svg}
\caption{In full lines the initial safe operator graph. In thin, sparse
and irregularly dotted lines respectively a subgoal, alternative and
threat caused causal link.}\label{fig:experimentplan}
}
\end{figure}

\textbf{FIXME: names to indexes}

Then the main refinement function starts (time markers \textbf{1}).
LOLLIPOP selects as resolver a causal link from \(a\) to satisfy the
open condition of the goal step. Once the first threat between \(a\) and
\(t\) is resolved the second threat is invalidated. On a second
execution, the domain changes for online planning with \(6\) added to
the initial step. This solving (time markers \textbf{2}) adds as flaw an
alternative on the link from \(c\) to the goal step. A subgoal is added
that links the initial and goal step for this fluent. An orphan flaw is
also added that removes \(c\) from the plan. Another solving takes place
as the goal step doesn't need \(3\) as a precondition (time markers
\textbf{3}). This causes the link from \(a\) to be cut since it became a
liar link. This adds \(a\) as an orphan that gets removed from the plan
even if it was hanging by the bare link to \(t\).

The measurements exposed in table~\ref{tbl:results} were made with an
Intel® Core™ i7-4720HQ with a 2.60GHz clock. Only one core was used for
the solving. The same experiment done only with the chronometer code
gave a result of \(70 ns\) of errors. We can see an increase of
performance in the online runs because of the way they are conducted by
LOLLIPOP.

\begin{longtable}[]{@{}llll@{}}
\caption{Average times of \(1.000\) executions on the problem. The first
column is for a simple run on the problem. Second and third columns are
times to replan with one and two changes done to the domain for online
planning. \{\#tbl:results\}}\tabularnewline
\toprule
\textbf{Experiment} & \emph{Single} & \emph{Online 1} & \emph{Online
2}\tabularnewline
\midrule
\endfirsthead
\toprule
\textbf{Experiment} & \emph{Single} & \emph{Online 1} & \emph{Online
2}\tabularnewline
\midrule
\endhead
\textbf{Time (\(ms\))} & \(0.86937\) & \(0.38754\) &
\(0.48123\)\tabularnewline
\bottomrule
\end{longtable}

\textbf{TODO: section that explains why LOLLIIPOP failed}

\hypertarget{heart}{%
\section{HEART}\label{heart}}

\hypertarget{domain-compilation}{%
\subsection{Domain Compilation}\label{domain-compilation}}

In order to simplify the input of the domain, the causes of the causal
links in the methods are optional. If omitted, the causes are inferred
by unifying the preconditions and effects with the same mechanism as in
the subgoal resolution in our POCL algorithm. Since we want to guarantee
the validity of abstract plans, we need to ensure that user provided
plans are solvable. We use the following formula to compute the final
preconditions and effects of any composite action \(a\):
\(\mathit{pre}(a) = \bigcup_{l \in L^+(I_m)}^{m \in methods(a)} \mathit{causes}(l)\)
and
\(\mathit{eff}(a) = \bigcup_{l \in L^-(G_m))}^{m \in methods(a)} \mathit{causes}(l)\).
An instance of the classical POCL algorithm is then run on the problem
\(\mathcal{P}_a = \langle \mathcal{D}, C_{\mathcal{P}} , a\rangle\) to
ensure its coherence. The domain compilation fails if POCL cannot be
completed. Since our decomposition hierarchy is acyclic
(\(a \notin A_a\), see definition~\ref{def:proper}) nested methods
cannot contain their parent's action as a step.

\hypertarget{abstraction-in-pocl}{%
\subsection{Abstraction in POCL}\label{abstraction-in-pocl}}

In order to properly introduce the changes made for using HTN domains in
POCL, we need to define a few notions.

Transposition is needed to define decomposition.

\begin{definition}[Transposition]

In order to transpose the causal links of an action \(a'\) with the ones
of an existing step \(a\) in a plan \(\pi\), we use the following
operation:

\[a \rhd^-_\pi a' = \left \lbrace \phi^-(l) \xrightarrow{\mathit{causes}(l)} a': l \in L^-_\pi(a) \right \rbrace\]

It is the same with \(a' \xrightarrow{\mathit{causes}(l)} \phi^+(l)\)
and \(L^+\) for \(a \rhd^+ a'\). This supposes that the respective
preconditions and effects of \(a\) and \(a'\) are equivalent. When not
signed, the transposition is generalized:
\(a \rhd a' = a\rhd^-a' \cup a\rhd^+a'\).

\end{definition}

\begin{example}

\(a\rhd^-a'\) gives all incoming links of \(a\) with the \(a\) replaced
by \(a'\).

\end{example}

\begin{definition}[Proper Actions]\label{def:proper}

Proper actions are actions that are ``contained'' within an entity
(either a domain, plan or action). We note this notion
\(A_a = A_a^{lv(a)}\) for an action \(a\). It can be applied to various
concepts:

\begin{itemize}
\tightlist
\item
  For a \emph{domain} or a \emph{problem},
  \(A_{\mathcal{P}} = A_{\mathcal{D}}\).
\item
  For a \emph{plan}, it is \(A^0_\pi = S_\pi\).
\item
  For an \emph{action}, it is
  \(A^0_a = \bigcup_{m \in \mathit{methods}(a)} S_m\). Recursively:
  \(A_a^n = \bigcup_{b\in A_a^0} A_{b}^{n-1}\). For atomic actions,
  \(A_a = \emptyset\).
\end{itemize}

\end{definition}

\begin{example}

The proper actions of \(make(drink)\) are the actions contained within
its methods. The set of extended proper actions adds all proper actions
of its single proper composite action \(infuse(drink, water, cup)\).

\end{example}

\begin{definition}[Abstraction Level]\label{def:level}

This is a measure of the maximum amount of abstraction an entity can
express, defined recursively by:\footnote{We use Iverson brackets here,
  see notations in table~\ref{tbl:symbols}.}
\[lv(x) = \left ( \max_{a \in A_x}(lv(a)) + 1 \right ) [A_x \neq \emptyset]\]

\end{definition}

\begin{example}

The abstraction level of any atomic action is \(0\) while it is \(2\)
for the composite action \(make(drink)\). The example domain (in
listing~\ref{lst:domain}) has an abstraction level of \(3\).

\end{example}

The most straightforward way to handle abstraction in regular planners
is illustrated by Duet (Gerevini \emph{et al.}
\protect\hyperlink{ref-gerevini_combining_2008}{2008}) by managing
hierarchical actions separately from a task insertion planner. We chose
to add abstraction in POCL in a manner inspired by the work of Bechon
\emph{et al.} (\protect\hyperlink{ref-bechon_hipop_2014}{2014}) on a
planner called HiPOP. The difference between the original HiPOP and our
implementation of it is that we focus on the expressivity and the ways
flaw selection can be exploited for partial resolution. Our version is
lifted at runtime while the original is grounded for optimizations. All
mechanisms we have implemented use POCL but with different management of
flaws and resolvers. The original algorithm~\ref{alg:pocl} is left
untouched.

One of those changes is that resolver selection needs to be altered for
subgoals. Indeed, as stated by the authors of HiPOP: the planner must
ensure the selection of high-level operators in order to benefit from
the hierarchical aspect of the domain. Otherwise, adding operators only
increases the branching factor. Composite actions are not usually meant
to stay in a finished plan and must be decomposed into atomic steps from
one of their methods.

\begin{definition}[Decomposition Flaws]\label{def:decomposition}

They occur when a partial plan contains a non-atomic step. This step is
the needer \(a_n\) of the flaw. We note its decomposition
\(a_n \oplus\).

\begin{itemize}
\tightlist
\item
  \emph{Resolvers:} A decomposition flaw is solved with a
  \textbf{decomposition resolver}. The resolver will replace the needer
  with one of its instantiated methods \(m \in \mathit{methods}(a_n)\)
  in the plan \(\pi\). This is done by using transposition such that:
  \(a_n \oplus^m_\pi = \langle S_m \cup (S_\pi \setminus \lbrace a \rbrace) , a_n \rhd^- I_m \cup a_n \rhd^+ G_m \cup (L_\pi \setminus L_\pi(a_n))\).
\item
  \emph{Side effects:} A decomposition flaw can be created by the
  insertion of a composite action in the plan by any resolver and
  invalidated by its removal:
  \[\bigcup^{f \in \mathit{pre}(a_m)}_{a_m \in S_m} \pi' \downarrowbarred_f a_m\bigcup^{l \in L_{\pi'}}_{a_b \in S_{\pi'}} a_b \olcross l \bigcup_{a_c \in S_m}^{lv(a_c) \neq 0} a_c \oplus\]
\end{itemize}

\end{definition}

\begin{example}

When adding the step \(make(tea)\) in the plan to solve the subgoal that
needs tea being made, we also introduce a decomposition flaw that will
need this composite step replaced by its method using a decomposition
resolver. In order to decompose a composite action into a plan, all
existing links are transposed to the initial and goal step of the
selected method, while the composite action and its links are removed
from the plan.

\end{example}

The main differences between HiPOP and HEART in our implementations are
the functions of flaw selection and the handling of the results (one
plan for HiPOP and a plan per cycle for HEART). In HiPOP, the flaw
selection is made by prioritizing the decomposition flaws. Bechon
\emph{et al.} (\protect\hyperlink{ref-bechon_hipop_2014}{2014}) state
that it makes the full resolution faster. However, it also loses
opportunities to obtain abstract plans in the process.

\hypertarget{planning-in-cycle}{%
\subsection{Planning in cycle}\label{planning-in-cycle}}

The main focus of our work is toward obtaining \textbf{abstract plans}
which are plans that are completed while still containing composite
actions. In order to do that the flaw selection function will enforce
cycles in the planning process.

\begin{definition}[Cycle]

A cycle is a planning phase defined as a triplet
\(c = \langle lv(c), agenda, \pi_{lv(c)}\rangle\) where: \(lv(c)\) is
the maximum abstraction level allowed for flaw selection in the
\(agenda\) of remaining flaws in partial plan \(\pi_{lv(c)}\). The
resolvers of subgoals are therefore constrained by the following:
\(a_p \downarrow_f a_n: lv(a_p) \leq lv(c)\).

\end{definition}

During a cycle all decomposition flaws are delayed. Once no more flaws
other than decomposition flaws are present in the agenda, the current
plan is saved and all remaining decomposition flaws are solved at once
before the abstraction level is lowered for the next cycle:
\(lv(c') = lv(c)-1\). Each cycle produces a more detailed abstract plan
than the one before.

Abstract plans allow the planner to do an approximate form of anytime
execution. At any given time the planner is able to return a fully
supported plan. Before the first cycle, the plan returned is
\(\pi_{lv(a_0)}\).

\begin{example}

In our case using the method of intent recognition of Sohrabi \emph{et
al.} Sohrabi \emph{et al.}
(\protect\hyperlink{ref-sohrabi_plan_2016}{2016}), we can already use
\(\pi_{lv(a_0)}\) to find a likely goal explaining an observation (a set
of temporally ordered fluents). That can make an early assessment of the
probability of each goal of the recognition problem.

\end{example}

For each cycle \(c\), a new plan \(\pi_{lv(c)}\) is created as a new
method of the root operator \(a_0\). These intermediary plans are not
solutions of the problem, nor do they mean that the problem is solvable.
In order to find a solution, the HEART planner needs to reach the final
cycle \(c_0\) with an abstraction level \(lv(c_0) = 0\). However, these
plans can be used to derive meaning from the potential solution of the
current problem and give a good approximation of the final result before
its completion.

\begin{example}

\begin{figure}
\hypertarget{fig:cycles}{%
\centering
\includegraphics{graphics/cycles.svg}
\caption{Illustration of how the cyclical approach is applied on the
example domain. Atomic actions that are copied from a cycle to the next
are omitted.}\label{fig:cycles}
}
\end{figure}

In the figure~\ref{fig:cycles}, we illustrate the way our problem
instance is progressively solved. Before the first cycle \(c_2\), all we
have is the root operator and its plan \(\pi_3\). Then within the first
cycle, we select the composite action \(make(tea)\) instantiated from
the operator \(make(drink)\) along with its methods. All related flaws
are fixed until all that is left in the agenda is the abstract flaws. We
save the partial plan \(\pi_2\) for this cycle and expand \(make(tea)\)
into a copy of the current plan \(\pi_1\) for the next cycle. The
solution of the problem will be stored in \(\pi_0\) once found.

\end{example}

\hypertarget{properties-of-abstract-planning}{%
\subsection{Properties of Abstract
Planning}\label{properties-of-abstract-planning}}

In this section, we prove several properties of our method and resulting
plans: HEART is complete, sound and its abstract plans can always be
decomposed into a valid solution.

The completeness and soundness of POCL has been proven in (Penberthy
\emph{et al.} \protect\hyperlink{ref-penberthy_ucpop_1992}{1992}). An
interesting property of POCL algorithms is that flaw selection
strategies do not impact these properties. Since the only modification
of the algorithm is the extension of the classical flaws with a
decomposition flaw, all we need to explore, to update the proofs, is the
impact of the new resolver. By definition, the resolvers of
decomposition flaws will take into account all flaws introduced by its
resolution into the refined plan. It can also revert its application
properly.

\begin{lemma}[Decomposing preserves acyclicity]

The decomposition of a composite action with a valid method in an
acyclic plan will result in an acyclic plan. Formally
\(\forall a_s \in S_\pi: a_s \nsucc_\pi a_s \implies \forall a'_s \in S_{a \oplus^m_\pi}: a'_s \nsucc_{a \oplus^m_\pi} a'_s\).

\end{lemma}

\begin{proof}

When decomposing a composite action \(a\) with a method \(m\) in an
existing plan \(\pi\), we add all steps \(S_m\) in the refined plan.
Both \(\pi\) and \(m\) are guaranteed to be cycle free by definition. We
can note that
\(\forall a_s \in S_m: \left ( \nexists a_t \in S_m: a_s \succ a_t \land \lnot f \in \mathit{eff}(a_t)\right ) \implies f \in \mathit{eff}(a)\).
Said otherwise, if an action \(a_s\) can participate a fluent \(f\) to
the goal step of the method \(m\) then it is necessarily present in the
effects of \(a\). Since higher level actions are preferred during the
resolver selection, no actions in the methods are already used in the
plan when the decomposition happens. This can be noted
\(\exists a \in \pi \implies S_m \cupdot S_\pi\) meaning that in the
graph formed both partial plans \(m\) and \(\pi\) cannot contain the
same edges therefore their acyclicity is preserved when inserting one
into the other.

\end{proof}

\begin{lemma}[Solved decomposition flaws cannot reoccur]

The application of a decomposition resolver on a plan \(\pi\),
guarantees that \(a \notin S_{\pi'}\) for any partial plan refined from
\(\pi\) without reverting the application of the resolver.

\end{lemma}

\begin{proof}

As stated in the definition of the methods
(definition~\ref{def:action}): \(a \notin A_a\). This means that \(a\)
cannot be introduced in the plan by its decomposition or the
decomposition of its proper actions. Indeed, once \(a\) is expanded, the
level of the following cycle \(c_{lv(a)-1}\) prevents \(a\) to be
selected by subgoal resolvers. It cannot either be contained in the
methods of another action that are selected afterward because otherwise
following definition~\ref{def:level} its level would be at least
\(lv(a)+1\).

\end{proof}

\begin{lemma}[Decomposing to abstraction level 0 guarantees solvability]

Finding a partial plan that contains only decomposition flaws with
actions of abstraction level 1, guarantees a solution to the problem.

\end{lemma}

\begin{proof}

Any method \(m\) of a composite action \(a: lv(a) = 1\) is by definition
a solution of the problem
\(\mathcal{P}_a = \langle \mathcal{D}, C_{\mathcal{P}} , a\rangle\). By
definition, \(a \notin A_a\), and \(a \notin A_{a \oplus^m_\pi}\)
(meaning that \(a\) cannot reoccur after being decomposed). It is also
given by definition that the instantiation of the action and its methods
are coherent regarding variable constraints (everything is instantiated
before selection by the resolvers). Since the plan \(\pi\) only has
decomposition flaws and all flaws within \(m\) are guaranteed to be
solvable, and both are guaranteed to be acyclical by the application of
any decomposition \(a \oplus^m_\pi\), the plan is solvable.

\end{proof}

\begin{lemma}[Abstract plans guarantee solvability]

Finding a partial plan \(\pi\) that contains only decomposition flaws,
guarantees a solution to the problem.

\end{lemma}

\begin{proof}

Recursively, if we apply the previous proof on higher level plans we
note that decomposing at level 2 guarantees a solution since the method
of the composite actions are guaranteed to be solvable.

\end{proof}

From these proofs, we can derive the property of soundness (from the
guarantee that the composite action provides its effects from any
methods) and completeness (since if a composite action cannot be used,
the planner defaults to using any action of the domain).

\hypertarget{computational-profile}{%
\subsection{Computational Profile}\label{computational-profile}}

In order to assess its capabilities, HEART was evaluated on two
criteria: quality and complexity. All tests were executed on an Intel®
Core™ i7-7700HQ CPU clocked at 2.80GHz. The Java process used only one
core and was not limited by time or memory (32 GB that wasn't entirely
used up) . Each experiment was repeated between \(700\) and \(10 000\)
times to ensure that variations in speed were not impacting the results.

\begin{figure}
\hypertarget{fig:quality}{%
\centering
\includegraphics{graphics/quality-speed.svg}
\caption{Evolution of the quality with computation
time.}\label{fig:quality}
}
\end{figure}

Figure~\ref{fig:quality} shows how the quality is affected by the
abstraction in partial plans. The tests are made using our example
domain (see listing~\ref{lst:domain}). The quality is measured by
counting the number of providing fluents in the plan
\(\left| \bigcup_{a \in S_\pi} \mathit{eff}(a) \right|\). This metric is
actually used to approximate the probability of a goal given
observations in intent recognition (\(P(G|O)\) with noisy observations,
see (Sohrabi \emph{et al.}
\protect\hyperlink{ref-sohrabi_plan_2016}{2016})). The percentages are
relative to the total number of unique fluents of the complete solution.
These results show that in some cases it may be more interesting to plan
in a leveled fashion to solve HTN problems. For the first cycle of level
\(3\), the quality of the abstract plan is already of \(60\%\). This is
the quality of the exploitation of the plan \emph{before any planning}.
With almost three quarters of the final quality and less than half of
the complete computation time, the result of the first cycle is a good
quality/time compromise.

\begin{figure}
\hypertarget{fig:width}{%
\centering
\includegraphics{graphics/level-spread.svg}
\caption{Impact of domain shape on the computation time by levels. The
scale of the vertical axis is logarithmic. Equations are the definition
of the trend curves.}\label{fig:width}
}
\end{figure}

In the second test, we used generated domains. These domains consist of
an action of abstraction level \(5\). This action has a single method
containing a number of actions of levels \(4\). We call this number the
width of the domain. All needed actions are built recursively to form a
tree shape. Atomic actions only have single fluent effects. The goal is
the effect of the higher level action and the initial state is empty.
These domains do not contain negative effects. Figure~\ref{fig:width}
shows the computational profile of HEART for various levels and widths.
We note that the behavior of HEART seems to follow an exponential law
with the negative exponent of the trend curves seemingly being
correlated to the actual width. This means that computing the first
cycles has a complexity that is close to being \emph{linear} while
computing the last cycles is of the same complexity as classical
planning which is at least \emph{P-SPACE} (depending on the expressivity
of the domain) (Erol \emph{et al.}
\protect\hyperlink{ref-erol_complexity_1995}{1995}).

\hypertarget{ch:rico}{%
\chapter{Toward Intent Recognition}\label{ch:rico}}

Since the original goal of this thesis was on intent recognition of
dependent persons, we need to explain some more about that specific
domain. The problem is to infer the goals of an external agent through
only observations without intervention. In the end, the idea is to infer
that goal confidently enough to start assisting the other agent without
explicit instructions.

\hypertarget{domain-problems}{%
\section{Domain problems}\label{domain-problems}}

This field was widely studied. Indeed, at the end of the last century,
several works started using \emph{abduction} to infer intents from
observational data. Of course this comes as a challenge since there is a
lot of uncertainty involved. We have no reliable information on the
possible goals of the other agent and we don't have the same knowledge
of the world. Also, observations can sometimes be incomplete or
misleading, the agent can abandon goals or pursue several goals at once
while also doing them sub-optimally or even fail at them. To finish,
sometimes agents can actively try to hide their intents to the viewer.

\hypertarget{observations-and-inferences}{%
\subsection{Observations and
inferences}\label{observations-and-inferences}}

As explained above, we can only get close to reality and any progress in
relevance and detail is exponentially expensive in terms of computing
and memory resources. That is why any system will maintain a high degree
of abstraction that will cause errors inherent in this approximation.

This noise phenomenon can impact the activity and situation recognition
system and therefore seriously impact the intention recognition and
decision-making system with an amplification of the error as it is
processed. It is also important to remember that this phenomenon of data
noise is also present in inhibition and that the lack of perception of
an event is as disabling as the perception of the wrong event.

It is possible to protect recognition systems in an appropriate way, but
this often implies a restriction on the levels of possibilities offered
by the system such as specialized recognition or recognition at a lower
level of relevance.

\hypertarget{cognitive-inconsistencies}{%
\subsection{Cognitive inconsistencies}\label{cognitive-inconsistencies}}

In the field of personal assistance, activity recognition is a crucial
element. However, it happens that these events are very difficult to
recognize. The data noise mentioned above can easily be confused with an
omission on the part of the observed agent. This dilemma is also present
during periods of inactivity, the system can start creating events from
scratch to fill what it may perceive as inhibition noise.

These problems are accompanied by others related to the behavior of the
observed agent. For example, they may perform unnecessary steps, restart
ongoing activities or suddenly abandon them. It is added that other
aspects of observations can make automated inferences such as ambiguous
actions or the agent performing an action that resembles another
complicated.

However, some noise problems can be easily detected by simple cognitive
processes such as impossible sequences (e. g. closing a closed door).
Contextual analyses provide a partial solution to some of these
problems.

\hypertarget{sequentiality}{%
\subsection{Sequentiality}\label{sequentiality}}

Since our recognition is based on a highly temporal planning aspect, we
must take into account the classic problems of sequentiality.

A first problem is to determine the end of one plan and the beginning of
another. Indeed, it is possible that some transitions between two planes
may appear to be a plane in itself and therefore may cause false
positives. Another problem is that of intertwined planes. A person can
do two things at once, such as answering the phone while cooking. An
action in an intertwined plan can then be identified as a
discontinuation of activity or a logical inconsistency. A final problem
is that of overloaded actions. Not only can an agent perform two tasks
simultaneously, but also perform an action that contributes to two
activities. These overloaded actions make the process of intention
recognition complex because they are close to data noise.

\hypertarget{existing-approaches}{%
\section{Existing approaches}\label{existing-approaches}}

The problem of intention recognition has been strongly addressed from
many angles. It is therefore not surprising that there are many
paradigms in the field. The first studies on the subject highlight the
fact that intention recognition problems are problems of abductive logic
or graph coverage. Since then, many models have competed in imagination
and innovation to improve the field. These include constraint
system-based models that provide a solution based on pre-established
rules and compiled plan libraries, those that use state or action
networks that then launch algorithms on this data, and reverse planning
systems.

\hypertarget{constraint}{%
\subsection{Constraint}\label{constraint}}

One of the approaches to intention recognition is the one that builds a
system around a strong logical constraint. There is often a time
constraint system that is complemented by various extensions to cover as
many sequential problems as possible.

\hypertarget{deductive-approach}{%
\subsubsection{Deductive Approach}\label{deductive-approach}}

In order to solve a problem of intention recognition, abductive logic
can be used. Contrary to deductive logic, the goal is to determine the
objective from the observed actions. Among the first models introduced
is Goldman \emph{et al.}
(\protect\hyperlink{ref-goldman_new_1999}{1999})'s model, which uses the
principle of action to construct a logical representation of the
problem. This paradigm consists in creating logical rules as if the
action in question was actually carried out, but in hypothesizing the
predicates that concretize the action and thus being able to browse the
research space thus created in order to find all the possible plans
containing the observed actions and concretizing defined intentions.
This model is strongly based on first-order logic and SWI Prolog logic
programming languages. Although revolutionary for the time, this system
pale in comparison to recent systems, particularly in terms of
prediction accuracy.

\hypertarget{algebraic-approach}{%
\subsubsection{Algebraic Approach}\label{algebraic-approach}}

Some paradigms use algebra to determine possible plans from observed
actions. In particular, we find the model of Bouchard \emph{et al.}
(\protect\hyperlink{ref-bouchard_smart_2006}{2006}) which extends the
subsumption relationship from domain theory to the description of action
and sequence of action in order to introduce it as an order relationship
in the context of the construction of a lattice composed of possible
plans considering the observed actions. This model simply takes into
account the observed actions and selects any plan from the library of
plans that contains at least one observed action. Then this paradigm
will construct all the possible plans that correspond to the Cartesian
product of the observed actions with the actions contained in the
selected plans (while respecting their order). This system makes it
possible to obtain a subsumption relationship that corresponds to the
fact that the plans are more or less general. Unfortunately, this
relationship alone does not provide any information on which plan is
most likely.

\begin{figure}
\centering
\includegraphics{graphics/bouchard.svg}
\caption{The lattice formed by observations (top), matching plans,
possible hypothesis and problem (bottom)}
\end{figure}

That is why Roy \emph{et al.}
(\protect\hyperlink{ref-roy_possibilistic_2011}{2011}) created a
probabilistic extension of this model. This uses frequency data from a
system learning period to calculate the influence probabilities of each
plane in the recognition space. This makes it possible to calculate
probabilistic intervals for each plan, action as well as for a plan to
know a given action. In order to determine the probability of each plane
knowing the upper bound of the lattice (plane containing all observed
actions) the sum of the conditional probabilities of the plane for each
observed action divided by the number of observed actions is made. This
gives a probability interval for each plane allowing the ordinates. This
model has the advantage of considering many possible plans but has the
disadvantage of seeing a computational explosion as soon as the number
of observed actions increases and the context is not taken into account.

\hypertarget{grammatical-approach}{%
\subsubsection{Grammatical Approach}\label{grammatical-approach}}

Another approach is that of grammar. Indeed, we can consider actions as
words and sequences as sentences and thus define a system that allows us
to recognize shots from incomplete sequences. Vidal \emph{et al.}
(\protect\hyperlink{ref-vidal_online_2010}{2010}) has therefore created
a system of intention recognition based on grammar. It uses the
evaluated grammar system to specify measurements from observations.
These measures will make it possible to select specific plans and thus
return a hierarchical hypothesis tree with the actions already carried
out, the future and the plans from which they are derived. This model is
very similar to first order logic-based systems, and uses a SWI Prolog
type logic language programming system. Given the scope of maritime
surveillance, this model, although taking very well into account the
context and the evolution of the measures, is only poorly adapted to an
application in assistance, particularly in the absence of a system for
discriminating against results plans.

\begin{figure}
\centering
\includegraphics{graphics/vidal.svg}
\caption{The valued grammar used for intent recognition}
\end{figure}

\hypertarget{linear-programming-approach}{%
\subsubsection{Linear programming
approach}\label{linear-programming-approach}}

Another class of approaches is that of diverting standard
problem-solving tools to solve the problem of intention recognition. It
is therefore possible, by modifying traditional algorithms or by
transforming a problem, to ensure that the solution of the tool
corresponds to the one sought.

Inoue and Inui (\protect\hyperlink{ref-inoue_ilpbased_2011}{2011})
develops the idea of a model that uses linear programming to solve the
recognition problem. Indeed, observations are introduced in the form of
causes in relation to hypotheses, in a first-order logic predicate
system. Each atom is then weighed and introduced into a process of
problem transformation by feedback and the introduction of order and
causality constraints in order to force the linear program towards
optimal solutions by taking into account observations. Although
ingenious, this solution does not discriminate between possible plans
and is very difficult to apply to real-time recognition situations,
mainly because of the problem transformation procedure required each
time the problem is updated.

\hypertarget{markovian-logic-approach}{%
\subsubsection{Markovian Logic
Approach}\label{markovian-logic-approach}}

Another constraint paradigm is the one presented by Raghavana \emph{et
al.} using a Markovian extension of first-order logic. The model
consists of a library of plans represented in the form of Horn clauses
indicating which actions imply which intentions. The aim is therefore to
reverse the implications in order to transform the deduction mechanism
into an abduction mechanism. Exclusionary constraints and a system of
weights acquired through learning must then be introduced to determine
the most likely intention. Once again, despite the presence of a system
of result discrimination, there is no consideration of context and
abductive transformation remains too cumbersome a process for real-time
recognition.

\hypertarget{networks}{%
\subsection{Networks}\label{networks}}

\hypertarget{andor-trees-approach}{%
\subsubsection{And/Or trees approach}\label{andor-trees-approach}}

As in its early days, intention recognition can still be modeled in the
form of graphs. Very often in intention recognition, trees are used to
exploit the advantages of acyclicity in resolution and path algorithms.
In the prolific literature of Geib et al.~we find the model at the basis
of PHATT (Geib \protect\hyperlink{ref-geib_problems_2002}{2002}) which
consists of an AND/OR tree representing a HTN that contains the
intentions as well as their plans or methods. A prior relationship is
added to this model and it is through this model that constraints are
placed on the execution of actions. Once an action is observed, all the
successors of the action are unlocked as potential next observed action.
We can therefore infer by hierarchical path the candidate intentions for
the observed sequence.

Since this model does not allow discrimination of results, Geib and
Goldman (\protect\hyperlink{ref-geib_partial_2005}{2005}) then adds
probabilities to the explanations of the observations. The degree of
coverage of each possible goal is used to calculate the probability of
each goal. That is, the goal with the plan containing the most observed
action and the least unobserved action will be the most likely. This is
very ingenious, as the coverage rate is one of the most reliable
indicators. However, the model only takes into account temporality and
therefore has no contextual support. The representation in the form of a
tree also makes it very difficult to be flexible in terms of the plans,
which are then fixed a priori.

\hypertarget{htn-approach}{%
\subsubsection{HTN Approach}\label{htn-approach}}

The HTN model is often used in the field, such as the hierarchical tree
form used by Avrahami-Zilberbrand \emph{et al.}
(\protect\hyperlink{ref-avrahami-zilberbrand_fast_2005}{2005}). The tree
consists of nodes that represent various levels of action and intent. A
hierarchical relationship links these elements together to define each
intention and its methods. To this tree is added an anteriority
relationship that constrains the execution order. This paradigm uses
time markers that guarantee order to use an actualization algorithm that
also updates a hypothesis tree containing possible intentions for each
observation.

\begin{figure}
\centering
\includegraphics{graphics/avrahami.svg}
\caption{The HTN and decision tree used for intent recognition}
\end{figure}

A probabilistic extension of the Avrahami-Zilberbrand and Kaminka
(\protect\hyperlink{ref-avrahami-zilberbrand_hybrid_2006}{2006}) applies
a hierarchical hidden Markov model to the action tree. Using three types
of probability that of plan tracking, execution interleaving and
interruption, we can calculate the probability of execution of each plan
according to the observed sequence. The logic and contextual model
filtered on the possible plans upstream leaving us with few calculations
to order these plans.

This contextual model uses a decision tree based on a system of world
properties. Each property has a finite (and if possible very limited)
number of possible values. This allows you to create a tree containing
for each node a property and an arc for each value. This is combined
with other nodes or leaves that are actions. While running through the
tree during execution, the branches that do not correspond to the
current value of each property are pruned. Once a leaf is reached, it is
stored as a possible action. This considerably reduces the research
space but requires a balanced tree that is not too large or restrictive.

\hypertarget{hidden-markovian-approach}{%
\subsubsection{Hidden Markovian
Approach}\label{hidden-markovian-approach}}

When we approach stochastic models, we very often find Markovian or
Bayesian models. These models use different probabilistic tools ranging
from simple probabilistic inference to the fusion of stochastic
networks. It can be noted that probabilities are often defined by
standard distributions or are isomorphic to weighted systems.

A stochastic model based on THRs is the one presented by Blaylock and
Allen (\protect\hyperlink{ref-blaylock_fast_2006}{2006}). This creates
hierarchical stacks to categorize abstraction levels from basic actions
to high level intentions. By chaining a hidden Markov model to these
stacks, the model is able to affect a probability of intention according
to the observed action.

\hypertarget{bayesian-approach}{%
\subsubsection{Bayesian Approach}\label{bayesian-approach}}

Another stochastic paradigm is the one of Han and Pereira
(\protect\hyperlink{ref-han_contextdependent_2013}{2013}). It uses
Bayesian networks to define relationships between causes, intentions and
actions in a given field. Each category is treated separately in order
to reduce the search space. The observed actions are then selected from
the action network and extracted. The system then uses the intention
network to build a temporary Bayesian network using the NoisyOR method.
The network created is combined in the same way with the network of
causes and makes it possible to have the intention as well as the most
probable cause according to the observations.

\hypertarget{markovian-network-approach}{%
\subsubsection{Markovian network
approach}\label{markovian-network-approach}}

The model of Kelley \emph{et al.}
(\protect\hyperlink{ref-kelley_contextbased_2012}{2012}) (based on
(Hovland \emph{et al.}
\protect\hyperlink{ref-hovland_skill_1996}{1996})) is a model using
hidden Markov networks. This stochastic network is built here by
learning data from robotic perception systems. The goal is to determine
intent using past observations. This model uses the theory of mind by
invoking that humans infer the intentions of their peers by using a
projection of their own.

Another contextual approach is the one developed for robotics by Hofmann
and Williams (\protect\hyperlink{ref-hofmann_intent_2007}{2007}). The
stochastic system is completed by a weighting based on an analysis of
vernacular corpuses. We can therefore use the context of an observation
to determine the most credible actions using the relational system built
with corpus analysis. This is based on the observation of the objects in
the scene and their condition. This makes common sense actions much more
likely and almost impossible actions leading to semantic contradictions.

\hypertarget{bayesian-theory-of-mind}{%
\subsubsection{Bayesian Theory of Mind}\label{bayesian-theory-of-mind}}

This principle is also used as the basis of the paradigm of Baker and
Tenenbaum (\protect\hyperlink{ref-baker_modeling_2014}{2014}) which
forms a Bayesian theory of the mind. Using a limited representation of
the human mind, this model defines formulas for updating beliefs and
probabilities a posteriori of world states and actions. This is
constructed with sigmoid distributions on the simplex of inferred
beliefs. Then the probabilities of desire are calculated in order to
recover the most probable intention. This has been validated as being
close to the assessment of human candidates on simple intention
recognition scenarios.

\hypertarget{inverted-planning}{%
\section{Inverted planning}\label{inverted-planning}}

Another way to do intent recognition do not rely on having a plan
library at all by using inverted planning. In fact, intent recognition
is the opposite problem as planning. In planning we compute the plan
from the goal and in intent recognition we seek the goal from the plan.
This means that planning is a \emph{deduction} problem while intent
recognition is an \emph{abduction} problem. It is therefore possible to
transform an intent recognition problem into a planning one.

More intuitively, this transformation relies on the \emph{theory of
mind}. This notion of psychology states that one of the easiest ways to
predict the mind of another agent is by projecting our own way of
thinking onto the target. The familiar way to understand this is to ask
the question, ``what would \textbf{I} do if I were them ?''. This is
obviously imperfect since we don't have the complete knowledge of the
other mind but is often good enough at basic predictions.

This theory is based on the Belief, Desire and Intention (BDI) model. In
our case the belief part is akin to the knowledge database, the desires
are the set of possible goals (weighted by costs) and the intent is the
plan that achieve a selected goal.

A good analysis of this way of thinking in the context of intent
recognition can be found in \textbf{CITATION} (Baker)'s work on the
subject.

To get further, the work of \textbf{CITATION} (Ramirez) is the founding
paper on the principle of transforming the intent recognition problem
into a planning one. That work was later improved by \textbf{CITATION}
(Chen) in order to support multiple and concurrent goals at once.

In order to do that Ramirez rediscovers an old tool called constraints
encoding into planning \textbf{CITATION}. This allows to force the
selection of operators in a given a certain order or adding arbitrary
constraints on the solution.

\begin{figure}
\centering
\includegraphics{graphics/encoding_constraints.svg}
\caption{Representation of the encoding of constraints using extra
fluents.}
\end{figure}

The encoding on itself is quite straight forward : Adding artificial
fluents to derive an action's behavior considering its selection. In the
case of Ramirez's work, the fluents ensure that the observed actions are
selected at the start. The resulting plan is then compared to another
plan computed while avoiding the observed action. The difference in cost
is proportional to the likelihood of the goal to be pursued.

This problem transformation was more recently improved significantly by
\textbf{CITATION} (Sohrabi). Indeed, their work allows for using
observed fluents instead of actions. This modification allows for a more
accurate and flexible prediction with less advanced observations. It
also takes into account the missing and noisy observation to affect
negatively the likelihood of a goal. Along with the use of diverse
planning, this technique allows for seamless multi-goal recognition.

In order to see this technique used in practice we refer to the works of
\textbf{CITATION} (Tamadupula).

\hypertarget{probabilities-and-approximations}{%
\subsection{Probabilities and
approximations}\label{probabilities-and-approximations}}

Now that the intuition is covered, we need to prove that the result of
the planning process is indeed correlated to the probability of the
agent pursuing that plan knowing the observations. But first we need to
formalize how probabilities work.

An \emph{event} is a fixed fluent, a logical proposition that can occur.
The likelihood of an event happening ranges from \(0\) (impossible) to
\(1\) (certain). This is represented by a relation named
\textbf{probability} of any event \(e\) noted
\(\proba(e) \in [0,1]_{\bb{R}}\). This means that probabilities are real
numbers restricted between \(0\) and \(1\).

\begin{definition}[Conditional probabilities]

Conditional probabilities are probabilities of an event assuming that
another related event happened. This allows to evaluate the ways in
which events are affecting one another. The probability of the event
\(A\) occurring knowing that \(B\) occurred is written:

\[\proba(A|B) = \frac{\proba(A \cap B)}{\proba(B)}\]

with, \(\proba(A \cap B)\) being the probability that both events occur.
In the case of two \emph{independent} events
\(\proba(A|B) = \proba(A)\).

\end{definition}

We note the set of goals that can be pursued \(G\) and the temporal
sequence of observations \(\obs\). Using conditional probabilities, we
seek to have a measure of \(\proba(g|\obs)\) for any goal \(g \in G\).

In that section we explain how inverted planning does that computation.

For any set of observations \(\obs\) the probability of the set is the
product of the probability of any observation \(o \in \obs\). We can
then note \(\proba(\obs)=\prod_{o\in \obs} \proba(o)\).

We assume that the observed agent is pursuing one of the known goals.
The event of an agent pursuing a specific goal is noted \(g\). This
means that \(\proba(G) = \sum_{g\in G}\proba(G) = 1\) because the event
is considered certain.

Using \emph{conditional probabilities} we can also note
\(\proba(G|\plan) = 1\) for a valid plan \(\plan\) that achieves any
goals \(g \in G\).

\begin{theorem}[Bayes]

Bayes's theorem allows to find the probability of an event based on
prior knowledge of other factors related to said event. It is another
basic way to compute conditional probabilities as follows:

\[\proba(A|B) = \frac{\proba(B|A)\proba(A)}{\proba(B)}\]

\end{theorem}

In the Bayesian logic, one should start with a \emph{prior} probability
of an event and actualize it with any new information to make it more
precise. In our case, we suppose that \(\proba(G)\) is given or computed
by an external tool.

From the direct application of Bayes's theorem and the previous
assumptions, we have :

\[\proba(\plan|\obs) = \frac{\proba(\obs|\plan) \proba(\plan)}{\proba(\obs)} = \frac{\proba(\obs|\plan) \proba(\plan|G) \proba(G)}{\proba(\obs)}\]
\{\#eq:plan-obs\}

Using the event \(\plan\) transitively we can simplify to:

\[ \proba(G|\obs) = \frac{\proba(\obs|G)\proba(G)}{\proba(\obs)}\]
\{\#eq:goal-obs\}

\begin{theorem}[Total Probability]

If we have a countable set of disjoint events \(E\) we can compute the
probability of all events happening as the sum of the probabilities of
each event:

\[\proba(E) = \sum_{e \in E} \proba(e)\]

\end{theorem}

Since we consider that we have all likely plans for a given goal we can
neglect the very improbable ones and assert that the events of any given
plans being acted are independent from one another. Also, the total
probability of all plans is certain. From the total probability formula:

\[\proba(\obs|G) = \sum_{\plan \in \plans_G} \proba(\obs|\plan) \proba(\plan|G)\]
\{\#eq:obs-goal\}

In equation~\ref{eq:goal-obs}, we have \(\proba(G)\) and
\(\proba(\obs)\) known from prior knowledge. Along with
equation~\ref{eq:obs-goal}, we can say that:

\[\proba(G|\obs)= \alpha \proba(\obs|G) \proba(G)\]

With \(\alpha\) being a normalizing constant. Also, using the previous
formula we can assert that:

\[\proba(g|\obs) \propto \sum_{\plan \in \plans_g} \proba(\plan|\obs)\]

This means that if the cost of the plan is related to its likeliness of
being pursued knowing the observation sequence, we can evaluate the
probability of any goal being pursued. This allows for Sohrabi's problem
transformation to work.

That transformation is simply affecting the cost of a plan by dissuading
any missed or added fluents while rewarding correct predicted fluents
relative to the observations. This process is illustrated in
figure~\ref{fig:proba}.

\begin{figure}
\hypertarget{fig:proba}{%
\centering
\includegraphics{graphics/proba.svg}
\caption{Illustration of how plan costs are affecting the resulting
probabilities.}\label{fig:proba}
}
\end{figure}

\hypertarget{intent-recognition-using-abstract-plans}{%
\section{Intent recognition Using Abstract
Plans}\label{intent-recognition-using-abstract-plans}}

The initial objective of this thesis was to make intent predictions use
abstract plans and repairs. Since plan repair is very susceptible to the
heuristic, it is not a reliable tool for general and uncertain planning
and will perform worse on all cases not handled well by a given
heuristic.

This is not the case of abstract HTN planning since the plan generated
is valid while incomplete. The idea here is to quantify the quality of
an abstract plan to weigh its plausibility and the likelihood of missing
fluents using Sohrabi's method.

The unforeseen problem is that, while in theory this seems like a very
efficient approach, in practice it can lack a lot of performance since
turning a sequence of observed fluents into a backward chaining
heuristic is tricky at best.

In this section, some idea of how that could be done is explored along
with perspectives for further works regarding the subject.

\hypertarget{linearization}{%
\subsection{Linearization}\label{linearization}}

In order to use the approach of Sohrabi, we need total ordered plans.
This is quite an issue since our planner generates partial order plans.
Each of these plans have one or several \emph{linearizations}: totally
ordered plans that correspond to all possible orders of the plan.

The idea here is to merge parallel actions into one using graph
quotient. This is the same mechanism behind sheaves. To do this we use
the fact that there are no threats in valid plans and therefore parallel
actions have compatible preconditions and effects. This allows to merge
several actions into one that is equivalent in terms of fluents and
cost.

\begin{figure}
\centering
\includegraphics{graphics/clothes.svg}
\caption{Example of linearization of partial order plan.}
\end{figure}

\hypertarget{ch:conclusion}{%
\chapter{Conclusion and Perspectives}\label{ch:conclusion}}

\hypertarget{fondation}{%
\section{Fondation}\label{fondation}}

\hypertarget{issues-with-current-fondations}{%
\subsection{Issues with current
fondations}\label{issues-with-current-fondations}}

Like stated, these three aspect of knowledge representation makes
defining a fondation of mathematics way more difficult than it may
appear. In this section, we analyse some of the existing fondation of
mathematics and their dependancies.

\hypertarget{set-theory}{%
\subsubsection{Set theory}\label{set-theory}}

From the book releasing the complete formulation of the ZFC theory, we
can find that:

\begin{quote}
\emph{``the discipline underlying ZF will be the \textbf{first-order
predicate calculus}. The primitive symbols of this set theory, taken
from logic, are the connectives, quantifiers and variables mentioned
above and, possibly, also the symbol of equality (discussed below), as
well as such auxiliary symbols as commas, parentheses and
brackets.''}\footnote{Fraenkel \emph{et al.}
  (\protect\hyperlink{ref-fraenkel_foundations_1973}{1973}, 67,22)}
\end{quote}

This means that ZFC is using FOL as a host language. In turn FOL is
using formal grammar as its host language. And at last, formal grammar
are defined using set theory. This means that there is a dependancy
cycle in the definition of ZFC.

This is significative as this theory is the ground work for most of the
classical mathematics still used to this day. Morever, several
assumptions are made in the theory that are not explicitely stated. For
example, when naming an element or a set we supose the principle of
identity as we can distinguish them from one another. Also, the notion
of set isn't properly defined as well as the classical FOL quantifiers
that are used in even the very first formula of the theory.

\hypertarget{type-theory}{%
\subsubsection{Type theory}\label{type-theory}}

Russell and Whitehead
(\protect\hyperlink{ref-russell_principia_1978}{1978})

\textbf{TODO?} Read the book since typed Lambda calculus is posterior to
type theory

Type theory -\textgreater{} Typed Lambda calculus (no definition of
type) -\textgreater{} Lambda claculus -\textgreater{} set theory or
formal grammar

\hypertarget{category-theory-1}{%
\subsubsection{Category theory}\label{category-theory-1}}

Awodey (\protect\hyperlink{ref-awodey_category_2010}{2010})

\textbf{TODO?} Is that what I found ?

Category theory -\textgreater{} Typed Lamda calculus

\hypertarget{proof-theory}{%
\subsubsection{Proof theory}\label{proof-theory}}

\textbf{TODO?}

Proof Theory -\textgreater{} Hilbert calculus -\textgreater{} Mathematic
logic -\textgreater{} (FOL \textbar{} Set \textbar{} etc)

\hypertarget{reverse-mathematics}{%
\subsection{Reverse mathematics}\label{reverse-mathematics}}

Another radical way to approach the fondations of mathematics is called
reverse mathematics. In this field, the goal is to find the minimal set
of axioms needed to prove any theorem. This is not directly meant to be
a fondation of mathematics but helps approaching it by finding groups of
axioms that prove all theorems of a given field of mathematics.

This is the approach that should be used to find the most adequate set
of axioms while keeping in mind the constraints any fondation of
mathematics has.

\hypertarget{possible-paradox-in-proposed-theory}{%
\subsection{Possible paradox in proposed
theory}\label{possible-paradox-in-proposed-theory}}

\textbf{TODO}: Speak about Russel paradox in fonctional form: A fonction
that associates any function that doesn't associate itself. This arrise
from the complement operation.

\hypertarget{knowledge-representation}{%
\section{Knowledge representation}\label{knowledge-representation}}

Listing the contributions there are a couple that didn't make the cut.
It is mainly ideas or projects that were too long to check or implement
and needed more time to complete. SELF is still a prototype, and even if
the implementation seemed to perform well on a simple example, no
benchmarks have been done on it. It might be good to make a theoretical
analysis of OWL compared to SELF along with some benchmark results.

On the theoretical parts there are some works that seems worthy of
exposure even if unfinished.

\hypertarget{sec:peano}{%
\subsection{Literal definition using Peano's axioms}\label{sec:peano}}

The only real exceptions to the axioms and criteria are the first
statement, the comments and the liberals.

For the first statement, there is yet to find a way to express both
inclusion, the equality relation and solution quantifier. If such a
convenient expression exists, then the language can become almost
entirely self described.

Comments can be seen as a special kind of container. The difficult part
is to find a clever way to differentiate them from regular containers
and to ignore their content in the regular grammar. It might be possible
to at first describe their structure but then they become parseable
entities and fail at their purpose.

Lastly, and perhaps the most complicated violation to fix: laterals. It
is quite possible to define literals by structure. First we can define
boolean logic quite easily in SELF as demonstrated by
listing~\ref{lst:bool}.

\begin{lstlisting}[language=Java, caption={Possible definition of boolean logic in SELF.}, escapechar={$}, label=lst:bool]
~(false) = true;$\label{line:negation}$
( false, true ) :: Boolean;$\label{line:boolean}$
true =?; //conflicts with the first statement!$\label{line:true}$
*a : ((a | true) = true);$\label{line:logic}$
*a : ((false | a) = a);
*a : ((a & false) = false); 
*a : ((true & a) = a);
\end{lstlisting}

Starting with line~\ref{line:negation}, we simply define the negation
using the exclusive quantifier. From there we define the boolean type as
just the two truth values. And now it gets complicated. We could either
arbitrarily say that the false literal is always parameters of the
exclusion quantifier or that it comes first on either first two
statements but that would just violate minimalism even more. We could
use the solution quantifier to define truth but that collides with the
first statement definition. There doesn't seem to be a good answer for
now.

From line~\ref{line:logic} going on, we state the definition of the
logical operators \(\land\) and \(\lor\). The problem with this is that
we either need to make a native property for those operators or the
inference to compute boolean logic will be terribly inefficient.

We can use Peano's axioms
(\protect\hyperlink{ref-peano_arithmetices_1889}{1889}) to define
integers in SELF. The attempt at this definition is presented in
listing~\ref{lst:peano}.

\begin{lstlisting}[language=Java, caption={Possible integration of the Peano axioms in SELF.}, label=lst:peano]
0 :: Integer;
*n : (++(n) :: Integer);
(*m, *n) : ((m=n) : (++m = ++n));
*n : (++n ~= 0);
*n : ((n + 0) = n);
(*n, *m) : ((n + ++m)= ++(n + m));
*n : ((n × 0) = 0);
(*n, *m) : ((n × ++m) = (n + (n × m)));
\end{lstlisting}

We got several problems doing so. The symbols
\passthrough{\lstinline!*!} and \passthrough{\lstinline!/!} are already
taken in the default file and so would need replacement or we should use
the non-ASCII \passthrough{\lstinline!×!} and
\passthrough{\lstinline!÷!} symbols for multiplication and division.
Another more fundamental issue is as previously discussed for booleans:
the inference would be excruciatingly slow or we should revert to a kind
of parsing similar to what we have already under the hood. The last
problem is the definition of digits and bases that would quickly become
exceedingly complicated and verbose.

For floating numbers this turns out even worse and complicated and such
a description wasn't even attempted for now.

The last part concerns textual laterals. The issue is the same as the
one with comments but even worse. We get to interpret the content as
literal value and that would necessitate a similar system as we already
have and wouldn't improve the minimalist aspect of things much. Also we
should define ways to escape characters and also to input escape
sequences that are often needed in such case. And since SELF isn't meant
for programming that can become very verbose and complex.

\hypertarget{advanced-inference}{%
\subsection{Advanced Inference}\label{advanced-inference}}

The inference in SELF is very basic. It could be improved a lot more by
simply checking the consistency of the database on most aspects.
However, such a task seems to be very difficult or very slow. Since that
kind of inference is undecidable in SELF, it would be all a research
problem just to find a performant inference algorithm.

Another kind of inference is more about convenience. For example, one
can erase singlets (containers with a single value) to make the database
lighter and easier to maintain and query.

\hypertarget{queries}{%
\subsection{Queries}\label{queries}}

We haven't really discussed quarries in SELF. They can be made using the
main syntax and the solution quantifiers but efficiency of such queries
is unknown. Making an efficient query engine is a big research project
on its own.

For now a very simplified query API exists in the prototype and seems to
perform well but further tests are needed to assess its scalability
capacities.

\hypertarget{general-automated-planning}{%
\section{General Automated Planning}\label{general-automated-planning}}

\hypertarget{planning-improvements}{%
\section{Planning Improvements}\label{planning-improvements}}

\hypertarget{heuristics-using-semantics}{%
\subsection{Heuristics using
Semantics}\label{heuristics-using-semantics}}

\hypertarget{macro-action-learning}{%
\subsection{Macro-Action learning}\label{macro-action-learning}}

\hypertarget{references}{%
\chapter{References}\label{references}}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-alami_multirobot_1995}{}%
Alami, R., F. Robert, F. Ingrand, and S. Suzuki\\
Multi-robot cooperation through incremental plan-merging, \emph{Robotics
and Automation, 1995. Proceedings., 1995 IEEE International Conference
on}, IEEE, 1995, 3,2573--2579.

\leavevmode\hypertarget{ref-alessandro_ometa_2007}{}%
Alessandro, W., and I. Piumarta\\
OMeta: An object-oriented language for pattern matching,
\emph{Proceedings of the 2007 symposium on Dynamic languages}, 2007.

\leavevmode\hypertarget{ref-aljazzar_heuristic_2011}{}%
Aljazzar, H., and S. Leue\\
K*: A heuristic search algorithm for finding the k shortest paths,
\emph{Artificial Intelligence}, 175 (18), 2129--2154, 2011.

\leavevmode\hypertarget{ref-ambite_planning_1997}{}%
Ambite, J. L., and C. A. Knoblock\\
\emph{Planning by Rewriting: Efficiently Generating High-Quality
Plans.}, DTIC Document, 1997.

\leavevmode\hypertarget{ref-americanheritagedictionary_formal_2011}{}%
American Heritage Dictionary\\
Formal (adj.), \emph{American Heritage Dictionary of the English
Language}, 2011a.

\leavevmode\hypertarget{ref-americanheritagedictionary_circularity_2011}{}%
American Heritage Dictionary\\
Circularity (n.d.), \emph{American Heritage Dictionary of the English
Language}, 2011b.

\leavevmode\hypertarget{ref-asimov_gods_1973}{}%
Asimov, I.\\
\emph{The gods themselves}, Greenwich, Connecticut: Fawcett Crest, 1973.

\leavevmode\hypertarget{ref-avrahami-zilberbrand_hybrid_2006}{}%
Avrahami-Zilberbrand, D., and G. A. Kaminka\\
Hybrid symbolic-probabilistic plan recognizer: Initial steps,
\emph{Proceedings of AAAI workshop on modeling others from observations
(MOO-06)}, 2006.

\leavevmode\hypertarget{ref-avrahami-zilberbrand_fast_2005}{}%
Avrahami-Zilberbrand, D., G. A. Kaminka, and H. Zarosim\\
Fast and complete plan recognition: Allowing for duration, interleaved
execution, and lossy observations, \emph{IJCAI Workshop on Modeling
Others from Observations}, 2005.

\leavevmode\hypertarget{ref-awodey_category_2010}{}%
Awodey, S.\\
\emph{Category theory}, 2nd ed. Oxford logic guides 52Oxford ; New York:
Oxford University Press, 2010.

\leavevmode\hypertarget{ref-baader_description_2003}{}%
Baader, F., D. Calvanese, D. McGuinness, P. Patel-Schneider, and D.
Nardi\\
\emph{The description logic handbook: Theory, implementation and
applications}, Cambridge university press, 2003.

\leavevmode\hypertarget{ref-babli_use_2015}{}%
Babli, M., E. Marzal, and E. Onaindia\\
On the use of ontologies to extend knowledge in online planning,
\emph{KEPS 2018}, 54, 2015.

\leavevmode\hypertarget{ref-backus_syntax_1959}{}%
Backus, J. W.\\
The syntax and semantics of the proposed international algebraic
language of the Zurich ACM-GAMM conference, \emph{Proceedings of the
International Comference on Information Processing, 1959}, 1959.

\leavevmode\hypertarget{ref-baker_bayesian_2011}{}%
Baker, C. L., R. R. Saxe, and J. B. Tenenbaum\\
Bayesian theory of mind: Modeling joint belief-desire attribution,
\emph{Proceedings of the thirty-second annual conference of the
cognitive science society}, 2011, 2469--2474.

\leavevmode\hypertarget{ref-baker_modeling_2014}{}%
Baker, C. L., and J. B. Tenenbaum\\
Modeling Human Plan Recognition using Bayesian Theory of Mind, 2014.

\leavevmode\hypertarget{ref-barendregt_lambda_1984}{}%
Barendregt, H. P.\\
The Lambda Calculus: Its Syntax and Semantics. 1984, \emph{Studies in
Logic and the Foundations of Mathematics}, 1984.

\leavevmode\hypertarget{ref-barr_category_1990}{}%
Barr, M., and C. Wells\\
\emph{Category theory for computing science}, vol. 49Prentice Hall New
York, 1990.

\leavevmode\hypertarget{ref-bechon_hipop_2014}{}%
Bechon, P., M. Barbier, G. Infantes, C. Lesire, and V. Vidal\\
HiPOP: Hierarchical Partial-Order Planning, \emph{European Starting AI
Researcher Symposium}, IOS Press, 2014, 264,51--60.

\leavevmode\hypertarget{ref-becket_dcgs_2008}{}%
Becket, R., and Z. Somogyi\\
DCGs+ memoing= packrat parsing but is it worth it?, \emph{International
Symposium on Practical Aspects of Declarative Languages}, Springer,
2008, 182--196.

\leavevmode\hypertarget{ref-beckett_turtle_2011}{}%
Beckett, D., and T. Berners-Lee\\
\emph{Turtle - Terse RDF Triple Language}, W3C Team Submission W3C,
March 2011.

\leavevmode\hypertarget{ref-bercher_tutorial_2018}{}%
Bercher, P., and D. Höller\\
Tutorial: An Introduction to Hierarchical Task Network (HTN) Planning
2018.

\leavevmode\hypertarget{ref-bercher_hybrid_2014}{}%
Bercher, P., S. Keen, and S. Biundo\\
Hybrid planning heuristics based on task decomposition graphs,
\emph{Seventh Annual Symposium on Combinatorial Search}, 2014.

\leavevmode\hypertarget{ref-blaylock_fast_2006}{}%
Blaylock, N., and J. Allen\\
Fast hierarchical goal schema recognition, \emph{Proceedings of the
National Conference on Artificial Intelligence}, Menlo Park, CA;
Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006, 21,796.

\leavevmode\hypertarget{ref-borrajo_multiagent_2013}{}%
Borrajo, D.\\
Multi-agent planning by plan reuse, \emph{Proceedings of the 2013
international conference on Autonomous agents and multi-agent systems},
International Foundation for Autonomous Agents and Multiagent Systems,
2013, 1141--1142.

\leavevmode\hypertarget{ref-borrajo_progress_2015}{}%
Borrajo, D., A. Roubíčková, and I. Serina\\
Progress in Case-Based Planning, \emph{ACM Computing Surveys}, 47 (2),
1--39, January 2015.
doi:\href{https://doi.org/10.1145/2674024}{10.1145/2674024}.

\leavevmode\hypertarget{ref-bouchard_smart_2006}{}%
Bouchard, B., S. Giroux, and A. Bouzouane\\
A Smart Home Agent for Plan Recognition of Cognitively-impaired
Patients., \emph{Journal of Computers}, 1 (5), 53--62, 2006.

\leavevmode\hypertarget{ref-brenner_multiagent_2003}{}%
Brenner, M.\\
A multiagent planning language, \emph{Proc. Of the Workshop on PDDL,
ICAPS}, 2003, 3,33--38.

\leavevmode\hypertarget{ref-burckert_terminologies_1994}{}%
Bürckert, H.-J.\\
Terminologies and rules, \emph{Workshop on Information Systems and
Artificial Intelligence}, Springer, 1994, 44--63.

\leavevmode\hypertarget{ref-cantor_property_1874}{}%
Cantor, G.\\
On a Property of the Class of all Real Algebraic Numbers.,
\emph{Crelle's Journal for Mathematics}, 77, 258--262, 1874.

\leavevmode\hypertarget{ref-cantor_beitrage_1895}{}%
Cantor, G.\\
Beiträge zur Begründung der transfiniten Mengenlehre,
\emph{Mathematische Annalen}, 46 (4), 481--512, 1895.

\leavevmode\hypertarget{ref-chomsky_three_1956}{}%
Chomsky, N.\\
Three models for the description of language, \emph{IRE Transactions on
information theory}, 2 (3), 113--124, 1956.

\leavevmode\hypertarget{ref-ciesielski_set_1997}{}%
Ciesielski, K.\\
\emph{Set Theory for the Working Mathematician}, Cambridge University
Press, August 1997.

\leavevmode\hypertarget{ref-coles_popf2_2011}{}%
Coles, A., A. Coles, M. Fox, and D. Long\\
Popf2 : A forward-chaining partial order planner, \emph{IPC}, 65, 2011.

\leavevmode\hypertarget{ref-collinsenglishdictionary_abstraction_2014}{}%
Collins English Dictionary\\
Abstraction (n.d.), \emph{Collins English Dictionary Complete and
Unabridged}, 2014.

\leavevmode\hypertarget{ref-curry_studies_1958}{}%
Curry, H. B., R. Feys, and W. Craig\\
Studies in Logic and the Foundations of Mathematics, \emph{Combinatory
logic}, Vol. 1North-Holland Amsterdam, 1958.

\leavevmode\hypertarget{ref-dalfonso_generalized_2011}{}%
D'Alfonso, D.\\
Generalized Quantifiers: Logic and Language, 2011.

\leavevmode\hypertarget{ref-dornhege_semantic_2012}{}%
Dornhege, C., P. Eyerich, T. Keller, S. Trüg, M. Brenner, and B. Nebel\\
Semantic attachments for domain-independent planning systems,
\emph{Towards service robots for everyday environments}, Springer, 2012,
99--115.

\leavevmode\hypertarget{ref-dvorak_flexible_2014}{}%
Dvorak, F., A. Bit-Monnot, F. Ingrand, and M. Ghallab\\
A flexible ANML actor and planner in robotics, \emph{Planning and
Robotics (PlanRob) Workshop (ICAPS)}, 2014.

\leavevmode\hypertarget{ref-ephrati_multiagent_1993}{}%
Ephrati, E., and J. S. Rosenschein\\
Multi-agent planning as the process of merging distributed sub-plans,
\emph{In Proceedings of the Twelfth International Workshop on
Distributed Artificial Intelligence (DAI-93}, Citeseer, 1993.

\leavevmode\hypertarget{ref-erol_umcp_1994}{}%
Erol, K., J. A. Hendler, and D. S. Nau\\
UMCP: A Sound and Complete Procedure for Hierarchical Task-network
Planning, \emph{Proceedings of the International Conference on
Artificial Intelligence Planning Systems}, University of Chicago,
Chicago, Illinois, USA: AAAI Press, June 1994, 2,249--254.

\leavevmode\hypertarget{ref-erol_complexity_1995}{}%
Erol, K., D. S. Nau, and V. S. Subrahmanian\\
Complexity, decidability and undecidability results for
domain-independent planning, \emph{Artificial intelligence}, 76 (1-2),
75--88, 1995.

\leavevmode\hypertarget{ref-fikes_strips_1971}{}%
Fikes, R. E., and N. J. Nilsson\\
STRIPS: A new approach to the application of theorem proving to problem
solving, \emph{Artificial intelligence}, 2 (3-4), 189--208, 1971.

\leavevmode\hypertarget{ref-ford_packrat_2002}{}%
Ford, B.\\
Packrat parsing:: Simple, powerful, lazy, linear time, functional pearl,
\emph{ACM SIGPLAN Notices}, ACM, 2002, 37,36--47.

\leavevmode\hypertarget{ref-ford_parsing_2004}{}%
Ford, B.\\
Parsing expression grammars: A recognition-based syntactic foundation,
\emph{ACM SIGPLAN Notices}, ACM, 2004, 39,111--122.

\leavevmode\hypertarget{ref-fox_natural_1997}{}%
Fox, M.\\
Natural hierarchical planning using operator decomposition,
\emph{European Conference on Planning}, Springer, 1997, 195--207.

\leavevmode\hypertarget{ref-fox_plan_2006}{}%
Fox, M., A. Gerevini, D. Long, and I. Serina\\
Plan Stability: Replanning versus Plan Repair., \emph{ICAPS}, 2006,
6,212--221.

\leavevmode\hypertarget{ref-fox_pddl_2002}{}%
Fox, M., and D. Long\\
PDDL+: Modeling continuous time dependent effects, \emph{Proceedings of
the 3rd International NASA Workshop on Planning and Scheduling for
Space}, 2002, 4,34.

\leavevmode\hypertarget{ref-fraenkel_foundations_1973}{}%
Fraenkel, A. A., Y. Bar-Hillel, and A. Levy\\
\emph{Foundations of set theory}, vol. 67Elsevier, 1973.

\leavevmode\hypertarget{ref-geib_problems_2002}{}%
Geib, C. W.\\
Problems with intent recognition for elder care, \emph{Proceedings of
the AAAI-02 Workshop ``Automation as Caregiver}, 2002, 13--17.

\leavevmode\hypertarget{ref-geib_partial_2005}{}%
Geib, C. W., and R. P. Goldman\\
Partial observability and probabilistic plan/goal recognition,
\emph{Proceedings of the International workshop on modeling other agents
from observations (MOO-05)}, 2005.

\leavevmode\hypertarget{ref-gerevini_planlibrary_2013}{}%
Gerevini, A. E., A. Roubíčková, A. Saetti, and I. Serina\\
On the plan-library maintenance problem in a case-based planner,
\emph{International Conference on Case-Based Reasoning}, Springer, 2013,
119--133.

\leavevmode\hypertarget{ref-gerevini_combining_2008}{}%
Gerevini, A., U. Kuter, D. S. Nau, A. Saetti, and N. Waisbrot\\
Combining Domain-Independent Planning and HTN Planning: The Duet
Planner, \emph{Proceedings of the European Conference on Artificial
Intelligence}, 2008, 18,573--577.

\leavevmode\hypertarget{ref-ghallab_pddl_1998}{}%
Ghallab, M., C. Knoblock, D. Wilkins, A. Barrett, D. Christianson, et
al.\\
PDDL -- The planning domain definition language, 1998.

\leavevmode\hypertarget{ref-ghallab_automated_2004}{}%
Ghallab, M., D. Nau, and P. Traverso\\
\emph{Automated planning: Theory \& practice}, Elsevier, 2004.

\leavevmode\hypertarget{ref-ghallab_automated_2016}{}%
Ghallab, M., D. Nau, and P. Traverso\\
\emph{Automated Planning and Acting}, Cambridge University Press, 2016.

\leavevmode\hypertarget{ref-godel_consistency_1940}{}%
Godel, K., and G. W. Brown\\
\emph{The consistency of the axiom of choice and of the generalized
continuum-hypothesis with the axioms of set theory}, Princeton
University Press Princeton, NJ, 1940.

\leavevmode\hypertarget{ref-goldman_new_1999}{}%
Goldman, R. P., C. W. Geib, and C. A. Miller\\
A new model of plan recognition, \emph{Proceedings of the Fifteenth
conference on Uncertainty in artificial intelligence}, Morgan Kaufmann
Publishers Inc., 1999, 245--254.

\leavevmode\hypertarget{ref-gobelbecker_coming_2010}{}%
Göbelbecker, M., T. Keller, P. Eyerich, M. Brenner, and B. Nebel\\
Coming Up With Good Excuses: What to do When no Plan Can be Found,
\emph{Proceedings of the International Conference on Automated Planning
and Scheduling}, AAAI Press, May 2010, 20,81--88.

\leavevmode\hypertarget{ref-gradel_twovariable_1997}{}%
Gradel, E., M. Otto, and E. Rosen\\
Two-variable logic with counting is decidable, \emph{Logic in Computer
Science, 1997. LICS'97. Proceedings., 12th Annual IEEE Symposium on},
IEEE, 1997, 306--317.

\leavevmode\hypertarget{ref-grunwald_minimum_1996}{}%
Grünwald, P.\\
A minimum description length approach to grammar inference, in S.
Wermter, E. Riloff, and G. Scheler (eds.), \emph{Connectionist,
Statistical and Symbolic Approaches to Learning for Natural Language
Processing}, Lecture Notes in Computer Science; Springer Berlin
Heidelberg, 1996, 203--216.

\leavevmode\hypertarget{ref-han_contextdependent_2013}{}%
Han, T. A., and L. M. Pereira\\
Context-dependent incremental decision making scrutinizing the
intentions of others via Bayesian network model construction,
\emph{Intelligent Decision Technologies}, 7 (4), 293--317, 2013.

\leavevmode\hypertarget{ref-hart_opencog_2008}{}%
Hart, D., and B. Goertzel\\
Opencog: A software framework for integrative artificial general
intelligence, \emph{AGI}, 2008, 468--472.

\leavevmode\hypertarget{ref-hehner_practical_2012}{}%
Hehner, E. C.\\
\emph{A practical theory of programming}, Springer Science \& Business
Media, 2012.

\leavevmode\hypertarget{ref-helmert_fast_2011}{}%
Helmert, M., G. Röger, and E. Karpas\\
Fast downward stone soup: A baseline for building planner portfolios,
\emph{ICAPS 2011 Workshop on Planning and Learning}, Citeseer, 2011,
28--35.

\leavevmode\hypertarget{ref-henglein_peg_2017}{}%
Henglein, F., and U. T. Rasmussen\\
PEG parsing in less space using progressive tabling and dynamic
analysis, \emph{Proceedings of the 2017 ACM SIGPLAN Workshop on Partial
Evaluation and Program Manipulation}, ACM, 2017, 35--46.

\leavevmode\hypertarget{ref-hernandez_reifying_2015}{}%
Hernández, D., A. Hogan, and M. Krötzsch\\
Reifying RDF: What works well with wikidata?, \emph{SSWS@ ISWC}, 1457,
32--47, 2015.

\leavevmode\hypertarget{ref-hirankitti_metareasoning_2011}{}%
Hirankitti, V., and T. Xuan\\
A meta-reasoning approach for reasoning with SWRL ontologies,
\emph{Internation Multiconference of Engineers}, 2011.

\leavevmode\hypertarget{ref-hoffmann_ff_2001}{}%
Hoffmann, J.\\
FF: The fast-forward planning system, \emph{AI magazine}, 22 (3), 57,
2001.

\leavevmode\hypertarget{ref-hofmann_intent_2007}{}%
Hofmann, A. G., and B. C. Williams\\
Intent Recognition for Human-Robot Interaction., \emph{Interaction
Challenges for Intelligent Assistants}, 2007, 60--61.

\leavevmode\hypertarget{ref-horrocks_shiq_2003}{}%
Horrocks, I., P. F. Patel-Schneider, and F. V. Harmelen\\
From SHIQ and RDF to OWL: The Making of a Web Ontology Language,
\emph{Journal of Web Semantics}, 1, 2003, 2003.

\leavevmode\hypertarget{ref-hovland_skill_1996}{}%
Hovland, G. E., P. Sikka, and B. J. McCarragher\\
Skill acquisition from human demonstration using a hidden markov model,
\emph{Robotics and Automation, 1996. Proceedings., 1996 IEEE
International Conference on}, Ieee, 1996, 3,2706--2711.

\leavevmode\hypertarget{ref-hoschele_mining_2017}{}%
Höschele, M., and A. Zeller\\
Mining input grammars with AUTOGRAM, \emph{Proceedings of the 39th
International Conference on Software Engineering Companion}, IEEE Press,
2017, 31--34.

\leavevmode\hypertarget{ref-hutton_monadic_1996}{}%
Hutton, G., and E. Meijer\\
Monadic parser combinators, 1996.

\leavevmode\hypertarget{ref-inoue_ilpbased_2011}{}%
Inoue, N., and K. Inui\\
ILP-Based Reasoning for Weighted Abduction., \emph{Plan, Activity, and
Intent Recognition}, 2011.

\leavevmode\hypertarget{ref-jonsson_maximal_1984}{}%
Jónsson, B.\\
Maximal algebras of binary relations, \emph{Contemporary Mathematics},
33, 299--307, 1984.

\leavevmode\hypertarget{ref-kambhampati_design_1994}{}%
Kambhampati, S.\\
Design Tradeoffs in Partial Order (Plan space) Planning., \emph{AIPS},
1994, 92--97.

\leavevmode\hypertarget{ref-kambhampati_hybrid_1998}{}%
Kambhampati, S., A. Mali, and B. Srivastava\\
Hybrid planning for partially hierarchical domains, \emph{AAAI/IAAI},
1998, 882--888.

\leavevmode\hypertarget{ref-kelley_contextbased_2012}{}%
Kelley, R., A. Tavakkoli, C. King, A. Ambardekar, and M. Nicolescu\\
Context-based bayesian intent recognition, \emph{Autonomous Mental
Development, IEEE Transactions on}, 4 (3), 215--225, 2012.

\leavevmode\hypertarget{ref-klein_metacompiling_1975}{}%
Klein, S.\\
Meta-compiling Text Grammars As a Model for Human Behavior,
\emph{Proceedings of the 1975 Workshop on Theoretical Issues in Natural
Language Processing}, TINLAP '75; Stroudsburg, PA, USA: Association for
Computational Linguistics, 1975, 84--88.
doi:\href{https://doi.org/10.3115/980190.980217}{10.3115/980190.980217}.

\leavevmode\hypertarget{ref-klyne_resource_2004}{}%
Klyne, G., and J. J. Carroll\\
\emph{Resource Description Framework (RDF): Concepts and Abstract
Syntax}, Language Specification W3C RecommendationW3C, 2004.

\leavevmode\hypertarget{ref-korzybski_science_1933}{}%
Korzybski, A.\\
\emph{Science and Sanity: An Introduction to Non-Aristotelian Systems
and General Semantics}, International Non-Aristotelian Library
Publishing Company, 1933.

\leavevmode\hypertarget{ref-korzybski_science_1958}{}%
Korzybski, A.\\
\emph{Science and sanity; an introduction to non-Aristotelian systems
and general semantics.}, Lakeville, Conn.: International
Non-Aristotelian Library Pub. Co.; distributed by Institute of General
Semantics, 1958.

\leavevmode\hypertarget{ref-kovacs_bnf_2011}{}%
Kovacs, D. L.\\
\emph{BNF Description of PDDL 3.1}, Unpublished manuscript from the
IPC-2011 websiteIPC, 2011.

\leavevmode\hypertarget{ref-kovacs_multiagent_2012}{}%
Kovács, D. L.\\
A multi-agent extension of PDDL3. 1, 2012.

\leavevmode\hypertarget{ref-kovacs_converting_2013}{}%
Kovács, D. L., and T. P. Dobrowiecki\\
Converting MA-PDDL to extensive-form games, \emph{Acta Polytechnica
Hungarica}, 10 (8), 27--47, 2013.

\leavevmode\hypertarget{ref-kovitz_terminology_2018}{}%
Kovitz, B.\\
Terminology - What do you call graphs that allow edges to edges?,
\emph{Mathematics Stack Exchange}, January 2018.

\leavevmode\hypertarget{ref-krotzsch_description_2013}{}%
Krötzsch, M., F. Simancik, and I. Horrocks\\
A Description Logic Primer, June 2013.

\leavevmode\hypertarget{ref-kunen_set_1980}{}%
Kunen, K.\\
\emph{Set theory an introduction to independence proofs}, vol.
102Elsevier, 1980.

\leavevmode\hypertarget{ref-lindstrom_first_1966}{}%
Lindström, P.\\
First Order Predicate Logic with Generalized Quantifiers, 1966.

\leavevmode\hypertarget{ref-loff_computational_2018}{}%
Loff, B., N. Moreira, and R. Reis\\
The Computational Power of Parsing Expression Grammars,
\emph{International Conference on Developments in Language Theory},
Springer, 2018, 491--502.

\leavevmode\hypertarget{ref-luis_plan_2014}{}%
Luis, N., and D. Borrajo\\
Plan merging by reuse for multi-agent planning, \emph{Distributed and
Multi-Agent Planning}, 38, 2014.

\leavevmode\hypertarget{ref-mcdermott_opt_2005}{}%
McDermott, D.\\
\emph{OPT Manual Version 1.7. 3 (Reflects Opt Version 1.6. 11)* DRAFT},
2005.

\leavevmode\hypertarget{ref-mcdermott_representing_2002}{}%
McDermott, D., and D. Dou\\
Representing disjunction and quantifiers in RDF, \emph{International
Semantic Web Conference}, Springer, 2002, 250--263.

\leavevmode\hypertarget{ref-motik_properties_2007}{}%
Motik, B.\\
On the properties of metamodeling in OWL, \emph{Journal of Logic and
Computation}, 17 (4), 617--637, 2007.

\leavevmode\hypertarget{ref-nau_shop2_2003}{}%
Nau, D. S., T.-C. Au, O. Ilghami, U. Kuter, J. W. Murdock, et al.\\
SHOP2: An HTN planning system, \emph{J. Artif. Intell. Res.(JAIR)}, 20,
379--404, 2003.

\leavevmode\hypertarget{ref-nebel_plan_1995}{}%
Nebel, B., and J. Koehler\\
Plan reuse versus plan generation: A theoretical and empirical analysis,
\emph{Artificial Intelligence}, 76 (1), 427--454, 1995.

\leavevmode\hypertarget{ref-nguyen_reviving_2001}{}%
Nguyen, X., and S. Kambhampati\\
Reviving partial order planning, \emph{IJCAI}, 2001, 1,459--464.

\leavevmode\hypertarget{ref-paulson_semanticsdirected_1982}{}%
Paulson, L.\\
A semantics-directed compiler generator, \emph{Proceedings of the 9th
ACM SIGPLAN-SIGACT symposium on Principles of programming languages -
POPL '82}, Albuquerque, Mexico: ACM Press, 1982, 224--233.
doi:\href{https://doi.org/10.1145/582153.582178}{10.1145/582153.582178}.

\leavevmode\hypertarget{ref-peano_arithmetices_1889}{}%
Peano, G.\\
\emph{Arithmetices principia: Nova methodo exposita}, Fratres Bocca,
1889.

\leavevmode\hypertarget{ref-pednault_adl_1989}{}%
Pednault, E. P.\\
ADL: Exploring the Middle Ground Between STRIPS and the Situation
Calculus., \emph{Kr}, 89, 324--332, 1989.

\leavevmode\hypertarget{ref-penberthy_ucpop_1992}{}%
Penberthy, J. S., D. S. Weld, and others\\
UCPOP: A Sound, Complete, Partial Order Planner for ADL, \emph{Kr}, 92,
103--114, 1992.

\leavevmode\hypertarget{ref-peot_threatremoval_1993}{}%
Peot, M. A., and D. E. Smith\\
Threat-removal strategies for partial-order planning, \emph{AAAI}, 1993,
93,492--499.

\leavevmode\hypertarget{ref-peot_postponing_1994}{}%
Peot, M. A., and D. E. Smith\\
Postponing Threats in Partial-Order Planning 1994.

\leavevmode\hypertarget{ref-raghavana_plan}{}%
Raghavana, S., P. Singlab, and R. J. Mooneya\\
Plan Recognition using Statistical Relational Models.

\leavevmode\hypertarget{ref-ramirez_plan_2009}{}%
Ramırez, M., and H. Geffner\\
Plan recognition as planning, \emph{Proceedings of the International
Conference on International Conference on Automated Planning and
Scheduling}, AAAI Press, 2009, 19,1778--1783.

\leavevmode\hypertarget{ref-ramoul_mixedinitiative_2018}{}%
RAMOUL, A.\\
Mixed-initiative planning system to assist the management of complex IT
systems 2018.

\leavevmode\hypertarget{ref-jinghairao_logicbased_2004}{}%
Rao, J., P. Kungas, and M. Matskin\\
Logic-based Web services composition: From service description to
process model, \emph{Proceedings. IEEE International Conference on Web
Services, 2004.}, July 2004, 446--453.
doi:\href{https://doi.org/10.1109/ICWS.2004.1314769}{10.1109/ICWS.2004.1314769}.

\leavevmode\hypertarget{ref-renggli_practical_2010}{}%
Renggli, L., S. Ducasse, T. Gîrba, and O. Nierstrasz\\
Practical Dynamic Grammars for Dynamic Languages, \emph{Workshop on
Dynamic Languages and Applications}, Malaga, Spain, 2010, 4.

\leavevmode\hypertarget{ref-richter_lama_2010}{}%
Richter, S., and M. Westphal\\
The LAMA planner: Guiding cost-based anytime planning with landmarks,
\emph{Journal of Artificial Intelligence Research}, 39 (1), 127--177,
2010.

\leavevmode\hypertarget{ref-roy_possibilistic_2011}{}%
Roy, P. C., A. Bouzouane, S. Giroux, and B. Bouchard\\
Possibilistic activity recognition in smart homes for cognitively
impaired people, \emph{Applied Artificial Intelligence}, 25 (10),
883--926, 2011.

\leavevmode\hypertarget{ref-russell_mysticism_1917}{}%
Russell, B.\\
\emph{Mysticism and Logic and Other Essays}, London: George Allen and
Unwin, 1917.

\leavevmode\hypertarget{ref-russell_principia_1978}{}%
Russell, B., and A. N. Whitehead\\
\emph{Principia mathematica}, Cambridge University Press, 1978.

\leavevmode\hypertarget{ref-sanner_relational_2010}{}%
Sanner, S.\\
Relational dynamic influence diagram language (rddl): Language
description Unpublished ms. Australian National University Unpublished
ms. Australian National University, 2010.

\leavevmode\hypertarget{ref-sapena_combining_2014}{}%
Sapena, O., E. Onaindıa, and A. Torreno\\
Combining heuristics to accelerate forward partial-order planning,
\emph{CSTPS}, 25, 2014.

\leavevmode\hypertarget{ref-say_mathematical_2016}{}%
Say, B., A. A. Cire, and J. C. Beck\\
Mathematical programming models for optimizing partial-order plan
flexibility, \emph{22nd European Conference of Artificial Intelligence},
2016.

\leavevmode\hypertarget{ref-schwarzentruber_hintikka_2018}{}%
Schwarzentruber, F.\\
Hintikka's World: Agents with Higher-order Knowledge, \emph{Proceedings
of the Twenty-Seventh International Joint Conference on Artificial
Intelligence}, Stockholm, Sweden: International Joint Conferences on
Artificial Intelligence Organization, July 2018, 5859--5861.
doi:\href{https://doi.org/10.24963/ijcai.2018/862}{10.24963/ijcai.2018/862}.

\leavevmode\hypertarget{ref-sebastia_graphbased_2000}{}%
Sebastia, L., E. Onaindia, and E. Marzal\\
A Graph-based Approach for POCL Planning, \emph{ECAI}, 2000, 531--535.

\leavevmode\hypertarget{ref-shekhar_learning_2016}{}%
Shekhar, S., and D. Khemani\\
Learning and Tuning Meta-heuristics in Plan Space Planning, \emph{arXiv
preprint arXiv:1601.07483}, 2016.

\leavevmode\hypertarget{ref-silberschatz_port_1981}{}%
Silberschatz, A.\\
Port directed communication, \emph{The Computer Journal}, 24 (1),
78--82, January 1981.
doi:\href{https://doi.org/10.1093/comjnl/24.1.78}{10.1093/comjnl/24.1.78}.

\leavevmode\hypertarget{ref-sjoberg_automated_2015}{}%
Sjöberg, J., and H. Nissar\\
Automated scheduling: Performance in different scenarios, 2015.

\leavevmode\hypertarget{ref-sohrabi_plan_2016}{}%
Sohrabi, S., A. V. Riabov, and O. Udrea\\
Plan Recognition as Planning Revisited, \emph{Proceedings of the
International Joint Conference on Artificial Intelligence}, Vol. 25,
2016.

\leavevmode\hypertarget{ref-souto_dynamic_1998}{}%
Souto, D. C., M. V. Ferro, and M. A. Pardo\\
Dynamic Programming as Frame for Efficient Parsing, \emph{Proceedings
SCCC'98. 18th International Conference of the Chilean Society of
Computer Science (Cat. No.98EX212)(SCCC)}, November 1998, 68.
doi:\href{https://doi.org/10.1109/SCCC.1998.730784}{10.1109/SCCC.1998.730784}.

\leavevmode\hypertarget{ref-sugawara_reusing_1995}{}%
Sugawara, T.\\
Reusing Past Plans in Distributed Planning., \emph{ICMAS}, 1995,
360--367.

\leavevmode\hypertarget{ref-takesaki_theory_2013}{}%
Takesaki, M.\\
\emph{Theory of operator algebras II}, vol. 125Springer Science \&
Business Media, 2013.

\leavevmode\hypertarget{ref-tan_complexity_2014}{}%
Tan, X., and M. Gruninger\\
The Complexity of Partial-Order Plan Viability Problems., \emph{ICAPS},
2014.

\leavevmode\hypertarget{ref-thiebaux_defense_2005}{}%
Thiébaux, S., J. Hoffmann, and B. Nebel\\
In defense of PDDL axioms, \emph{Artificial Intelligence}, 168 (1-2),
38--69, 2005.

\leavevmode\hypertarget{ref-tolksdorf_semantic_2004}{}%
Tolksdorf, R., L. Nixon, F. Liebsch, D. Minh Nguyen, and E. Paslaru
Bontas\\
Semantic web spaces, 2004.

\leavevmode\hypertarget{ref-toro_reflexive_2008}{}%
Toro, C., C. Sanín, E. Szczerbicki, and J. Posada\\
Reflexive Ontologies: Enhancing Ontologies with Self-Contained Queries,
\emph{Cybernetics and Systems}, 39 (2), 171--189, February 2008.
doi:\href{https://doi.org/10.1080/01969720701853467}{10.1080/01969720701853467}.

\leavevmode\hypertarget{ref-unicodeconsortium_unicode_2018a}{}%
Unicode Consortium\\
\emph{The Unicode Standard, Version 11.0}, Core Specification
11.0Mountain View, CA, June 2018a.

\leavevmode\hypertarget{ref-unicodeconsortium_unicode_2018}{}%
Unicode Consortium\\
Unicode Character Database, \emph{About the Unicode Character Database},
June 2018b.

\leavevmode\hypertarget{ref-vanderkrogt_plan_2005}{}%
Van Der Krogt, R., and M. De Weerdt\\
Plan Repair as an Extension of Planning., \emph{ICAPS}, 2005,
5,161--170.

\leavevmode\hypertarget{ref-vanharmelen_handbook_2008}{}%
Van Harmelen, F., V. Lifschitz, and B. Porter\\
\emph{Handbook of knowledge representation}, vol. 1Elsevier, 2008.

\leavevmode\hypertarget{ref-venn_diagrammatic_1880}{}%
Venn, J.\\
I. On the diagrammatic and mechanical representation of propositions and
reasonings, \emph{The London, Edinburgh, and Dublin philosophical
magazine and journal of science}, 10 (59), 1--18, 1880.

\leavevmode\hypertarget{ref-vepstas_hypergraph_2008}{}%
Vepstas, L.\\
Hypergraph edge-to-edge, \emph{Wikipedia}, May 2008.

\leavevmode\hypertarget{ref-vepstas_sheaves_2008}{}%
Vepštas, L.\\
\emph{Sheaves: A Topological Approach to Big Data}, 2008.

\leavevmode\hypertarget{ref-vidal_online_2010}{}%
Vidal, N., P. Taillibert, and S. Aknine\\
Online behavior recognition: A new grammar model linking measurements
and intents, \emph{2010 22nd IEEE International Conference on Tools with
Artificial Intelligence}, IEEE, 2010, 2,129--137.

\leavevmode\hypertarget{ref-w3c_rdf_2004a}{}%
W3C\\
\emph{RDF Semantics}, W3C, 2004a.

\leavevmode\hypertarget{ref-w3c_rdf_2004}{}%
W3C\\
RDF Vocabulary Description Language 1.0: RDF Schema February 2004b.

\leavevmode\hypertarget{ref-w3c_owl_2012}{}%
W3C\\
OWL 2 Web Ontology Language Document Overview (Second Edition) December
2012.

\leavevmode\hypertarget{ref-w3c_rdf_2014}{}%
W3C\\
RDF 1.1 Turtle: Terse RDF Triple Language January 2014.

\leavevmode\hypertarget{ref-younes_ppddl_2004}{}%
Younes, H. akan L., and M. L. Littman\\
PPDDL 1.0: An extension to PDDL for expressing planning domains with
probabilistic effects, \emph{Techn. Rep. CMU-CS-04-162}, 2004.

\leavevmode\hypertarget{ref-younes_vhpop_2003}{}%
Younes, H. akan L., and R. G. Simmons\\
VHPOP : Versatile heuristic partial order planner, \emph{JAIR},
405--430, 2003.

\leavevmode\hypertarget{ref-young_dpocl_1994}{}%
Young, R. M., and J. D. Moore\\
DPOCL: A principled approach to discourse planning, \emph{Proceedings of
the Seventh International Workshop on Natural Language Generation},
Association for Computational Linguistics, 1994, 13--20.

\leavevmode\hypertarget{ref-zhuo_modellite_2017}{}%
Zhuo, H. H., and S. Kambhampati\\
Model-lite planning: Case-based vs. Model-based approaches,
\emph{Artificial Intelligence}, 246, 1--21, 2017.

\hypertarget{apendix}{%
\chapter*{Apendix}\label{apendix}}
\addcontentsline{toc}{chapter}{Apendix}

\hypertarget{chapter-1}{%
\section{Chapter 1}\label{chapter-1}}

\textbf{SAMIR}:

\begin{itemize}
\tightlist
\item
  Cite slow start 1.2
\item
  System : implentation -\textgreater{} Model : theory
\item
  Examples
\item
  theorems or not
\item
  s : D -\textgreater{} D notation in table
\item
  Double arrow too for ±
\item
  to port : porter une notion in smth
\item
  1.2.2 : criterion
\item
  Example of parsing
\item
  1.2.6.1 : light defeasible logic
\item
  lacks properties and proofs: what are the advantages
\end{itemize}

\begin{longtable}[]{@{}llllll@{}}
\toprule
\begin{minipage}[b]{0.07\columnwidth}\raggedright
Symbol\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\raggedright
Name\strut
\end{minipage} & \begin{minipage}[b]{0.03\columnwidth}\raggedright
Arity\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\raggedright
Arguments\strut
\end{minipage} & \begin{minipage}[b]{0.04\columnwidth}\raggedright
Type\strut
\end{minipage} & \begin{minipage}[b]{0.52\columnwidth}\raggedright
Definition\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(=\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Equal\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
expr, expr\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\(x = x : \top\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\neq\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Not Equal\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
expr, expr\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\(x \neq x : \bot\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(:\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Such that\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
element, predicate\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
element\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
axiom of \nameref{axi:specification}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(?\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Predicate\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
n\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
expr\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
arbitrary\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\top\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
True\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
NA\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\bot\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
False\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
NA\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\lnot\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Negation\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\(\lnot \top = \bot\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\land\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Conjunction\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
n\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\(a \land b \vdash (a = b = \top)\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\lor\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Disjunction\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
n\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\(\lnot(a \lor b) \vdash (a = b = \bot)\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\veeonwedge\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Logic operator\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
n\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
arbitrary\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\vdash\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Entailment\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\forall\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Universality\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
var\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
modifier\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\exists\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Existentiality\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
var\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
modifier\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\exists!\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Unicity\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
var\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
modifier\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\nexists\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Exclusivity\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
var\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
modifier\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\(\lneq \exists\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\textsection\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Solution\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
var\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
modifier\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\([]\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Iverson's brackets\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
int\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\([\bot]=0 \land [\top]=1\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\{\}\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Set\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
n\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
elements\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\emptyset\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Empty set\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
NA\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\(\emptyset = \{\}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\in\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Member\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
element, set\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\subset\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Subset\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\(\cal{S} \subset \cal{T} \vdash ((e \in \cal{S} \vdash e\in \cal{T}) \land \cal{S} \neq \cal{T})\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\cup\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Union\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
n\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\(\cal{S} \cup \cal{T} = \{e : e \in \cal{S} \lor e \in \cal{T} \}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\cap\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Intersection\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
n\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\(\cal{S} \cap \cal{T} = \{e : e \in \cal{S} \land e \in \cal{T} \}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\setminus\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Difference\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\(\cal{S} \setminus \cal{T} = \{e : e \in \cal{S} \land e \notin \cal{T} \}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\times\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Cartesian product\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
n\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\(\cal{S} \times \cal{T} = \{\langle e_{\cal{S}}, e_{\cal{T}} \rangle : e_{\cal{S}} \in \cal{S} \land e_{\cal{T}}\in \cal{T}\}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(| |\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Cardinal\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
int\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\wp\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Powerset\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
axiom of \nameref{axi:powerset}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\circ\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Function composition\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
n\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
function\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
function\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\sigma\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Selection\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
predicate, set\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\(\sigma_?(\cal{S}) = \{e : ?(e) \land e\in \cal{S}\}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\pi\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Projection\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
function, set\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\(\pi_f(\cal{S}) = \{ f(e) : e \in \cal{S}\}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\mapsto\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Substitution\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
var, function, expr\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
expr\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\((e \mapsto f(e))(\bb{e}(e)) = \bb{e}(f(e))\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\langle \rangle\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Tuple\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
n\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
elements\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
tuple\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\to\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Association\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
elements\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
tuple\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\phi\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Incidence/Adjacence\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
n\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
various\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
various\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\chi\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Transitivity\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
relation\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
relation\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\div\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Quotient\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
function, graph\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
graph\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\mu\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Meta\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
entity\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
entity\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\nu\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Name\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
entity\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
string\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\rho\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Parameter\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
entity\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
list\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\otimes\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Flaws\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
plan\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\odot\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Resolvers\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
flaw\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\downdasharrow\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Partial support\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
link, action\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\downarrow\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Full support\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
plan, action\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\prec\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Precedance\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
action\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\succ\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Succession\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
action\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
boolean\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\Mapsto^*\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Shortest path\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
?\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(h\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Heuristic\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
element\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
float\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(\gamma\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Constraints\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
element\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
set\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(¢\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Cost\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
element\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
float\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\(d\)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
Duration\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
1\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
element\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
time\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.03\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.04\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[t]{0.52\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\end{document}
